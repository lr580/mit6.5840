[mit6.5840网址](http://nil.csail.mit.edu/6.5840/2025/)

# 实现过程

## Lab1

### 环境准备

根据 [lab1](http://nil.csail.mit.edu/6.5840/2025/labs/lab-mr.html) 的第一句话，克隆远程仓库获取源码

```sh
git clone git://g.csail.mit.edu/6.5840-golabs-2025 6.5840
```

在 [setup go](http://nil.csail.mit.edu/6.5840/2025/labs/go.html)，需求 Go 1.22 版本，使用 `go version` 可查询版本。

需求使用 Linux，这里我使用 WSL2。按照指示检查运行：

> ```go
> cd src/main
> go build -buildmode=plugin ../mrapps/wc.go
> rm mr-out*
> go run mrsequential.go wc.so pg*.txt
> more mr-out-0
> ```

其中，build的构建模式为plugin，它生成`.so` 文件，即 `wc.go`，项目结构 `src/`

- `mrapps/` 是 map-reduce 实际应用例子，如 `wc.go`，看到它定义了 Map 和 Reduce 函数。其中 Map 函数的第一个参数在例子中忽略，只看第二个参数(文件内容)，它把不是 Letter(a-zA-Z) 的连续内容作为分隔符，然后分割后的单词数组返回 (`mr/KeyValue`) (没有做合并)。Reduce 函数，传入 key 值和 values 列表，values 长度转字符串作为该 key 的合并结果。
- 对 `main/mrsequential.go`，传入 `.so` 执行，选择文件名执行。它的 `loadPlugin` 函数，导入插件，查找 Map, Reduce 函数，并返回。对每一份文件，把读取的文件内容和文件名传入 map 函数，得到 key-value 列表；所有文件的合并到一个列表里。(实际上应该分快存储)，然后进行排序，创建输出文件。排序后 key 相同的全部 value 集合并到一个 string[] 里。把这个 slice 传给 reduce 函数，然后输出结果到文件。

> 注意：只要修改了项目任意代码，plugin 一定要重新编译。

### 任务描述

#### 任务

实现：协调节点coordinator, 工作节点worker。单协调进程，任意多个工作节点，该lab只考虑工作节点都在同一个机子。使用RPC，循环请求协调者执行任务。协调者应当注意若工作者在10秒内未能完成任务，将它分发给其他工作者。

`main/mrcoorinator.go`, `main/mrworker.go` 是给定不可修改的代码，参考它们，实现 `mr/coordinator.go`，`mr/worker.go`，`mr/rpc.go`。执行 mapreduce 的过程：

```sh
go run mrcoordinator.go pg-*.txt # 一个
go run mrworker.go wc.so # 一个或多个
cat mr-out-* | sort | more # 查看结果
```

使用测试脚本检查 `wc, indexer` 任务是否产出正确，以及测试是否并行，能否恢复crash。

```sh
bash test-mr.sh
```

把 `mr/coordinator.go` 的 Done 函数改为`ret := true`，让协调者马上退出。如果全部通过，测试完成。

若 rpc 报错如 ` method "Done" has 1 input parameters`，可以忽略，因为它不会被远程调用。[文档](https://golang.org/src/net/rpc/server.go) 帮助检查是否所有方法对 RPC 合适，即有 3 输入。若出现 `connect: connection refused`，当协调者关闭后，无法连接是正常的。

#### 规则

- map 阶段，分割中间 keys 为桶，给 `nReduce` 个任务，在 `main/mrcoordinator.go` 该参数传递给 `MakeCoordinator()`，每个 mapper 要创建 `nReduce` 个中间文件给 reduce 任务消费。
- 工作者实现第 X 个 reduce 任务时，写到 `mr-out-X`。该文件对每个 Reduce 函数输出包含一行，使用 format `%v %v` 生成 key value 行。查阅 `main/sequential.go` 查看格式范例。
- 工作者把中间 Map 输出放当前工作目录。
- `main/mrcoordinator.go` 期望 `mr/coordinator.go` 实现 `Done()` 方法，返回 true 如果任务执行完毕，随后 `mrcoordinator.go` 退出。
- 全部任务完成，工作者进程关闭，可以用 `call()` 的返回值实现。若工作者无法联系协调者，可以假设任务完毕，并让工作者关闭。也可以自由发挥。

#### 提示

- 一个开始的办法是调整 `mr/worker.go` 的 `Worker()` 发送 RPC 给协调者请求任务，然后修改协调者回答未开始的map任务文件名，然后修改工作者去读取文件，调用 Map 函数。
- 使用 go plugin 加载 map, reduce 任务，`.so` 文件。如果修改 `mr/` 的内容，可能需要重新构造 `.so`。
- 工作者共享同一个文件系统。
- 中间文件可以命名为 `mr-X-Y`，分别表示 map, reduce 编号。
- map 任务需要存储中间键值对在文件，使其可以在 reduce 被读取，可以考虑 json 包，参考示例代码读写。
- 可以使用 `ihash(key)` 函数(`worker.go`)选择reduce任务。
- 读文件，排序，存储reduce输出，可以参考 `mrsequential.go`。
- 协调者并发，需要锁住共享数据。
- 使用 Go race detector，如 `go run -race`。在 `test-mr.sh` 开头有注释，提示如何使用。
- 工作者有时需要等待，如 reduce 需要等最后一个 map 完成。可以周期轮询+sleep；或者 RPC 做一个等待循环，sync.Cond。每个 RPC handler 独立线程，所以 handler 等待不会阻止协调者处理其他 RPC。
- 协调者无法区分崩掉的、静止的、太慢的工作者。只能让协调者等待一段时间，然后切换工作者。
- 若实现文章 Section 3.6 的 Backup 任务，它只应当长时间后才被安排，如10s，也就是有挂掉的。
- 测试 crash，可以尝试 `mrapps/crash.go`，它随机关闭 map/reduce 函数。
- 防止 crash 导致写了一半的文件，MapReduce 论文使用 trick：使用临时文件，并原子地重命名它，如果完成了写。使用 `ioutil.TempFile / os.CreateTemp` 来创建这样的文件，然后 `os.Rename` 原子重命名。
- `test-mr.sh` 运行所有进程在 `mr-tmp`，如果挂了，中间过程在该文件找。
- `test-mr-many.sh` 运行多次，可以发现低概率错误。参数：运行次数。不要通过自己跑多次 `test-nr.sh` 来做它，这样协调者会复用一个 socket。
- Go RPC 只发送大写结构体字段，子结构体也是如此。
- 调用 RPC `call()`，返回结构体要所有字段赋值为默认值，有参见例子。否则，可能结果不对。

#### 通用提示

[url](http://nil.csail.mit.edu/6.5840/2025/labs/guidance.html)：

- Easy：数小时；Moderate：6h每周；hard：超过6h每周。

  大部分任务只需要几百行代码，但概念+debug困难。

- 在线 Go 教程 [here](http://tour.golang.org/)，[Effective Go](https://golang.org/doc/effective_go.html)，[Editor](https://golang.org/doc/editors.html)

- [race detector](https://blog.golang.org/race-detector) 报告的 race 需要修复。

- [Raft](https://thesquareplanet.com/blog/students-guide-to-raft/) 指引

- [locking](http://nil.csail.mit.edu/6.5840/2025/labs/raft-locking.txt) 指引

- [Raft structuring](http://nil.csail.mit.edu/6.5840/2025/labs/raft-structure.txt) 指引

- [图解Raft交互](http://nil.csail.mit.edu/6.5840/2025/notes/raft_diagram.pdf)

- 在每个节点收发消息关键位置 print，日志重定向到文件来分析，方便 debug

  结构化 debug 输出信息，那么 grep 查找消息更方便

- `DPrintf` 比 `log.Printf` 更有用

- [Go format strings](https://golang.org/pkg/fmt/)

- 颜色 / 列来解析 log 输出 [参考](https://blog.josejg.com/debugging-pretty/)

- git 教程 [url](https://git-scm.com/book/en/v2) [url2](https://www.kernel.org/pub/software/scm/git/docs/user-manual.html)

#### 调试技巧

先对问题的潜在成因提出假设；收集可能相关的证据；综合分析已获取的信息；根据需要重复上述过程。对于长时间的调试任务，建议做好记录

一种有效策略是逐步定位问题首次出现的时间节点。可以在程序执行的不同阶段插入状态检查代码，或是添加输出关键状态信息的日志语句，将日志保存至文件后仔细排查首个出现异常的时间点

Raft实验涉及的事件（如RPC请求到达、超时触发、节点故障）可能在意料之外的时刻发生，或以难以预料的顺序交错出现。例如某个节点刚决定参与竞选，而另一个节点却认为自己是当前领导者。建议推演"后续可能发生的情况"：比如当Raft代码释放互斥锁后，下一瞬间可能就会处理到达的RPC请求或超时事件。可通过打印语句来捕获实际的事件执行顺序。

必须严格遵循Raft论文图2的规范，容易遗漏其中规定的状态检查条件或必须执行的状态变更。若出现故障，请重新核验代码是否完全符合图2要求。

在编写代码时（即出现故障前），建议对代码依赖的前提条件添加显式检查（例如使用Go的[panic](https://gobyexample.com/panic)机制）。这类检查有助于发现后续代码无意中违背假设的情况。

若原本正常的代码出现异常，很可能是近期修改引入的问题。

故障往往藏在最后检查的地方，因此即使对确信无误的代码也需保持审视

#### 挑战任务

- 实现自己的 MapReduce 任务，可以看 `mrapps/*` 例子，如文章 Section 2.3 的分布式 Grep。
- 让协调者，工作者在不同机器上工作，设置 RPC 在 TCP/IP 而不是 Unix sockets (参考注释 `Coordinator.server()`)，用共享文件系统读写文件。如 `ssh` 多个 [Athena cluster](http://kb.mit.edu/confluence/display/istcontrib/Getting+Started+with+Athena)，使用 [AFS](http://kb.mit.edu/confluence/display/istcontrib/AFS+at+MIT+-+An+Introduction) 共享文件，或自己用 AWS 实例 / S3 存储。

#### 代码梳理

`main/mrcoordinator.go` main 函数，把命令行参数全部传进去，以及 nReduce=10，执行 `mr.MakeCoordinator`，完毕 sleep 1s，返回值为 m，若 `m.Done()` 是 false，就一直 sleep 1s (轮询)。

`main/mrworker.go` main 函数，对 CLI 参数加载 plugin，调用 `mr.Worker` 执行 map, reduce。

`mr/rpc.go` 给定例子使用 RPC，对该例子，`mr/coordinator.go` 定义了 Example 函数，`mr/worker.go` 定义了 `CallExample()` 函数。在 `mr/worker.go` 定义了 call 函数，在 `mr/coordinator.go` 定义了 `server()` 结构体方法，用来执行 RPC。

`mr/worker.go` 定义了 ihash 函数用于字符串转 int。定义了 `KeyValue` 结构体。定义了 `Worker` 函数需要自己实现。

`mr/coordinator.go` 定义了 `Coordinator` 结构体及其成员方法(需要自己补充成员属性)。需要实现：①MakeCoordinator构造函数；②Done成员函数。

### 实现思路

具体细节见 `mr/worker.go`, `mr/coordinator.go`。分为多个部分描述：

#### coordinator.go

##### 构造函数

没有指定 map 任务数，故：按照文件数量分配 map 数量，一个文件一个 map。不考虑更加复杂的情况。

每个任务有两个属性：状态（空闲、运行中、完毕）、开始时间。构造两个任务列表，分别表示 map, reduce。

此外，给自己构造三个阶段：map 阶段，reduce 阶段，退出阶段。

##### 超时检测

10秒算超时。构造阶段设置一个 `deadWorkerChecker` 进程，隔段时间(如1s或5s)检测一次全体任务。对超时任务，直接重新设置为 IDLE。

*理由：如果后面发现 worker 没挂，让该任务被执行多次，因幂等不影响结果。可以给读写同一文件的过程加锁，避免 reducer 执行读到一半被覆盖*

由于超时检测会修改任务，而正常执行也会，避免竞态，在修改任务时加 mutex 锁

##### 分配任务

定义 Allocate RPC方法，对 worker 发送的请求，分配任务给 worker。

- 如果当前是退出阶段，发送信息给 worker 报告它退出。

- 否则，对当前任务列表(根据阶段选择 map 列表或 reduce 列表)，找到任意一个空闲任务分配给 worker

  简单起见，直接暴力遍历查找。后续需要的话可以用 bitset / map 等优化。

- 如果没有空闲任务，证明当前全部 map / reduce 都在执行，让 worker 继续等待。

加 mutex，下面结果收集同理。也是防止多个 RPC 竞态，不只是和 `deadWorkerChecker`。

##### 结果收集

定义 Report RPC方法，worker 完成任务后调用

- 如果当前状态和 worker 汇报的状态不一样，taskid 无意义，丢弃该饭会报错并告诉 worker。这证明 worker 挂掉了导致一个任务被执行多次。
- 否则，如果当前任务状态不是 RUNNING，报错。
- 否则，记录当前任务完成，更改状态。如果当前任务都完成了，进入下一阶段。

#### worker.go

##### 文件I/O

按要求进行文件读写。

- 除了 map 必须返回完整 content 外；对 KV 的读写使用流。
- 写文件，先写到一个临时文件(随机创建到当前目录)，写完后原子重命名为目标文件。
- 读文件，由于写操作 os.Rename 原子，不需要对读加锁，不会出现读写竞态等问题。

##### 主循环

由于在 `main` 里，它直接启动 `mrWorker`，不能像 `mrcoordinator` 那样 Done 轮询来结束，所以需要 worker 自己控制是否结束。

在这里设计结束主循环的情况只有两种：在发送 Allocate 时返回报错(coordinator挂了)、或者返回已完成。

主循环为：

1. 发送 Allocate 调用

   - 如果结果是等待，随机等待一段时间。

     *也可以设计指数退避*，这里从简考虑。

   - 否则，如果是 mapping / reducing，分别执行 map/reduce 任务。

2. 根据执行结果，发送 Report 调用。

##### mapping

过程：

1. 读文件

2. 执行 mapf 函数，得到 KV 列表

3. 将 KV 按 ihash 分桶

4. 每个桶的 KV 子列表直接写入中间文件 `mr-X-Y`，

   > X 是当前 map 号，Y 是全体 [0, nReduce)。这里 X,Y 0-indexed

如果写入报错，返回执行失败，否则都返回执行成功。

##### reducing

过程：

1. 读所有 `mr-X-Y` 文件，其中 Y 是当前 reduce 号，X 是全体任意。

   > 修复了 bugs，注意一定要严格是 X,Y 是整数，测试过程存在 `mr-??-Y`，其中 `??` 是字符串。避免匹配失败报错。

2. 合并所有 KV，然后排序

3. 排序后，同 K 的 V 分组，传入 reducef，然后记录该组答案

4. 写入该 reduce 任务的结果到 `mr-out-Y`

如果读写报错，返回执行失败，否则都返回执行成功。

## Lab2

### 任务描述

[lab2描述文档](http://nil.csail.mit.edu/6.5840/2025/labs/lab-kvsrv1.html)

##### 概述

实现 Key/value 服务器，单机，每个 Put 最多执行一次，保证 Linearizable 线性一致性。

每个用户与 KV 服务器用 Clerk 交互，它发送 RPC：`Put(key, value, version)` 和 `Get(key)`。服务器维护 in-memory map 记录每个 key 一个 (key, version) 元组。KV 都是字符串。版本数字记录 key 被写的次数。

Put 添加或覆盖当且仅当服务器版本与key版本一样，然后增加key的版本。不匹配返回 `rpc.ErrVersion`。客户端用版本0新建key，此时保存到服务器版本是1。若Put版本>0但key不存在，返回 `rpc.ErrNoKey`。

Get 获取 key 的当前值和版本，若不存在 key，返回 `rpc.ErrNoKey`。

版本对锁有意义，使得 at-most-one 语义有效，考虑到网络不可靠导致的重传。

`src/kvsrv1` 有框架和测试。`client.go` 实现 Clerk，使客户端用 RPC 与服务器交互，提供 Put, Get 方法。`server.go` 包含服务代码，如 Put, Get 处理器。`kvsrc1/rpc` 包里定义了 RPC 请求，回答，错误值。可用而不需要修改。测试；

```sh
cd src/kvsrv1
go test -v
```

##### 任务1：可靠网络

假设消息没有 drop，实现 RPC 发送和处理器。完成可靠性测试。

```sh
go test -v -run Reliable
```

Passed 行输出里的数字是：①花费的秒数；②1；③发送的RPC数(含客户端)；④执行的 KV 操作(Clert, Get, Put)。

检查代码竞态，需要使用 `go test -race`。

##### 任务2：锁

> 分布式应用的客户端跑在不同机器，使用 KV 服务器协调活动，如 ZooKeeper, Etcd 允许分布式锁。类似于Go程序中的线程可以通过锁（即sync.Mutex）进行协调。ZooKeeper和Etcd通过条件写入（conditional put）机制实现这种锁功能。

实现锁，支持 `Acquire` and `Release`。只有一个客户端可以成功获取锁，其他必须等它释放。在 `src/kvsrv1/lock/` 里有框架和测试，修改 `src/kvsrv1/lock/lock.go`，用 `lk.ck.Put/Get()` 与 KV 服务器沟通。

> 若客户端崩了，它的锁无法释放。所以一般锁有[租期](https://en.wikipedia.org/wiki/Lease_(computer_science)#:~:text=Leases%20are%20commonly%20used%20in,to%20rely%20on%20the%20resource.)，过期后会释放锁。这里可以忽略。

必须通过 Reliable test (lock 目录下的)

```sh
go test -v -run Reliable
```

对每个锁客户端，需要唯一标识符，调用 `kvtest.RandValue(8)`。

锁服务用特殊 key 存储锁状态，定义锁状态。传入参数 l (MakeLock, `lock/lock.go`)

##### 任务3：传输失败

网络会重排序、延迟、丢弃 RPC 请求和回复。需要回复它们，让 Clerk 保持重试直到收到回答。

网络丢弃请求，客户端重发即可解决。但网络还会丢弃回答，如果重发，Get 没问题，但 Put 不行，需要对 Put，若之前已接收并执行过该Put请求，服务端会向重传的副本返回rpc.ErrVersion错误而非重复执行操作，因此携带相同版本号重传Put RPC也是安全的。此时客户端不知道执行没有，可能执行了、可能没执行而是其他Clerk更新了。故，若重传Put收到了rpc.ErrVersion，它必须向应用程序返回rpc.ErrMaybe而非rpc.ErrVersion。后续应由应用程序处理此情况。若服务端对首次（非重传）Put RPC返回rpc.ErrVersion，则Clerk应向应用程序返回rpc.ErrVersion，因为此时可确定该RPC未被服务端执行。

> 若能使Put操作具备"精确一次"的执行语义（即避免出现rpc.ErrMaybe错误），应用程序开发者的体验将得到显著改善。但若不在服务端为每个Clerk维护状态信息，这一点将难以保证。在本实验的最后一项练习中，您将使用Clerk实现锁机制，以此探索如何基于"至多一次"的Clerk.Put语义进行编程。

修改 `kvsrv1/client.go`，`ck.clnt.Call()` 返回 true 表示收到 RPC 回答，false 表示收不到(超时)，此时要不断重发直到回答。该任务不应该改变服务器。

通过所有 `kvsrv1/` 的测试，就是过了：

```sh
go test -v
```

Client 重试之前，应当等待，如 100 ms 睡眠。

##### 任务4：传输失败锁

修改锁的实现，使得它可以通过所有 `kvsrv1/lock` 的测试。

##### 代码梳理

`rpc/rpc.go` 略，可自行观看。

> `client.go` 里使用到了：`src/tester1/Clnt`，它又基于 `src/labrpc/labrpc.go` 实现了基于 channel 的 RPC。模拟网络实现失去请求、延迟、断连。这些具体暂时略。感觉已经屏蔽了细节，所以我不需要考虑。

`client.go` 里 `Clert` 是结构体，定义了构造方法，Get 方法和 Put 方法。

`server.go` 里实现了 debug 方法，`KVServer` 结构体，它的构造方法，以及 `Get, Put` 方法。还有可忽略的 `Kill()` 方法和 `StartKVServer`。

##### 线性一致性

[QA](http://nil.csail.mit.edu/6.5840/2025/papers/linearizability-faq.txt) 的直接翻译

> 线性一致性：如果客户端操作不是并发的，那么每个客户端的 Clerk.Get 和 Clerk.Put 操作都将观察到前序操作序列对系统状态所做的修改。对于并发操作，其返回值和最终的系统状态，将等同于这些操作以某种顺序依次执行（一次只执行一个）所得到的结果。，假设客户端 X 调用了 Clerk.Put()，随后客户端 Y 也调用了 Clerk.Put()，然后客户端 X 的调用先返回。在这种情况下，任何一个操作都必须能够观察到在该操作开始之前已经完成的所有操作所产生的效果。
>
> 线性一致性对应用程序非常便利，因为它呈现的行为就如同单个服务器逐次处理请求时的表现。例如，若某客户端从服务器获得了更新请求的成功响应，那么其他客户端后续发起的读取操作就能确保看到该更新的效果。对于单台服务器而言，实现线性一致性相对容易。

问：什么是可线性化？

答：可线性化是一种定义服务在面临并发客户端请求时行为正确性的方式。粗略来说，它要求服务的行为必须表现得像是按照客户端操作到达的顺序逐个执行这些操作。

可线性化是基于“历史记录”来定义的：即客户端操作的轨迹，标注了客户端开始每个操作的时间点以及客户端感知到操作完成的时间点。可线性化用于判断单个历史记录是否合法；如果一个服务能够生成的所有历史记录都是可线性化的，就可以说该服务是符合可线性化的。

历史记录中包含客户端开始操作的事件，以及客户端判定操作已完成的事件。因此，历史记录明确体现了客户端之间的并发性以及网络延迟的影响。通常，开始和结束事件对应于客户端与服务器之间交换的请求和响应消息。

一个历史记录是可线性化的，前提是能为每个操作分配一个“线性化点”（即一个时间点），使得每个操作的线性化点位于其开始和结束事件的时间点之间，并且历史记录的响应值与按照线性化点顺序逐个执行这些操作时得到的响应值一致。如果无法找到满足这两个要求的线性化点分配方案，则该历史记录不符合可线性化。

可线性化的一个重要推论是：服务在执行并发（时间上重叠的）操作时拥有顺序上的自由度。具体来说，如果客户端C1和C2的操作是并发的，即使C1的操作早于C2开始，服务器也可以先执行C2的操作。另一方面，如果C1的操作在C2开始之前已经结束，可线性化要求服务的行为必须像是先执行了C1的操作再执行C2的操作（即C2的操作需要能观察到C1的操作所产生的任何效果）。



问：可线性化检查器是如何工作的？

答：一个简单的可线性化检查器会尝试所有可能的顺序（或线性化点的选择），以判断是否存在一种顺序符合可线性化定义中的规则。由于在大规模历史记录上这种操作会非常缓慢，智能检查器会通过以下方式优化：避免检查明显不可能的排序（例如，若某提议的线性化点位于操作开始时间之前）、将历史记录分解为可独立检查的子历史记录（在可能的情况下），以及使用启发式方法优先尝试更可能的顺序。

以下论文描述了相关技术；我认为 Knossos 基于第一篇论文，而 Porcupine 则加入了第二篇论文的思想。

```
http://www.cs.ox.ac.uk/people/gavin.lowe/LinearizabiltyTesting/paper.pdf
https://arxiv.org/pdf/1504.00204.pdf
```



问：服务会使用可线性化检查器来实现可线性化吗？

答：不会；检查器仅用于测试环节。



问：那么服务如何实现可线性化呢？

答：如果服务以单服务器形式实现，且没有复制、缓存或内部并行机制，那么服务基本上只需按照客户端请求到达的顺序逐个执行即可。主要的复杂性来自因认为网络丢包而重发请求的客户端：对于具有副作用（side-effects）的请求，服务必须确保每个客户端请求仅执行一次。如果服务涉及复制或缓存，则需要更复杂的设计。

> 我认为复制就是多台机子。



问：是否有使用 Porcupine 或类似测试框架测试真实系统的案例？

答：此类测试很常见——例如，可以查看 [https://jepsen.io/analyses](https://jepsen.io/analyses)；Jepsen 是一个组织，已对许多存储系统的正确性（以及在适用情况下的可线性化）进行了测试。

具体到 Porcupine，这里有一个示例：[https://www.vldb.org/pvldb/vol15/p2201-zare.pdf](https://www.vldb.org/pvldb/vol15/p2201-zare.pdf)



问：还有哪些其他一致性模型？

答：可以参考以下模型：

- 最终一致性（eventual consistency）
- 因果一致性（causal consistency）
- 分支一致性（fork consistency）
- 可串行化（serializability）
- 顺序一致性（sequential consistency）
- 时间线一致性（timeline consistency）

此外，数据库、CPU 内存/缓存系统和文件系统领域还有其他一致性模型。

一般来说，不同模型的区别在于：

1. 对应用程序开发者而言的直观性；
2. 能实现的性能水平。

例如，最终一致性允许许多异常结果（例如，即使写入已完成，后续读取也可能看不到该写入），但在分布式/复制场景中，它可以比可线性化实现更高的性能。



问：为什么可线性化被称为强一致性模型？

答：它的"强"体现在禁止了许多可能让应用程序开发者感到意外的场景。

例如：

- 若调用 `put(x, 22)`且该写入操作已完成，且期间没有其他对 `x`的写入操作，随后调用 `get(x)`，可线性化保证看到的值一定是 22。即读取总能获取最新数据。
- 若没有人写入 `x`，且我调用 `get(x)`后也调用 `get(x)`，不会看到不同的值。

这些特性在其他一致性模型（如最终一致性和因果一致性）中并不成立。后者通常被称为"弱一致性"模型。



问：在实践中，人们如何确保分布式系统的正确性？

答：全面的测试是常见的方案。

形式化方法也经常被采用；可以查看以下示例：

```
https://arxiv.org/pdf/2210.13661.pdf
https://assets.amazon.science/67/f9/92733d574c11ba1a11bd08bfb8ae/how-amazon-web-services-uses-formal-methods.pdf
https://dl.acm.org/doi/abs/10.1145/3477132.3483540
https://www.ccs.neu.edu/~stavros/papers/2022-cpp-published.pdf
https://www.cs.purdue.edu/homes/pfonseca/papers/eurosys2017-dsbugs.pdf
https://www.andrew.cmu.edu/user/bparno/papers/ironfleet.pdf
```



问：为什么选择可线性化作为一致性模型，而不是最终一致性等其他模型？

答：人们确实经常构建提供弱于可线性化一致性（如最终一致性和因果一致性）的存储系统。

可线性化对应用开发者具有以下优势：

- 读取总能观察到最新数据。
- 若无并发写入，所有读取者会看到相同的数据。
- 在大多数可线性化系统上，可实现小型事务（如测试并设置），因为可线性化设计通常要求对每个数据项逐个执行操作。

而最终一致性等弱一致性方案能提供更高性能，因为它们不要求立即更新所有数据副本。这种性能优势往往是决定性因素。对于某些应用（如存储从不更新的图像或视频），弱一致性不会引发问题。

但弱一致性会给应用开发者带来复杂性：

- 读取可能观察到过时（陈旧）数据。
- 读取可能看到乱序的写入结果。
- 写入后立即读取可能看不到自己的写入，反而看到旧数据。
- 对同一项的并发更新无法逐个执行，因此难以实现测试并设置、原子递增等小型事务。



问：如何确定线性化操作中的橙色小线段（即线性化点）的位置？在图表中，它看起来像是随机画在请求主体内的某个位置。

答：其核心思想是，为了证明某个执行过程是线性化的，需要（由人工）找到放置这些橙色小线段（线性化点）的位置。也就是说，要证明某个历史记录是线性化的，需要找到满足以下要求的线性化点分配方案（从而确定操作顺序）：

- 所有函数调用都必须在其调用时刻和返回时刻之间的某个瞬间拥有一个线性化点。
- 所有函数在各自的线性化点必须瞬间完成，且行为符合顺序规范的要求。

因此，某些线性化点的放置位置是无效的——可能因为它们超出了请求的时间范围，也可能因为它们违反了顺序规范（对于键值存储而言，违反规范意味着读取操作未能观察到最近写入的值，这里的“最近”指的是线性化点的顺序）。

对于复杂的历史记录，可能需要尝试多种线性化点分配方案，才能找到能证明该历史记录具备线性化的方案。如果尝试了所有方案都无效，则该历史记录不具备线性化特性。



问：是否存在这种情况：当两个命令同时执行时，能够强制实施某种特定行为，使得某个命令总是先执行（即始终拥有更早的线性化点）？

答：在线性化存储服务（例如GFS或实验四）中，如果来自多个客户端的请求是并发的，服务可以自由选择执行这些请求的顺序。实际上，大多数服务会按照请求恰好到达网络的顺序来执行。实际的实现通常并不涉及明确的线性化点概念。



问：能否进行更强的一致性检查？从直觉上说，线性化似乎并不太实用，因为即使同时执行两个命令，读取的数据也可能不同。

答：确实，线性化让人联想到在程序中不使用锁的多线程编程。这种方式虽然能编写出正确程序，但需要格外谨慎。

更强一致性概念的典型例子是数据库中的事务机制，它能有效锁定所有使用的数据。对于需要读写多个数据项的程序而言，事务编程比线性化更简单。"可串行化"就是一种提供事务支持的一致性模型。

然而，事务系统相比线性化系统要复杂得多，速度更慢，且容错实现难度也更大。

> 线性一致性 关注的是 单个数据对象 在 时间线上的即时可见性。它回答了“现在这个数据的最新值是什么？”这个问题。
>
> 可串行化（事务） 关注的是 多个数据对象 上 一组操作执行的原子性和隔离性。它回答了“这组操作是否像没有并发一样执行？”这个问题。



问：为什么验证现实系统需要"巨大努力"？

答：验证是指证明程序是正确的，保证其符合某些规范。事实证明，证明复杂程序的重要定理是困难的——比普通编程要困难得多。

通过尝试本课程的实验，可以体会到这一点：[https://6826.csail.mit.edu/2020/](https://6826.csail.mit.edu/2020/)



问：根据指定阅读材料，大多数分布式系统并未经过形式化验证。那么团队如何判定产品已充分测试完毕、可交付客户？

答：明智的做法是在公司资金耗尽破产前就开始交付产品并获取收入。在此之前团队会尽可能充分测试，通常还会争取让部分早期客户在知悉产品可能存在缺陷的情况下先行使用（协助发现漏洞）。当产品功能足以满足多数客户需求且已知重大缺陷均已修复时，或许就是合适的交付时机。

除此之外，理性的客户也会对自己依赖的软件进行测试。任何严谨的组织都不会指望软件毫无缺陷。



问：为何不采用客户端发送命令的时间作为线性化点？即让系统按照客户端发送操作的顺序来执行？

答：要构建能保证这种行为的系统非常困难——开始时间是指客户端代码发出请求的时刻，但由于网络延迟，服务端可能很久之后才收到请求。也就是说，请求到达服务端的顺序可能与开始时间的顺序截然不同。理论上服务端可以延迟处理每个到达的请求，以等待更早发出的请求抵达，但网络延迟可能无限长，很难确定应该等待多久。这还会增加每个请求的延迟，甚至可能大幅增加。不过后续将学习的Spanner系统就采用了相关技术。

像线性一致性这样的正确性规范需要在两个维度间取得平衡：既要足够宽松以实现高效执行，又要足够严格以便为应用程序提供有用保证。"看似按照调用顺序执行操作"的要求过于严格，难以高效实现；而线性一致性的"看似在调用和响应之间的某个时间点执行"虽然对应用程序开发者不够直观，却是可实现的方案。



问：在哪些应用场景下，使用线性一致性比弱一致性模型更易于编程？

答：假设应用程序的某个模块计算出一个值，将其写入存储系统，随后设置标志位表明计算值已就绪：

v = compute...

put("value", v)

put("done", true)

在另一台计算机上，程序检查"done"标志判断数值是否可用，若可用则进行调用：

if get("done") == true:

v = get("value")

print v

若实现put()和get()的存储系统具备线性一致性，上述程序将按预期运行。

而对于许多弱一致性模型，上述程序可能无法达到预期效果。例如，提供"最终一致性"的存储系统可能会重排两个put操作的顺序（导致"done"已设为true而"value"尚未就绪），或者可能返回陈旧的get()查询结果。



问：现实中有哪些线性化存储系统的例子？弱一致性存储系统又有哪些？

答：谷歌的Spanner和亚马逊的S3是提供线性一致性的存储系统。

而谷歌的GFS、亚马逊的Dynamo以及Cassandra则提供较弱的一致性保障，它们通常被归类为最终一致性系统。

### 实现思路

#### KV system

按题意模拟即可，所有都给好了提示。非常简单。

1. 对 `server.go` 实现一个 map，key 是字符串，value 是结构体 (string, uint64 (rpc.TVersion) )，按提示 CRUD 和处理错误情况即可。用给定的 mutex 加锁。
2. 对 `client.go` 死循环调用 RPC，返回不是 ok 继续调用。对 Put，如果不是第一次调用而且返回 error，按题意特判即可

按题意失败就等100ms再试。不断试。

#### 锁

参数 l 是锁的名字，Lock 的 ck 是锁代表的客户端。多个客户端的锁名字一样的话，它们竞争这个锁。为了区分是谁可以占有和释放锁，键就是锁名，值就是这个客户端代表的随机字符串。由于不能删除 Key，用字符串 `0` 代表空锁，随机的字符串一定取不到 `0`。

循环过程，执行下面操作：

1. 取当前 key 为 l 的锁的值和版本(版本为了进行修改)
2. 取的过程不应该有任何错误，只可能是 OK 或 NOKEY；上面屏蔽掉了网络情况。如果是 NOKEY，设置版本为 0 (不设置也行)
3. 检查取出来的 value，如果不是空，也不是自己，那么无权操作，忙等睡眠 100ms
4. 否则，允许操作，如果是 Acquire，把 value 设置为自己的标识符；否则设置为空锁标识符，并退出循环；不然继续

> 难度不大，理解流程设计出来即可

## Lab3

### 任务描述

#### 概述

[lab3](http://nil.csail.mit.edu/6.5840/2025/labs/lab-raft1.html) 实现 Raft，一个复制状态机协议。复制服务通过存储完整副本到多个服务器达到错误容忍。它允许服务器崩溃的情况下继续服务，如崩溃或网络问题。在这种情况下，不同副本数据可能不一致。

Raft 组织客户端请求为序列，即 log，确保所有副本服务器(replica server)看到相同的 log。每个replica按log顺序执行请求，更新到本地服务器状态。所有replica都执行，得到一致的状态。如果服务器挂了，只需要更新log就可以恢复，只要大部分服务器alive，可以相互沟通，就可以恢复。但大部分挂了就只能等恢复了。

一个Raft实例集用RPC相互沟通，维护副本log。Raft交互支持无限序列的编号命令，即log entries。这些entries用index numbers编号，最终提交，随后Raft发送log entry。

依照 [论文](http://nil.csail.mit.edu/6.5840/2025/papers/raft-extended.pdf) 设计，特别是图2。需要实现大部分论文内容，如保存持续状态、节点重启后读。不需要实现级群成员关系改变（第六章）。

> 论文翻译(by GPT5)见其他 `.md`。

代码在 `src/raft/raft.go`，提供一系列测试 `src/raft/raft_test.go`。请自行测试 `-race`。可以在 `src/raft1` 执行 `go test`。

通过在 `raft/raft.go` 添加代码实现。必须支持下面的接口：

```go
// create a new Raft server instance:
rf := Make(peers, me, persister, applyCh)

// start agreement on a new log entry:
rf.Start(command interface{}) (index, term, isleader)

// ask a Raft for its current term, and whether it thinks it is leader
rf.GetState() (term, isLeader)

// each time a new entry is committed to the log, each Raft peer should send an ApplyMsg to the service (or tester).
type ApplyMsg
```

- 服务器调用 `Make` 来创建 Raft peer，参数是含自己的peer的网络标识符数组，`me` 是自己在 peer 数组里的下标。
- `Start` 请求 Raft 启动添加命令到replicated log。需要立刻返回，不等待添加完毕。期望发送 `ApplyMsg` 给每个新提交log entry导`applyCh`管道给`Make()`。
- 提供样例 `sendRequestVote()` 和 `RequestVote()` 例子 RPC。使用 `src/labrpc` 包，测试器可以模拟错误情况。只使用 RPC 进行 Raft 实例互动，不允许使用共享 Go 变量/文件沟通。

#### 领导选举

实现领导选举和心跳(`AppendEntries` RPC，无 log entries)，单领导选举，领导维持和接管。测试：`go test -run 3A`。

提示：

1. 不要自己运行，要用 test
2. 根据图2，完成收发 RequestVote，遵守选举规则和状态
3. 把选举的状态添加到 `Raft` 结构体 (`raft.go`)，定义结构体维护 log entry 信息
4. 补充 `RequestVoteArgs` 和 `RequestVoteReply` 结构体，修改 `Make()` 创建一个后台 goroutine，它负责在没收到 `RequestVote` 答复时开启选举
5. 心跳实现，`AppendEntries` RPC 结构体定义，定期发送。写 `AppendEntries` 处理方法
6. 测试要求 Raft 在旧领导挂了的5秒内选举出新领导
7. 文章 5.2 方法超时是 150-300ms，它有用只当领导发送心跳远比150ms频繁，如10ms。测试限制每秒10个心跳，所以必须用比150-300更大的时间，但不要太大，否则无法5秒通过
8. 使用 go [rand](https://golang.org/pkg/math/rand/)
9. 不要使用 `time.Timer / time.Ticker`，使用 [`time.Sleep()`](https://golang.org/pkg/time/#Sleep)，如 `ticker()` goroutine，为 `Make()` 创建。执行周期任务。
10. 选举全逻辑在图2贯穿多部分
11. 实现 `GetState()`
12. 调用 `rf.Kill()` 永远关闭一个实例，使用 `rf.killed()` 检测是否已调用，防止死 Raft 进程混淆
13. `labgob` 会检测是否 RPC 没用大写字段
14. 最难的是 debug，参考[debug技巧](http://nil.csail.mit.edu/6.5840/2025/labs/guidance.html)
15. 测试失败，会产生一个文件，可视化时间线表示事件，[例子]([Porcupine](http://nil.csail.mit.edu/6.5840/2025/labs/vis.html))。也可以自己添加注释，如 `tester.Annotate("Server 0", "short description", "details")`

每个通过包含五个信息：用时、Raft 数、RPC 发送数、RPC 消息总字节、提交总 log entries 数。可以帮助 debug。如果总用时超过 600s / 单个超过 120s，会测试失败。

#### log

测试 `go test -run 3B`。

提示：

1. log 1-indexed。但可以视为 0-indexed，即做一个 0 的 entry，允许 `AppendEntries` 的 `PrevLogIndex` 有效
2. 先通过 `TestBasicAgree3B()`，实现 `Start()`，用 `AppendEntries` 收发新 log entries，如图2，发送提交的 entry 在 `applyCh`
3. 实现选举限制，如论文 5.4.1
4. 循环检测事件，它们需要暂停如 10ms，或者使用 [`Cond`](https://golang.org/pkg/sync/#Cond) condition variables
5. 为后续任务顺利实现，看 guidance 查看开发和debug技巧
6. 查看 `raft_test.go` 检查究竟什么错误

检查实际上使用的 CPU 时间和实际时间，可以查看测试输出

```
go test -run 3B  1.37s user 0.74s system 4% cpu 48.865 total
```

user CPU 时是执行指令而不是等待/睡眠的时间，如果超过 1 分钟 real time，或 5 秒 CPU 时，可能出现问题

#### 持久化

Raft 重启，恢复，如论文图 2。

实际实现会把持久化状态写盘，该实验使用 `persister.go` 的 `Persister` 对象，调用 `Raft.Make()` 会提供该对象，获得最近的持续状态。用它的 `ReadRaftState(), Save()` 方法

实现 `persist(), readPersist()`，需要序列化/编码状态为字节数组，传给 `Persister`。用 `labgob` 编码器，查看注释。对 `Save()`，第二个参数传递 nil。当改变持久状态时，调用 `persist()`，通过 3C 测试。

可能需要优化，查看论文 P7-P8 (灰色线)，当超过一个 entry 一次时。一个可能是提供拒绝信息。

```
XTerm:  term in the conflicting entry (if any)
XIndex: index of first entry with that term (if any)
XLen:   log length
```

```
Case 1: leader doesn't have XTerm:
	nextIndex = XIndex
Case 2: leader has XTerm:
	nextIndex = (index of leader's last entry for XTerm) + 1
Case 3: follower's log is too short:
	nextIndex = XLen
```

提示：3A, 3B 的故障可能导致 3C 的错误。

可以做多次测试 `go test -run 3C`

#### 压缩

持久化存储快照，时不时存储，它丢弃先前在快照的 log entries，让重启更快，持久数据更少。论文第7章提供方案，当follower离leader很远，以至于leader丢弃了log entries，则leader必须发送快照+对应log。

要求提供序列化快照方法 `Snapshot(index int, snapshot []byte)`，测试会定期调用。后期的 Lab4 也会调用，快照包含完整的 KV 表。服务层每个 peer 都调用。

index 参数是最高 log index 的，Raft 要丢弃该点之前的log entries，修改代码，存储 log 尾部即可

实现 `InstallSnapshot` RPC，让 leader 给延迟的 peer 替换它的 snapshot 和 state，如图2；follower 接收到时，使用 `applyCh` 发送快照给 `ApplyMsg` 服务，该结构体定义了需要的字段，不应该让服务状态向后退

若服务器崩了，重启带持久化数据，要储存 state 和对应的 snapshot，用 `persister.Save()` 第二个参数存储快照，如果无快照，传 nil。

服务器重启，应用层读取 persisted 快照，恢复保存状态。

提示：

1. 存储 log 从某 index X 开始，初始化 X=0；让 `Snapshot(index)` 丢弃 index 之前的，然后令 X=index，则可以通过 3D
2. 常见WA的原因：follower 花费太长时间去跟上 leader
3. leader 没有需要的 log entries，就请求 `InstallSnapshot`
4. 单一 RPC 发送整个 snapshot，不要实现图13的offset机制分裂snapshot
5. Raft必须丢弃旧log entries，让Go CV可以复用内存，让它们没有引用指针即可
6. 没有 `-race`，所有测试 6 分钟以内，其中1分钟 CPU time。如果`-race`，测试10分钟以内，2分钟 CPU time。

通过所有测试

### 实现思路

#### 领导选举

根据图2，在 `Raft` 结构体(三类(红色标题)成员属性)。其中，① log entries 要新建结构体，Make 初始化一条 log 哨兵保证恒有 last (其他属性也对应初始化)；②补充身份state取值(follower, candidate, leader)；③补充选举超时计时器和重置办法；④ `applyCh` 成员属性接受 make 传入的参数；⑤ nextIndex 是要给第 i 个 follower 发送的下一条日志索引，抛开哨兵，所以初始化为 1 下标；matchIndex 是确认一致的最后一个索引。

> 其他注解：第i个followers就是peers里的，自己的下标是me。

对应 RPC 补充成员属性。对 `RequestVote`，整个函数加锁。如果参数任期比自己小，返回自己的任期，投反对票。如果比自己大，自己落后，清空自己的投票、计时，转为follower，保存。按图2检查逻辑，看看当前自己的日志、任期选举人更新，如果还没选且选举人更新，投他，改变自己的候选人，返回自己的任期。

对 `GetState` 直接返回，上②实现了状态。同样因为访问共享资源故加锁。

对 `ticker` 周期执行的循环，如果不是leader且超过DDL，发起选举；如果是leader，发送心跳包。

发起选举：

1. 自增任期，投自己(votedFor设置)，持久化，重置计时，用多线程给每个peer发送一个RPC (RequestVote)
2. 统计RPC结果，若RPC失败直接返回，否则若发现新任期也返回(会重置，刷新状态等)，如果任期/状态对不上了返回。如果还对得上，更新票数(可以通过管道在线程传递，这里直接传入指针函数参数)，如果能获胜，更新并完成一次成为 Leader 过程
3. 此过程，设置自己是Leader，设置nextIndex均为lastIndex+1，表示领导者打算从自己日志末尾之后的位置开始给该 follower 发送新条目，然后matchIndex都归零，表示刚成为领导者时还无法确定各 follower 已复制的日志长度，需要从头开始积累匹配进度。最后发送心跳包

心跳包：

- 如果是Leader，给每个peer发送`AppendEntries`RPC，入参是leader ID，term；返回应答 term 以及 success 状态。具体而言：如果被调自己任期比入参小，返回失败，如果自己任期小，更新自己任期。如果任期相等，自己是跟随者，重置计时。
- 发送结束后，如果返回的任期比自己的大，更新自己的任期等信息，不再是Leader

实现时注意对成员属性的读写都需要锁，注意避免死锁，不能lock了再lock。

#### log

实现 `Start` 更新一条记录，不是领导就退；否则，插入到 log，其中 command 认为是任意数据即可。更新自己的已确认最高日志索引，即当前 log 最后一个下标。更新下一次发给 follower 的索引，即 log 最后下标+1 (log长度)。这一段加锁，持久化。然后执行广播，给每个 peer 发包。其逻辑是：不是领导就解锁；否则，更新发心跳包的时间，然后多线程，每个线程给每个 peer 发送复制 log 的 `AppendEntries`。

广播 `AppendEntries` 具体为：拿锁，检查 Leader。把 `rf.nextIndex` 的范围纠正(>=1, <= len log，纠正是避免回退或状态变化导致更新了)，取出 nextIndex，-1 为 prevIndex，取出 prev 的任期。取出 nextIndex 和往后的全部 entries 复制出来(为了不持有锁)。然后构造参数(比3A有新增)，解锁，发 RPC。注意根据论文 P7-8 和作业后文提示，返回值可以比图2多两个属性： `ConflictIndex, ConflictTerm`。

如果调用成功，加锁，检查任期，如果任期比自己更新就更新自己任期。在自己的任期和领导者还保持时，如果返回是成功，把参数的 prevIndex 加上参数给他的日志数，如果这个更新才更新 matchIndex, nextIndex（因为网络可能有旧请求被 reply），然后更新 leader commit。如果返回不是成功，调整 next index。

- 更新 leader commit：检查 commitIndex 能否更新。从高往低看现在的 log 状态，如果日志状态和任期一样，且有半数 peer 的 matchIndex 都比这条 log 下标新，确认可以更新。

为了理解调整逻辑，先看 `AppendEntries` RPC 本体：加锁，在之前的基础上进一步修改。返回值添加三个，初始 `ConflictIndex=0, ConflictTerm=-1`。如果参数任期比自己小，根据 `reply.Term` 就会自己降级，不是 conflict 处理情况。任期相同，刷新选举计时器，状态还是 follower (刷新)。然后检查日志是否一致：

1. 如果自己的 log 的最后下标还没有传来的 prevlogindex 大，自己太短，返回 conflictingindex 为自己的 log 长度，让 leader 知道要退到这个地方。
2. 否则，看看自己存储的 prevlogindex log 的任期参数告诉的 prevlogterm 是不是一样，如果不是，这个位置任期冲突，返回自己存储这个位置的任期。找到这个任期第一次出现的日志下标，作为 conflictingindex。

都通过，现在可以，一条条看要更新的日志，直到看到末尾，或者日志任期和参数传来的新日志的任期不一样，就中断(那么自己日志不一样的地方开始删掉)，然后把自己的日志插入要更新的那一部分，持久化，返回成功。然后更新自己的 commit index。具体而言，如果参数传来的更新，看自己最后一条日志的下标，和传来的之间，谁小取谁，保证不会越界。

倒回来看上文的调整 next index：避免一次只回退一条日志，快速修正。

1. 任期不对，从自己的日志，找到最后一个索引的任期和冲突任期一样的，把 nextIndex 设置为该索引+1，返回。

2. 如果找不到这样的日志，不更新。此时直接更新 nextIndex 为参数指示的 index，返回

3. 还有一种情况，不成功，但没有 conflict，那么任期不对，follower 的任期更新。把 nextindex -1 (最小减到1)，这对应了不设置 conflict 的实现。

   在这种实现下，上面两个检查是否一致，if 主体里只设置 success=false。其他地方不变。(但是还没做消融实验，这个暂时没成功pass，但时间差不多)

心跳包逻辑重写，直接调用广播 `AppendEntries`。

> 在 `becomeLeader`，把自己的 `matchIndex` 改成 lastIndex。使得统计多数时，自己也计数 (也可以不设置，就像上文一样，遍历时跳过自己，初始为1即可)

像 `ticker()` 一样，加一个一直运行的多线程函数，`lastApplied` 表示真正执行的 command 记录，通过半数之后，leader/follower把确认命令发送给上层，滑动指针跟踪没到 `commitIndex` 的部分。填入对应参数。把这个参数用管道传送出去，管道过程不要加锁，其他时候加锁。如果有一个分布式的KV服务器，每个主机各自维护一个KV，主机各自是一个raft peer，这里是raft发送给自己kv服务器，从而在上一层确保一致性。

#### 持久化

图2 State的persistent三项要存储。按照注释提示做读写(可以接受Err)。总结来说，在整个raft，所有修改了这三个字段的地方，都需要persist()，之前已经完成。

#### 压缩

快照：上层(如KV服务器)的某个确定性状态，对应某一段日志，上层定下来了，这里Raft就可以丢弃了。传入snapshot的index是假设从不截断的全局索引。添加两个成员属性：快照(最后被压)的全局index和term。然后把之前的逻辑都更新为局部index。所以需要根据last+log来透明，让上述部分屏蔽这个细节，假装没有被截断过。因此，last也需要持久化。之前的所有index现在都代表没被截断的下标，设计转换函数，修改全局逻辑。log标兵节点存储lastindex的term。

根据进度不同，调用snapshot的时间可能不一样，但每个peer使用一致的index和snapshot数据。那么，如果leader压了log，而follower需要这些log时，leader不发送log，发送snapshot，follower也不会恢复这个丢掉的log，从leader有的log开始，没有的话，直接把snapshot丢给上层。所以需要 `InstallSnapshot`。

给 peer 加一个 snapshot 成员属性。调用 persist 时，把快照也放进去 Save。在读时，把它载入。对 Snapshot() 函数，更新检查每个 index 不能低于 last 等。注意参数里的 snapshot 要拷贝使用，否则，其他线程还可以修改 snapshot，共享不安全。

给peer发送log的函数，若nextIndex比日记记录的在前，如果没有snapshot，纠正nextindex；如果有，发送 `InstallSnapshot`。构造参数：当前任期、leader id、两个 last，以及对 snapshot 的拷贝，可以解锁，发送完RPC再加锁。返回值只有 term，如果比自己大，更新自己，如果自己已经不是leader或者任期不对，解锁。否则，更新match/next index。

然后是 `InstallSnapshot`，如果参数任期小不理，大于则更新自己。自己刷为 follower，更新计时器，任期。如果参数的 last 旧，丢弃不管。否则，再复制 RPC 的 snapshot，防止 RPC 框架等重用内存。如果快照在日志数组范围内，任期一样，保留日志剩余的尾巴，如果日志冲突或被快照阶段，初始化日志。更新last，快照，index，持久化。然后发送消息通知上层，让上层加载快照。看 `raftapi.go` 看格式，二选一结构体里有效成分。

## Lab4

### 任务描述

#### 概述

客户端如 Lab2，然后服务器变成 Raft 集群，实现[如图所示](http://nil.csail.mit.edu/6.5840/2025/figs/kvraft.pdf) 的交互，客户端 Clients 与 KV 服务器通过 Clerk 通讯(如Lab2)，后者实现 Put, Get，可线性化。对分布式，这要求所有服务器选择相同的执行顺序(对并发请求)，避免用不是最新的状态回应客户端，在错误中恢复。要求实现一个复制状态机包(rsm)，使用 raft。基于它实现 KV 系统，然后使用快照。也可以参考 Chubby, Paxos Made Live, Spanner, Zookeeper, Harp, Viewstamped Replication。

在 `src/kvraft1/`，服务器必须实现 `StateMachine` 接口，提供服务无知状态（即对请求内容/类型保持通用、不可知）。调整 `client.go, server.go`，可以复用部分 lab2 代码。

#### RSM

replicated state machine (RSM) ，在 `src/kvraft1/rsm` 里做 `go test -v`。C/S 与 Raft 通过两种方式交互：leader 提交客户端操作，调用 `raft.Start()`，其他服务副本通过 `applyCh` 接受提交的操作。两类方式会交织。`rsm` 封装上述交互过程，是服务与 Raft 之间的一层，实现 reader goroutine，读取 `applyCh`，`rsm.Submit()` 函数，为客户端操作调用 `raft.Start()`，等待reader 把结果交付。使用 `rsm` 的服务是 `StateMachine` 对象，提供 `DoOp()` 方法，已提交的操作会交给该方法，返回值给 `rsm.Submit()`。注意检查类型结果一致。

每个客户端操作传给 `rsm.Submit()`，把操作包装为 `Op` 结构体+唯一标识符，等待操作被提交执行完成，返回执行结果(`DoOp()` 返回值)。如果当前节点不是 leader，`Submit()` 返回 `rpc.ErrWrongLeader`。如果调用完 `Submit()`，然后 leader 变化，操作丢失永远不提交，也要检测处理。

本节提交的操作是单整数自增。客户端请求事件序列为：

1. 客户端像服务leader发送请求
2. 服务leader用请求调用`rsm.Submit()`
3. `rsm.Submit()`使用该请求调用`raft.Start()`，进入等待
4. Raft提交改请求，像所有节点`applyCh`发送该请求
5. 各节点的rsm reader goroutine从`applyCn`读取该请求，交给服务`DoOp()`
6. leader上，rsm reader goroutine把`DoOp()`返回值交给最初提交请求的`Submit()`goroutine，后者返回该值

各个服务器不应直接相互通信；它们之间只能通过 Raft 来交互。通过 `go test -v -run 4A`。提示：

- 不需要给 Raft `ApplyMsg` 或 RPC 添加字段
- 需要处理：某 rsm leader 通过 `Submit()` 调用了 `Start()`，但该请求提交到日志前，失去 leader。解决办法：rsm 检测自己失去领导权，或在 `Start()` 返回的 index 上出现不同请求，让它返回错误。如果 leader 被单独分区隔离，不知道新的leader，且同一分区客户端无法与新leader通信，服务器一直等待直到网络分区恢复也是可以的。
- 关闭peer时调用`rf.Kill()`，关闭`applyCh`，让rsm知道该节点正在关闭，退出循环。

#### KV服务

使用 rsm 实现 K/V服务器，每个服务器 `kvservers` 关联一个 rsm/Raft 节点peer。客户端 Clerk 把 put, get 这两个 RPC 发给关联的 leader kvservers，提交，复制，每个节点调用 DoOp，应用到对应节点数据库。

Clerk 不知道哪个是 leader 的话，如果发错了或无法连接，发送给另一个 kvserver 重试。若提交了日志(即也应用到了状态机)，RPC 结果返回给 clerk，若没能提交，返回错误，换一个服务器重试。kvservers 不能相互通信，通过 raft 间接交互。

可以复用 Lab2 代码，在无故障环境实现 KV 服务器。`server.go` 实现 RPC 处理函数，执行状态机应用，DoOp。

kvserver 不应该在它不属于多数派（majority）时完成一个 Get() RPC（以避免返回陈旧数据）。一个简单做法是把每个 Get()（以及每个 Put()）都通过 Submit() 写入 Raft 日志。不需要实现第 8 节里描述的只读操作优化。最好一开始就加锁。

然后在有故障环境可以完成任务，Clerk 可能必须多次发送 RPC，直到找到一个能正向回复的 kvserver。如果 leader 在刚把一条日志提交进 Raft 日志后就失败了，Clerk 可能收不到回复，于是会把请求重新发送给另一个 leader。每次调用 Clerk.Put()，对于某个特定的版本号（version number），都应当只执行一次。添加处理故障的代码，比如重试，RPC丢失返回 ErrMaybe。

回忆一下：rsm 的 leader 可能会失去领导权，并在 Submit() 中返回 rpc.ErrWrongLeader。遇到这种情况，应安排 Clerk 把请求重新发送给其他服务器，直到找到新的 leader。很可能需要修改 Clerk，让它记住上一次 RPC 发现的 leader 是哪个服务器，并在下一次 RPC 时优先发给它。这样可以避免每个 RPC 都花时间重新搜索 leader，可能有助于更快通过一些测试

每个 “Passed” 后面的数字依次表示：实际耗时（秒）、节点数、发送的 RPC 数（包括客户端 RPC）、以及执行的键/值操作数（Clerk 的 Get/Put 调用）。

#### 快照

修改 kvserver 和 rsm，Raft Snapshot() 来节省日志空间并减少重启时间。测试程序会把 maxraftstate 传给 StartKVServer()，StartKVServer() 再把它传给 rsm。maxraftstate 表示持久化的 Raft 状态允许的最大大小（单位字节，包含日志，但不包含快照）。应将 maxraftstate 与 rf.PersistBytes() 进行比较。每当 rsm 检测到 Raft 状态大小接近该阈值时，就应该通过调用 Raft 的 Snapshot 来保存一个快照。rsm 可以通过调用 StateMachine 接口的 Snapshot 方法来获取 kvserver 的快照，从而创建该快照。如果 maxraftstate 为 -1，则不必做快照。maxraftstate 的限制作用于的 Raft 传给 persister.Save() 的第一个参数的、经 GOB 编码后的字节数。

在 tester1/persister.go 中找到 persister 对象的源码。rsm 要能检测到持久化的 Raft 状态增长得过大，然后把一个快照交给 Raft。rsm 服务器重启时，应使用 persister.ReadSnapshot() 读取快照；如果快照长度大于 0，则把快照传给 StateMachine 的 Restore() 方法。若能通过 rsm 中的 TestSnapshot4C，就完成了该任务。

思考 rsm 应该在什么时候对其状态做快照，以及快照中除了服务器状态之外还应包含什么。Raft 会使用 Save() 将每个快照与对应的 Raft 状态一起存入 persister 对象。可以用 ReadSnapshot() 读取最新存储的快照。将存入快照的结构体字段全部改为大写开头（导出字段）。实现 kvraft1/server.go 中的 Snapshot() 和 Restore() 方法（由 rsm 调用）。修改 rsm 以处理 applyCh 中包含快照的消息。

完成 Lab 4 测试的合理时间是：真实时间 400 秒、CPU 时间 700 秒。

### 实现思路

注意 lab3 的 raft 的 kill 方法，需要补充 `close(rf.applyCh)`。与此同时，添加一个 closeOnce sync.Once，用来关闭通道，确保只关闭一次。注意，在 applier，installsnapshot 检查 killed 就 return。

把 `replicateLogToPeer` 的死循环去掉，只执行一次。如果失败了，让下一次 Heartbeat/Start 触发的 broadcastAppendEntries 来重试，如若不然，更容易无法连续通过 test，go routine过多。

> 注意，千万不要用 `go test -count=3` 等办法多次测试，要多次测试要自己手动执行多次。例如我写的 `test-lab3.sh` 可以手动跑。

此外，选举超时时间延长为600+rand300；心跳缩短为100ms，ticker缩短为50+rand100。有助于之前的3D更快，且3B更稳定，避免经常冲突。

对 rsm，主要逻辑是如下：

Op 结构体是 Req any + id int64。后者初始化为 1，是 rsm 成员属性 `nextOpId` 控制，每次赋值自增。

新增结构体 `OpResult`，存储 reply any 和错误信息 err。

新增结构体 `waitingOp`，表示当前某个等待提交的 Op 操作，为了确保一致性，需要存储 opId, term，以及一个 `OpResult` 通道。在 rsm 成员属性添加 `waiting map[int]*waitingOp`，对每个 raft index的操作，挂上等待 op。

rsm 其他成员：dead (bool/int32)，判断服务器是否已经关闭。以及 `lastApplied`，上次应用的索引，如果 submit 的 id 比它小就丢弃，避免重放。

对 Submit 函数，如果 dead，返回 ErrWrongLeader。否则，加锁，构造 Op，调用 Start，其返回值如果说自己不是 leader，返回 ErrWrongLeader，然后，构造一个缓冲区大小为1的OpResult chan，把它填入 waiting，解锁。之后，等待读取这个chan的内容，读到了就返回；没读到的时候，隔100ms看看，是不是dead了，leader，term是不是变了，若是，删掉这个 waiting 里的项。

对 Kill 函数，原子设置 dead，调用 rf 的 kill。

一个 reader() 线程一直在跑，看 applyCh 的消息，根据消息类型调用。如果是 command，做其调用：如果在 lastApplied 内，把 command 转为 op，如果失败，报错返回塞入 replyCh，删除 waiting。否则，执行 DoOp 得到 reply，检查 leader, opid, term，如果都对，返回 reply 结果和 OK，否则，返回 ErrWrongLeader，不论如何，都在 waiting 里删掉它。

> 对快照来说，检查现在的持久化字节如果超过了，做快照。对快照命令，只有快照索引大于 lastApplied 才做快照，做的话，调用 sm，更新 lastApplied，清楚 waiting 里 index 小的。

在 reader() 结束时，设置 dead，把每个 waiting 里的都发送 err，然后删除。

#### KV服务

对 `server.go`，在 get/put 方法只需要做适配器转换参数什么的和检查即可。核心是后面调用 doOp，这时候把 lab2 的 get/post 选择去做。然后 snapshot 把哈希表持久化，restore 载入这个哈希表，做点检查即可。注意无需加锁，因为rsm已经有mu了。

对 `client.go`，在原有逻辑的基础上，维护上一次leader是谁，先从leader开始做取模轮询一次，如果RPC不成功或者不是leader，找下一个，否则，按原本逻辑处理。如果轮询一次都没有结果，那就sleep100ms再来。对get/put逻辑都一样，put记得第一次的errver和后面特判，因为重试后得到errver有可能是第一次成功了所以第二次err，也有可能第一次没成功，无法确定。

#### 快照

at-most-once要求不只幂等，还要防重，同一条指令不能被执行2次。在单线程put是幂等的。在多线程，如果两个客户端分别对同一版本的key，发送不同的value修改，存在先后顺序，A先发B后发，理论上是B的值。但是如果重试errmaybe，幂等不防重，允许A再执行一次的话，就会修改为A的值。所以put操作不能重放。即只要多客户端，不管服务端是不是集群，都要用防重才能保证put正确。

线性化是最强一致性模型，让分布式集群在外部看来和单核一份内存数据一样。要求所有客户端看到的顺序一致，并且符合真实时间上的前后关系。Raft 复制日志保证了操作在集群中以同一顺序提交，再配合 at-most-once，就能对外提供线性化的 k/v 服务。

单个 client 本身串行，不会并发get/put。故给它分配随机整数做 id，自增整数做请求序号，修改 RPC 参数传入这俩。因为串行，服务器只需要记录每个 client 最后一个请求序号以及结果。

对 server，在得到 raft 提交该 command 后，即 doOp 进行防重。也就是在真的决定要修改状态机的前一刻检测。用两个map，记录 client 最后的 requestId 和 reply，储存、更新、返回。主要不要用 any，无法持久化，改成像 RPC 一样用 bool switch 的。

持久化的具体调用复用 lab3 的 persist 即可。在 `labgob.Register` 加上 reply 等。

bugs修复：① 对 `rsm.go`，构造函数时，如果presister不是空切它读出来的snapshot有长度，做恢复快照。②对 `raft.go`，所有往 applyCh 写消息的操作，封装为函数，defer 一个忽略 panic 的 recover。

## Lab5

### 任务描述

#### 概述

[lab5](http://nil.csail.mit.edu/6.5840/2025/labs/lab-shard1.html) / [自选](http://nil.csail.mit.edu/6.5840/2025/project.html)

选做下面内容或自选。构建分片shard/分区到一组基于raft复制的KV分片组shardgrps。分片是键值对子集，如 `a*` 开头的键。每个分片组只处理一部分分片请求。如下图，分片组是蓝色方形，存储带键分片，clerk从kvsrc获取配置(lab2实现)，配置是分片到分片组的映射。管理员控制器添加移除分片，向分片组发起RPC(三种)并更新配置。

![shardkv design](http://nil.csail.mit.edu/6.5840/2025/labs/shardkv.png)

分片组之间转移分片，移动实现平衡负载，以及分片的上下线。需要确保线性一致性linearizability，分片到分片组配置变化时，或修改配置/分区时进行回复仍要保证。具体而言：

1. `ChangeConfigTo` 移动分片，可能客户端还在用旧分片组，有的用新的，导致破坏一致性，需要任何时候最多一个分片组提供服务
2. `ChangeConfigTo` 配置失败，正在移动的分片变得不可访问，让新的控制器完成之前的重新配置
3. 配置：分片到分片组的分配。这与 Raft 集群成员变更不同。不需要实现 Raft 集群成员变更

一个分片组服务器（server）仅是一个分片组的成员。给定分片组中的服务器集合永远不会改变

- part A：`shardctrler` 在 `krsrv` 存储和检索配置，实现 `shardgrp` 和对应 clerk，通信来移动分片
- part B：修改 `shardctrler` 处理配置更改期间的故障和网络分区
- part C：扩展 `shardctrler` 以允许并发控制器运行而不相互干扰
- part D：扩展解决方案

遵循与 Flat Datacenter Storage, BigTable, Spanner, FAWN, Apache HBase, Rosebud, Spinnaker 等系统相同的一般设计。基于 lab2,4.

`src/shardkv1` 中提供了测试和骨架代码：

- `client.go`：shardkv 的 Clerk。
- `shardcfg` 包：用于计算分片配置。
- `shardgrp` 包：用于 shardgrp 的 Clerk 和 server。
- `shardctrler` 包：包含 `shardctrler.go`，为控制器提供变更配置（`ChangeConfigTo`）和获取配置（`Query`）的方法

#### 移动分片

在没有故障的情况下实现 `shardgrps` 以及 `InitConfig`、`Query` 和 `ChangeConfigTo` 方法。在 `shardkv1/shardcfg` 中为提供了描述配置的代码。每个 `shardcfg.ShardConfig` 都有一个唯一的识别号、一个从分片编号到组编号的映射，以及一个从组编号到复制该组的服务器列表的映射。通常分片数会多于组数（因此每个组服务多个分片），以便可以以相当细的粒度转移负载

任务一：实现 `InitConfig` 和 `Query`。

在 `shardctrler/shardctrler.go` 中实现这两个方法：

- `InitConfig` 方法接收第一个配置（由测试程序作为 `shardcfg.ShardConfig` 传入）。`InitConfig` 应该将配置存储在 Lab 2 的 `kvsrv` 实例中。
- `Query` 方法返回当前配置；它应该从 `kvsrv` 读取配置（该配置之前由 `InitConfig` 存储）。

实现 `InitConfig` 和 `Query`，并将配置存储在 `kvsrv` 中。当代码通过第一个测试时，即表示完成。注意此任务不需要任何 `shardgrps`

使用 `ShardCtrler.IKVClerk` 的 `Get`/`Put` 方法与 `kvsrv` 通信来实现 `InitConfig` 和 `Query` 的存储和读取。使用 `ShardConfig` 的 `String` 方法将 `ShardConfig` 转换为可以传递给 `Put` 的字符串，并使用 `shardcfg.FromString()` 函数将字符串转换回 `ShardConfig`。

任务二：实现 `shardgrp` 和 Clerk

在 `shardkv1/shardgrp/server.go` 中实现 `shardgrp` 的初始版本，并在 `shardkv1/shardgrp/client.go` 中实现相应的 Clerk，可以通过复制 Lab 4 的 `kvraft` 解决方案代码来完成。

在 `shardkv1/client.go` 中实现一个 Clerk，该 Clerk 使用 `Query` 方法查找键对应的分片组，然后与该分片组通信。当代码通过 `Static` 测试时(`go test -run Static`)，即表示完成。

- `shardkv1/client.go` 中的代码为整个系统提供了 Put/Get Clerk：它通过调用 `Query` 方法查明哪个分片组持有目标键的分片，然后与该持有分片的 `shardgrp` 通信
- 使用 `shardcfg.Key2Shard()` 查找键的分片编号。测试程序会将一个 `ShardCtrler` 对象传递给 `shardkv1/client.go` 中的 `MakeClerk`。使用 `Query` 方法检索当前配置
- 向分片组 put/get 一个键，`shardkv` Clerk 应该通过调用 `shardgrp.MakeClerk` 为该分片组创建一个 `shardgrp` Clerk，传入配置中找到的服务器和 `shardkv` Clerk 的 `ck.clnt`。使用 `ShardConfig` 的 `GidServers()` 方法获取分片的组信息
- `Put` 必须在回复可能丢失时返回 `ErrMaybe`，但此 `Put` 是调用 `shardgrp` 的 `Put` 来与特定分片组通信。内部的 `Put` 可以用错误来信号这一点
- 创建时，第一个分片组（`shardcfg.Gid1`）应初始化为拥有所有分片

任务三：实现 `ChangeConfigTo` 及分片移动

来支持组间的分片移动，该方法将旧配置变更为新配置。新配置可能包含旧配置中不存在的新分片组，也可能排除旧配置中存在的分片组。控制器应该移动分片（键/值数据），以便每个分片组存储的分片集合与新配置匹配

首先在源分片组“冻结（freeze）”分片，导致该分片组拒绝针对正在移动的分片中键的 `Put` 请求。然后，将分片复制（安装，install）到目标分片组；之后删除被冻结的分片。最后，发布新配置，以便客户端可以找到移动后的分片。这种方法的优点在于避免了分片组之间的任何直接交互。它还支持在配置更改期间继续服务未受影响的分片

为了能够对配置更改进行排序，每个配置都有一个唯一的编号 `Num`（见 `shardcfg/shardcfg.go`）。Part A 的测试程序按顺序调用 `ChangeConfigTo`，传递给 `ChangeConfigTo` 的配置的 `Num` 将比前一个大 1；因此，`Num` 较高的配置比较低的更新

网络可能会延迟 RPC，且 RPC 可能会乱序到达分片组。为了拒绝旧的 `FreezeShard`、`InstallShard` 和 `DeleteShard` RPC，它们应该包含 `Num`（见 `shardgrp/shardrpc/shardrpc.go`），分片组必须记住它们为每个分片看到的最大的 `Num`

在 `shardctrler/shardctrler.go` 中实现 `ChangeConfigTo`，并扩展 `shardgrp` 以支持冻结、安装和删除。在 Part A 中，`ChangeConfigTo` 应该总是成功，因为测试程序在此部分不会引发故障。需要使用 `shardgrp/shardrpc` 包中的 RPC，在 `shardgrp/client.go` 和 `shardgrp/server.go` 中实现 `FreezeShard`、`InstallShard` 和 `DeleteShard`，并根据 `Num` 拒绝旧的 RPC。还需要修改 `shardkv1/client.go` 中的 `shardkv` Clerk 以处理 `ErrWrongGroup`，如果分片组不负责该分片，它应该返回此错误。

当通过 `JoinBasic` 和 `DeleteBasic` 测试时，即完成了此任务

- 如果是针对分片组不负责的键（即该键的分片未分配给该分片组）进行的 Client `Put`/`Get`，分片组应响应 `ErrWrongGroup` 错误。必须修改 `shardkv1/client.go` 以重新读取配置并重试 `Put`/`Get`

- 必须通过 `rsm` 包来运行 `FreezeShard`、`InstallShard` 和 `DeleteShard`，就像 `Put` 和 `Get` 一样

- 可以在 RPC 请求或回复中发送整个 map 作为状态，这有助于保持分片传输代码的简单

  RPC 处理程序之一在回复中包含了一个 map（例如键/值 map），而该 map 是服务器状态的一部分，可能会因竞争条件导致 bug。RPC 系统必须读取 map 以将其发送给调用者，但它并未持有覆盖该 map 的锁。然而，服务器可能会在 RPC 系统读取 map 的同时修改同一个 map。解决方案是 RPC 处理程序在回复中包含 map 的副本

任务四：扩展 `ChangeConfigTo` 以处理离开的分片组；即，存在于当前配置中但不存在于新配置中的分片组。必须继续服务那些未受正在进行的配置更改影响的分片。通过 `TestJoinLeaveBasic5A`。通过所有 Part A 测试 `go test -run 5A`。

#### 处理失效控制器

控制器是一个短生命周期的命令，由管理员调用：它移动分片然后退出。但是，它可能会在移动分片时失败或失去网络连接。从未能完成 `ChangeConfigTo` 的控制器中恢复。测试程序在对第一个控制器进行分区后启动一个新的控制器并调用其 `ChangeConfigTo`；需要修改控制器，以便新的控制器完成重新配置。测试程序在启动控制器时会调用 `InitController`；可以修改该函数以检查是否需要完成已中断的配置更改。

允许控制器完成前一个控制器启动的重新配置的一个好方法是保留两个配置：当前配置和下一个配置，都存储在控制器的 `kvsrv` 中。当控制器开始重新配置时，它存储下一个配置。一旦控制器完成重新配置，它将下一个配置变为当前配置。修改 `InitController`，首先检查是否存在具有比当前配置编号更高的已存储的下一个配置，如果是，则完成必要的移动分片操作以重新配置到下一个配置

测试程序在启动控制器时调用 `InitController`；可以在 `shardctrler/shardctrler.go` 中的该方法里实现恢复

接手失败控制器工作的控制器可能会重复 `FreezeShard`、`InstallShard` 和 `Delete` RPC；分片组可以使用 `Num` 来检测重复并拒绝它们。通过 Part B 测试

#### 并发配置更改

当控制器崩溃或被分区时，测试程序将启动一个新的控制器，该控制器必须完成旧控制器可能正在进行的任何工作。几个控制器可能会并发运行，并将 RPC 发送给分片组和存储配置的 `kvsrv`。

确保只有一个控制器更新下一个配置，以避免两个控制器（例如，一个被分区的和一个新的）在“下一个配置”中放入不同的配置。测试程序并发运行多个控制器，每个控制器通过读取当前配置并针对离开或加入的分片组进行更新来计算下一个配置，然后测试程序调用 `ChangeConfigTo`；因此，多个控制器可能会调用带有相同 `Num` 但不同配置的 `ChangeConfigTo`。可以使用键的版本号和版本化 Put 来确保只有一个控制器更新下一个配置，并且其他调用在不执行任何操作的情况下返回

修改控制器，使得对于一个配置 `Num`，只有一个控制器可以发布下一个配置。测试程序将启动许多控制器，但应该只有一个针对新配置启动 `ChangeConfigTo`

查看 `test.go` 中的 `concurCtrler` 以了解测试程序如何并发运行控制器。`go test -run Partition` 测试

> 将把旧控制器的恢复与新控制器结合起来：新控制器应该执行 Part B 中的恢复。如果旧控制器在 `ChangeConfigTo` 期间被分区，必须确保旧控制器不会干扰新控制器。如果所有的控制器更新都已经通过 Part B 的 `Num` 检查正确隔离，不必编写额外代码

```sh
go test ./raft1
go test ./kvraft1
go test ./shardkv1
```

#### 扩展

选择实现的任何扩展编写自己的测试。文件 `extension.md` 中写一段描述扩展。一些思路：

1. easy。更改测试程序使用 `kvraft` 代替 `kvsrv`（即，在 `test.go` 的 `MakeTestMaxRaft` 中替换 `kvsrv.StartKVServer` 为 `kvraft.StartKVServer`），以便控制器使用 `kvraft` 来存储其配置。编写一个测试，检查当其中一个 `kvraft` peer 宕机时，控制器是否可以查询和更新配置。测试程序的现有代码分布在 `src/kvtest1`、`src/shardkv1` 和 `src/tester1` 中
2. mid。更改 `kvsrv` 以实现像去年 Lab 2 一样的 Put/Get 的“一次且仅一次（exactly-once）”语义（参见丢弃消息部分）。可以移植 2024 年的一些测试，而不必从头开始编写。在 `kvraft` 中也实现“一次且仅一次”
3. mid。更改 `kvsrv` 以支持 `Range` 函数，该函数返回从低键到高键范围内的所有键。实现 `Range` 的懒惰方法是遍历服务器维护的键/值 map；更好的方法是使用支持范围搜索的数据结构（例如 B 树）。包含一个使懒惰解决方案失败但通过更好解决方案的测试
4. hard。`kvraft` 实现，允许领导者（Leader）在不通过 `rsm` 运行 Get 的情况下服务 Get 请求。也就是说，实现 Raft 论文第 8 节末尾描述的优化，包括租约（leases），以确保 `kvraft` 保持线性一致性。实现应该通过现有的 `kvraft` 测试。还应该添加一个测试来检查优化实现是否更快（例如，通过比较 RPC 数量），以及一个测试来检查任期切换是否更慢，因为新领导者必须等到租约过期
5. hard。在 `kvraft` 中支持事务，以便开发人员可以原子地执行多个 Put 和 Get。一旦有了事务，就不再需要版本化 Put；事务包含版本化 Put。查看 etcd 的事务以获取示例接口。编写测试以证明扩展有效
6. hard。修改 `shardkv` 以支持事务，以便开发人员可以跨分片原子地执行多个 Put 和 Get。这需要实现两阶段提交（2PC）和两阶段锁定（2PL）。编写测试以证明扩展有效

> #### 项目

> 代替lab5的自由项目。包括但不限于：
>
> - 重现 6.5840 课程论文中讨论过的某个系统。
> - 构建一个高性能的 Raft 实现，根据需要更改设计。
> - 构建一个分布式、去中心化、容错的 Reddit。
> - 构建一个系统使 Node.js 应用程序具备容错能力，可能使用某种形式的复制执行（replicated execution）。
> - 为 Lab 5 添加跨分片原子事务（cross-shard atomic transactions），使用两阶段提交（2PC）和/或快照。
> - 构建一个数据流处理系统，风格类似 Google FlumeJava、Spark 或 Naiad。
> - 构建一个具有异步复制功能的系统（像 Dynamo、Ficus 或 Bayou）。也许可以添加更强的一致性（如 COPS、Walter 或 Lynx）。
> - 构建一个文件同步器（像 Unison 或 Tra）。
> - 构建一个供网站使用的连贯缓存系统（类似于 memcached），也许可以参考 TxCache 的思路。
> - 构建一个分布式协作 Web 缓存，也许参考 Firecoral 或 Maygh。
> - 构建一个像 EtherPad 那样的协作编辑器，使用最终一致性或 CRDT 原语。
> - 利用区块链技术构建除加密货币以外的应用。
> - 构建一个容错和/或分片的文件服务。
> - 构建一个分布式共享内存（DSM）系统，使得原本为单台多核机器设计的并行代码可以运行在机器集群上。
> - 构建一个分布式块存储系统，风格类似 Amazon EBS 或 FAB。也许可以让标准操作系统通过 iSCSI 或 Linux 的 NBD（网络块设备）与网络虚拟磁盘通信。
> - 构建一个地理复制（geo-replicated）存储系统，像 Dynamo 或 COPS，也许在事务或一致性方面提供一些有用和/或高效的功能。
> - 利用现代高速网卡特性（如 RDMA 或 DPDK）构建高速服务，也许带有复制或事务功能。
> - 利用现代快速非易失性存储（如 Intel Optane）来简化容错系统的设计。
> - 构建一个比 Raft 更容易在其上分层构建服务代码的容错框架。
> - 研究应用程序是否真的需要严格一致的存储，或者不得不使用弱一致性存储带来的应用复杂性成本是多少，并得出一些有用的结论。
> - 构建一个既擅长大数据（像 MapReduce 和 Spark）又擅长在线处理（像键/值存储或 SQL 数据库）的数据处理系统

### 实现思路

#### 移动分片

对 `InitConfig` 和 `Query`，做一个常量 key 啥都行，配置版本号num初始为0，不用get，只调用一次(init)，然后读写 KV 即可。已经有序列化了。

> 梳理：
>
> 整个 kvserver 分成了若干片(nshards=12，在 shardcfg 里)，每片有一个 tshid 标识它是哪篇，通过对 key  哈希%nshards确定key存储在哪个片(shard)。
>
> 分片组shardgroup管理一到多个分片，在 `ShardConfig` 的 shards 看到关系。组Id用tgid标识。t是type。一个组有一到多个kvserver。用groups记录，存储服务器名字 `server-{gid}-{i}`，其中i是me，0-indexed连续，而gid可以不连续。
>
> server.go 里me是当前服务器在组里的编号。它负责的分片与它所在group负责的一样。

`server.go` 在复用 lab4 的基础上。添加 shards，即复制它组的维护的shards，如果组为1，默认全true (snapshot之前)。在 doOp 做了 at-most-once，没做操作之前，添加对 shard 的检查，如果 key 对应的不是自己负责的返回 err。注意 num, shards 要持久化。否则恢复后不知道自己负责什么分片，以及是否加载过配置。集群架构组成不变，只是每组负责的分片会变，所以gid和me不变且不用持久化。

`shardgrp/client.go` 是与 group 交互的 clerk。外层 `shardkv1` 使用它(Clerk)。类比lab4，一个内层连接该组全部服务器。外层需要根据key选组创建该组的Clerk，然后做操作并返回。如果错误gid,wronggroup重试。此时可以通过 Static。注意要缓存新建的clerk，否则每次操作requestId都新建为1是错的。没新建过新建，新建过就直接用。

> 类比lab4，流程一样都是：client调用clerk，向group下其中一个服务器kvserver提交指令(get/put/freeze等)，指令通过单个rsm，经由raft达成多数一致后，每个raft用管道传递消息给rsm，让每个rsm调用每个kvserver的DoOp，于是同一group下的每个服务器kvserver都执行了一致性操作。也就是说，同组的全部服务器，要么同时生效，要么同时不生效，跟get/put一样，不能某个kvserver自己独立执行freeze/install等

因此，它们也需要做 at-most-once 上层状态机记忆和持久化。并且同理实现RPC等经典流程。所以也要RPC加clientId等。并且snapshot快照需要当前分片的全部KV对以及last result。因为和get/put一样，所以也需要存储last result。把原本的单一kv表和单一last results，改成分片tshid分组存储的，这样方便分片快照。用map存每一片数据指针因为不一定有全部分片。

对服务端：

1. 冻结：验证 num 是新的。如果不是，直接返回数据(数据和at-most-once的lastResult)，遍历所有数据取当前 shard 的然后序列化。如果是，标记正在冻结，更新 num，然后返回。
2. 安装：取出每个分片的数据(如上)并解码赋值上。
3. 删除：把分片数据删掉并返回。

对客户端：构造相应的请求 RPC 发送并接受结果。

`ChangeConfigTo` 任务，遍历每个需要移动的分片，新建 clerk (开销很小，直接做临时的即可)，连接旧服务器，冻结获取shard数据，在新服务器安装shard数据，然后删除旧服务器数据 (冻结->安装->删除，必须按这个顺序)。如果更新成功就更新配置和num写入。任意阶段失败，尝试重新执行任务。具体而言，freeze 失败没事，install失败，进入下一轮再尝试，每个shard必须install后才能删除，保证不需要回滚。

> 任务4：离开的分片组，指的是某一组服务器被移除，不再负责任何shard，让某group离开集群，把它负责的shard分配给剩余/新加入的group。

给 Clerk 添加超时500ms，否则如果网络/leader 挂了，RPC 根本发不出去，shardkv Clerk 就卡在某个 group 的无限重试里，看不到后续配置更新。设一个有限超时，超时后直接返回 ErrWrongGroup（或错误）给 Client，这样让上层去调整配置。

把 clientId, requestId 移到上层 Client，于是 Clerk 可以调用时根据调用者选择不同的 clientId 等。不移动的话，原本是一个group对应一个clientid，换组了原本的请求就无法继续。所以要让id不绑定组。请求发现 errwronggroup 后重试。

每次做请求时，client看看当前查询配置kvsrc server得到的当前组id的服务器和自己记录的一不一样，一样就用原本的clerk做，不一样的话，new新的Clerk。其他逻辑不需要改变。

#### 处理失效控制器

无需也无法做删除等kv操作，设计原则是改前预写入要改的配置 `nextConfig`，改后两个配置一样，改时不一样。初始化时两个都读。读不到next忽略，报错sleep100ms等待。如果两个配置不一致（next num更大），把next重做，即进行迁移。迁移成功后写put当前config，并且再次检查。

修改配置的逻辑修改：读取 nextConfig，查无设置版本0，读取失败稍后重试，读取成功，开始修改前写入配置nextConfig，如果写入不成功或者版本不对，稍后重试。迁移失败的话，稍后重试。迁移成功后写入put当前新配置，写入成功返回，写入不成功稍后重试。

迁移：对每个分片，如果组不一样，进行迁移。新旧组

- freeze：旧非空(系统刚启动，旧可以空)，找到旧组(可能新没有，所以先找新，再找旧)，考虑到组下servers可能变化，所以先找target的。

- install：新非空(暂时未分配旧空)，尝试安装。找不到新servers或安装失败，直接return false。

- delete：只有 freeze 成功了，才去 delete 它。失败不管。

  > 即便 Delete RPC 失败（例如旧 gid 崩溃或网络问题），新 gid 仍然拥有数据并对客户端服务，系统的正确性不受影响。之后再做迁移时可以删掉。

> 注意不要反复写，只有old num比新小的时候才写，确保只写一次，完成后写入另一个。如果写不成功，此时是有并发的(网络问题以为挂了所以又开了一个去写)，是错误版本的话，可能别人干完了，这时候重新读取 old。

> 注意到 kvsrc 不可靠，put 可能会 errmaybe，此时可以 get 看看值，人为检查是不是 err，是的话重试。所有的 put 都要如此操作，而不是 put 不成功 panic。put 失败也不断重试。
>

#### 并发配置更改

修改 `InitConfig`，如果 put 失败，是 errmaybe/errversion，但可以读取到，返回不管，其他错误重试。

对 `ChangeConfigTo`，循环逻辑：

1. 当前版本不旧直接返回，否则做 record next，具体为：

   多个 controller 可能同时想把不同 target 写到nextConfigKey，函数需要确保同一个 Num 只允许一个配置进入“待迁移”槽，同时让滞后的 controller 观察到已有记录后直接等待或退出

   读取 next config，如果不是 ok / nokey(没写过)，尝试重写(retry)。如果读到的 next 比自己新，别人在写新的，当前过时(wait)。否则，版本一样，内容不同，也过时(避免覆盖)，旧就直接写(proceed)。内容一样，可以直接写，都一样。然后写入 next config，ok 继续，errver/nokey 等待，其他重写。

2. 回到外层，看 record next 结果，重试/重写的话，sleep后返回第一步判断。否则，尝试重写，迁移成功且put写入新配置成功返回，否则重试。迁移逻辑同上不变。

#### 扩展

##### kvraft控制器

较为简单，直接更改即可。对 `MakeShardCtrler`，更改为 kvraft 的 makeclert。在 `test.go`，把 `StartControllerKVServer` 里，切换启动，换为 kvraft 的。对 `makeShardCtrlerClnt`，创建 servers 名字，传入构造 `MakeShardCtrler`。做下面的测试在 `shardkv_test.go`：(在test里的顺序决定了测试的顺序)

```sh
go test ./shardkv1 -run TestControllerKvraftPeerFailure5D
```

测试主体：启动3个服务器，并宕机一个，然后马上调用查询，必须能读到非空配置。然后加入一个新的 gid，写入新配置，检查新配置成功。

可选择撤回，使用 `ControllerUseKVraft` 修改为 false 撤销。不撤销全 test 时间 550s。撤销486s。显然是因为raft导致的，合理。

##### exactly-once

> 2024 Lab2，对比：at-most-once 是至多一次，可能处理0次(丢失)或1次(成功)，但不会重复处理，如lab4实现。优点：简单、延迟低。缺点：可能丢失。用于：非关键操作丢了无所谓/可以手动重做。
>
> exactly-once：不丢消息，不是说消息只传输一次(传输层语义)，一般是结果只处理一次。成本更高，实现更复杂。

现在因为 client 每次发送请求都会 requestId++。如果得到了 errversion / errmaybe 等，会在上层再次调用，因此下一次 id 不一样，不是 exactly-once。

> 2025版本有version，可以安全重试，但会导致errmaybe。但2024没有。2024丢包，client重试，server去重。2025只需要client重试，服务器不管。

2024版本是不带version的KV服务器，实现put/append/get。他的实现是客户端是带了 clientId, requestId，不断重试put/get直到成功。由于requestid递增，同理，只需要记录 lastresult。因此，具体到 

测试用例：lab2 直接参照之后 lab4 的做 lastResult 即可。检测两次 put 得到同一个版本号且都成功。两次 get 也是。对 lab4 的测试，一样的逻辑只不过改成集群。lab4 我现在的实现本来就是 exactly-once，不需要在做别的了。重新运行各个 test 即可。

对 lab5 时间不变，都是 485s 左右。对 lab2，把 31s 优化到了 19s，因为做了缓存，很好理解。lab4 都是 230s 左右。

##### range

直接改结构即可。之前在 mit6.830 手写过 B+ 树了，这里直接调库。一种最小修改方案是，btree 只存键，值还是用原本的来存。思路是一样的，这里只做 lab2 kvsrv，如果要搞到 kvraft 同理，略。

> 当然也可以换红黑树，b+树。略。btree 的度/阶，控制节点最小度(非叶子结点最少key-1)。内存里建议 16/32，元素很多读多写少就 64/128 等。数据少就 8/16 等。非常影响性能。

通过测试，若度=4，30s，度=16，<u>25.5s</u>。不加的话，25.5s，很合理。

如果配合 exactly-once，是度=4，22s (只慢了3s)，度=16，<u>21s</u>。

##### 租期

> 要求把 kvraft get 改成 leader 用 kv 状态回答，而不是走 raft。但仍然要保持线性一致性，并测试确实更快，以及换 term/leader 更慢(新 leader 需等旧租约过期)。
>
> 在读多写少时，这种场景通过 get 不复制提速。但问题是破坏线性一致性：put已完成，则get必须读到它。①leader更换，旧leader返回过期读；②leader尚未处理(applyCh到map)，使得map读旧。
>
> 解决办法：租期 read-only + leases。leader 周期性心跳，只要在最近一段时间内收到了多数派对心跳的响应，就认为自己持有一个租约，在租约未过期前可以直接回答读。
>
> 租约长度小于 election timeout，心跳回复是多数派，且自己 commit 过项目(可以上任追加no-op，让多数派承认它的日志前缀)，更新租期。
>
> 那么，代价是租约过期更慢，新 leader 等待 leash 过期才能做 leash-read，避免上一个还在。因此：此时速度会更慢。
>
> 在工程界，TiKV/TiDB、CockroachDB、Google Spanner/Chubby使用了这个思想，etcd使用了 readindex，每次需要线性一致读时，通过多数派确认 leader 身份与日志新鲜度(仍然需要多数派，所以更慢)。

给 `raftapi.go` 添加 `HasLease()` 接口。在 `server.go` 添加对 raft 的引用，在做 get 时，先尝试做租约读，具体逻辑如下：

1. 如果 `HasLease` 为假返回，为真加锁再判断，如果有缓存，直接返回，能取的话，根据缓存类型对不对返回 true/false
2. 没有缓存，租期在，就绕过 rsm，直接自己做 `doGet` 操作，从而做 get 优化。

必须要加锁，租期读绕过了raft本身的串行apply，例如apply日志有put，但是此时来了一个租期读。

关键变量：(`raft.go`)

- 租约(lease)周期：200ms
- 最后通讯时间：与每个follower同步日志的最新时间(当前任期)
- 冷却时间：让上一个lease自然过期需要等待一个周期200ms
- 过期时间：当前租期啥时候过期

在 `raft.go` 修改如下：

- `HasLease()` 实现，如果不是 leader，或者没开功能，或者follower；否则看过期时间到没。
- 更新任期时，或者成为 leader 时，清空最后通讯时间，清空租期过期时间。
- 成为 leader 时，刷新自己的租期过期时间为下一个lease 周期( 200ms)。特别地，如果只有单机，不用冷却，直接更新过期时间。
- 给其他follower发送日志时，记录时间，即与每个follower的最后通讯时间。然后：如果还是leader，开启功能，冷却过了，那么，如果超过半数follower的最后通讯时间在周期内，刷新自己的过期时间。

测试用例：

- 先测 flag 关闭下的 RPC 数 (读5次)，再测开启效果，断言后者更少，用于验证快路径带来的性能收益。
- 启动 3 节点集群，先等待旧 leader 获得 lease 并确认快路径生效(RPC 数 <= 2)。杀死 leader，等待新的 leader 产生，并检查它初始阶段 `HasLease()==false`。在 lease 生效前发出一次 Get，期望 RPC 数 >= 3(说明走了慢路径)；之后等待新 leader 获得 lease，再次读取确认 RPC 数下降。

lab3 测试前 431s/440s。修改后测试 453s/457s。

lab4 测试前 216s/210s，修改后 209s/196s (含新测试)

> `/` 是多次测试。

##### 事务

简化 etcd 的事务 Txn 接口：compare 比较键版本号()，success 事务成功执行，failure 事务失败执行。原子性地判断+执行(判断必须是服务器做，而不是业务做，否则判断成功再给服务器做事务，可能发过来又不判断成功了)。三者都可以缺省。compare要全部满足，就执行success的全部，任意不满足，执行failure的全部。

为了保证原子性，整个事务在raft里日志只占一个位置，即等价于单条命令command。

- `TxnCompare` 是 key + version 结构体
- 事务操作 `TxnOp` 是类型(get/put)+键值
- 事务参数是 身份(俩id) + compare 数组 + 成功数组 + 失败数组(后两个都是 `TxnOp`)
- `TnxReply` 是 类型 + get结果 + put结果；事务结果是该数组+bool(是否成功)+err

对 `client`，提供 `Txn` 函数，调用服务器事务方法，返回结果。重点看 `server`，处理接口，记忆化，持久化等琐碎后，重点是 `doTxn` 实现：

- 检查所有 compare 为真，其中 version=0 表示 key 不存在
- 对所选(success/failure)，一条一条执行(直接对table操作)

ACID分析：

- A：如果失败，直接从快照恢复，然后重做日志，保证一定做完，所以没有中间态。

- I：单个server是串行的，隔离级别最高，满足。

  在租期读下，锁满足了读已提交的隔离级别，不会脏读，但是不可重复读(读锁释放，可以再写)且幻读。

- D：日志保证了。

- C：上述满足，业务正确自然会C。

##### 分布式事务

# 过程记录

- 2025/9/29 2h 阅读Lab1代码，学习go语法
- 2025/10/1 2h 阅读Lab1代码、要求与指示，学习go RPC等
- 2025/10/2 4h 设计Lab1代码，学习go语法、map-reduce流程等
- 2025/10/3 5h 完成Lab1代码，学习go语法，debug，文档
- 2025/10/4 3h 完成Lab2代码，并学习概念与相关知识原理
- 2025/10/5 2h 阅读Lab3要求，Raft 概念学习
- 2025/10/9 1h 阅读Lab3论文部分，学习

> 由于论文大修和科研工作，中间内容终止了数个月。

- 2026/1/12 4h 完成Lab3A代码，理解相关概念和语法
- 2026/1/14 3h 完成Lab3B代码，理解相关概念和流程
- 2026/1/16 3h 完成Lab3C、D代码，理解相关概念
- 2026/1/20 6h 完成Lab4A代码，理解相关概念，修复lab3一些问题
- 2026/1/21 4h 完成Lab4B，C代码，理解相关概念
- 2026/1/22 2h 阅读Lab5要求，开始代码工作
- 2026/1/24 4h 完成Lab5A
- 2026/1/27 3h 完成Lab5B，debug
- 2026/1/28 2h 完成Lab5C，完成所有基本功能 (共50h)
- 2026/1/29 5h 完成Lab5D的选做：kvraft ctrler, exactly-once, range，租期
- 