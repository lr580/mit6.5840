[mit6.5840网址](http://nil.csail.mit.edu/6.5840/2025/)

# 实现过程

## Lab1

### 环境准备

根据 [lab1](http://nil.csail.mit.edu/6.5840/2025/labs/lab-mr.html) 的第一句话，克隆远程仓库获取源码

```sh
git clone git://g.csail.mit.edu/6.5840-golabs-2025 6.5840
```

在 [setup go](http://nil.csail.mit.edu/6.5840/2025/labs/go.html)，需求 Go 1.22 版本，使用 `go version` 可查询版本。

需求使用 Linux，这里我使用 WSL2。按照指示检查运行：

> ```go
> cd src/main
> go build -buildmode=plugin ../mrapps/wc.go
> rm mr-out*
> go run mrsequential.go wc.so pg*.txt
> more mr-out-0
> ```

其中，build的构建模式为plugin，它生成`.so` 文件，即 `wc.go`，项目结构 `src/`

- `mrapps/` 是 map-reduce 实际应用例子，如 `wc.go`，看到它定义了 Map 和 Reduce 函数。其中 Map 函数的第一个参数在例子中忽略，只看第二个参数(文件内容)，它把不是 Letter(a-zA-Z) 的连续内容作为分隔符，然后分割后的单词数组返回 (`mr/KeyValue`) (没有做合并)。Reduce 函数，传入 key 值和 values 列表，values 长度转字符串作为该 key 的合并结果。
- 对 `main/mrsequential.go`，传入 `.so` 执行，选择文件名执行。它的 `loadPlugin` 函数，导入插件，查找 Map, Reduce 函数，并返回。对每一份文件，把读取的文件内容和文件名传入 map 函数，得到 key-value 列表；所有文件的合并到一个列表里。(实际上应该分快存储)，然后进行排序，创建输出文件。排序后 key 相同的全部 value 集合并到一个 string[] 里。把这个 slice 传给 reduce 函数，然后输出结果到文件。

### 任务描述

#### 任务

实现：协调节点coordinator, 工作节点worker。单协调进程，任意多个工作节点，该lab只考虑工作节点都在同一个机子。使用RPC，循环请求协调者执行任务。协调者应当注意若工作者在10秒内未能完成任务，将它分发给其他工作者。

`main/mrcoorinator.go`, `main/mrworker.go` 是给定不可修改的代码，参考它们，实现 `mr/coordinator.go`，`mr/worker.go`，`mr/rpc.go`。执行 mapreduce 的过程：

```sh
go run mrcoordinator.go pg-*.txt # 一个
go run mrworker.go wc.so # 一个或多个
cat mr-out-* | sort | more # 查看结果
```

使用测试脚本检查 `wc, indexer` 任务是否产出正确，以及测试是否并行，能否恢复crash。

```sh
bash test-mr.sh
```

把 `mr/coordinator.go` 的 Done 函数改为`ret := true`，让协调者马上退出。如果全部通过，测试完成。

若 rpc 报错如 ` method "Done" has 1 input parameters`，可以忽略，因为它不会被远程调用。[文档](https://golang.org/src/net/rpc/server.go) 帮助检查是否所有方法对 RPC 合适，即有 3 输入。若出现 `connect: connection refused`，当协调者关闭后，无法连接是正常的。

#### 规则

- map 阶段，分割中间 keys 为桶，给 `nReduce` 个任务，在 `main/mrcoordinator.go` 该参数传递给 `MakeCoordinator()`，每个 mapper 要创建 `nReduce` 个中间文件给 reduce 任务消费。
- 工作者实现第 X 个 reduce 任务时，写到 `mr-out-X`。该文件对每个 Reduce 函数输出包含一行，使用 format `%v %v` 生成 key value 行。查阅 `main/sequential.go` 查看格式范例。
- 工作者把中间 Map 输出放当前工作目录。
- `main/mrcoordinator.go` 期望 `mr/coordinator.go` 实现 `Done()` 方法，返回 true 如果任务执行完毕，随后 `mrcoordinator.go` 退出。
- 全部任务完成，工作者进程关闭，可以用 `call()` 的返回值实现。若工作者无法联系协调者，可以假设任务完毕，并让工作者关闭。也可以自由发挥。

#### 提示

- 一个开始的办法是调整 `mr/worker.go` 的 `Worker()` 发送 RPC 给协调者请求任务，然后修改协调者回答未开始的map任务文件名，然后修改工作者去读取文件，调用 Map 函数。
- 使用 go plugin 加载 map, reduce 任务，`.so` 文件。如果修改 `mr/` 的内容，可能需要重新构造 `.so`。
- 工作者共享同一个文件系统。
- 中间文件可以命名为 `mr-X-Y`，分别表示 map, reduce 编号。
- map 任务需要存储中间键值对在文件，使其可以在 reduce 被读取，可以考虑 json 包，参考示例代码读写。
- 可以使用 `ihash(key)` 函数(`worker.go`)选择reduce任务。
- 读文件，排序，存储reduce输出，可以参考 `mrsequential.go`。
- 协调者并发，需要锁住共享数据。
- 使用 Go race detector，如 `go run -race`。在 `test-mr.sh` 开头有注释，提示如何使用。
- 工作者有时需要等待，如 reduce 需要等最后一个 map 完成。可以周期轮询+sleep；或者 RPC 做一个等待循环，sync.Cond。每个 RPC handler 独立线程，所以 handler 等待不会阻止协调者处理其他 RPC。
- 协调者无法区分崩掉的、静止的、太慢的工作者。只能让协调者等待一段时间，然后切换工作者。
- 若实现文章 Section 3.6 的 Backup 任务，它只应当长时间后才被安排，如10s，也就是有挂掉的。
- 测试 crash，可以尝试 `mrapps/crash.go`，它随机关闭 map/reduce 函数。
- 防止 crash 导致写了一半的文件，MapReduce 论文使用 trick：使用临时文件，并原子地重命名它，如果完成了写。使用 `ioutil.TempFile / os.CreateTemp` 来创建这样的文件，然后 `os.Rename` 原子重命名。
- `test-mr.sh` 运行所有进程在 `mr-tmp`，如果挂了，中间过程在该文件找。
- `test-mr-many.sh` 运行多次，可以发现低概率错误。参数：运行次数。不要通过自己跑多次 `test-nr.sh` 来做它，这样协调者会复用一个 socket。
- Go RPC 只发送大写结构体字段，子结构体也是如此。
- 调用 RPC `call()`，返回结构体要所有字段赋值为默认值，有参见例子。否则，可能结果不对。

#### 通用提示

[url](http://nil.csail.mit.edu/6.5840/2025/labs/guidance.html)：

- Easy：数小时；Moderate：6h每周；hard：超过6h每周。

  大部分任务只需要几百行代码，但概念+debug困难。

- 在线 Go 教程 [here](http://tour.golang.org/)，[Effective Go](https://golang.org/doc/effective_go.html)，[Editor](https://golang.org/doc/editors.html)

- [race detector](https://blog.golang.org/race-detector) 报告的 race 需要修复。

- [Raft](https://thesquareplanet.com/blog/students-guide-to-raft/) 指引

- [locking](http://nil.csail.mit.edu/6.5840/2025/labs/raft-locking.txt) 指引

- [Raft structuring](http://nil.csail.mit.edu/6.5840/2025/labs/raft-structure.txt) 指引

- [图解Raft交互](http://nil.csail.mit.edu/6.5840/2025/notes/raft_diagram.pdf)

- 在每个节点收发消息关键位置 print，日志重定向到文件来分析，方便 debug

  结构化 debug 输出信息，那么 grep 查找消息更方便

- `DPrintf` 比 `log.Printf` 更有用

- [Go format strings](https://golang.org/pkg/fmt/)

- 颜色 / 列来解析 log 输出 [参考](https://blog.josejg.com/debugging-pretty/)

- git 教程 [url](https://git-scm.com/book/en/v2) [url2](https://www.kernel.org/pub/software/scm/git/docs/user-manual.html)

#### 调试技巧

先对问题的潜在成因提出假设；收集可能相关的证据；综合分析已获取的信息；根据需要重复上述过程。对于长时间的调试任务，建议做好记录

一种有效策略是逐步定位问题首次出现的时间节点。可以在程序执行的不同阶段插入状态检查代码，或是添加输出关键状态信息的日志语句，将日志保存至文件后仔细排查首个出现异常的时间点

Raft实验涉及的事件（如RPC请求到达、超时触发、节点故障）可能在意料之外的时刻发生，或以难以预料的顺序交错出现。例如某个节点刚决定参与竞选，而另一个节点却认为自己是当前领导者。建议推演"后续可能发生的情况"：比如当Raft代码释放互斥锁后，下一瞬间可能就会处理到达的RPC请求或超时事件。可通过打印语句来捕获实际的事件执行顺序。

必须严格遵循Raft论文图2的规范，容易遗漏其中规定的状态检查条件或必须执行的状态变更。若出现故障，请重新核验代码是否完全符合图2要求。

在编写代码时（即出现故障前），建议对代码依赖的前提条件添加显式检查（例如使用Go的[panic](https://gobyexample.com/panic)机制）。这类检查有助于发现后续代码无意中违背假设的情况。

若原本正常的代码出现异常，很可能是近期修改引入的问题。

故障往往藏在你最后检查的地方，因此即使对确信无误的代码也需保持审视

#### 挑战任务

- 实现自己的 MapReduce 任务，可以看 `mrapps/*` 例子，如文章 Section 2.3 的分布式 Grep。
- 让协调者，工作者在不同机器上工作，设置 RPC 在 TCP/IP 而不是 Unix sockets (参考注释 `Coordinator.server()`)，用共享文件系统读写文件。如 `ssh` 多个 [Athena cluster](http://kb.mit.edu/confluence/display/istcontrib/Getting+Started+with+Athena)，使用 [AFS](http://kb.mit.edu/confluence/display/istcontrib/AFS+at+MIT+-+An+Introduction) 共享文件，或自己用 AWS 实例 / S3 存储。

#### 代码梳理

`main/mrcoordinator.go` main 函数，把命令行参数全部传进去，以及 nReduce=10，执行 `mr.MakeCoordinator`，完毕 sleep 1s，返回值为 m，若 `m.Done()` 是 false，就一直 sleep 1s (轮询)。

`main/mrworker.go` main 函数，对 CLI 参数加载 plugin，调用 `mr.Worker` 执行 map, reduce。

`mr/rpc.go` 给定例子使用 RPC，对该例子，`mr/coordinator.go` 定义了 Example 函数，`mr/worker.go` 定义了 `CallExample()` 函数。在 `mr/worker.go` 定义了 call 函数，在 `mr/coordinator.go` 定义了 `server()` 结构体方法，用来执行 RPC。

`mr/worker.go` 定义了 ihash 函数用于字符串转 int。定义了 `KeyValue` 结构体。定义了 `Worker` 函数需要自己实现。

`mr/coordinator.go` 定义了 `Coordinator` 结构体及其成员方法(需要自己补充成员属性)。需要实现：①MakeCoordinator构造函数；②Done成员函数。

### 实现思路

具体细节见 `mr/worker.go`, `mr/coordinator.go`。分为多个部分描述：

#### coordinator.go

##### 构造函数

没有指定 map 任务数，故：按照文件数量分配 map 数量，一个文件一个 map。不考虑更加复杂的情况。

每个任务有两个属性：状态（空闲、运行中、完毕）、开始时间。构造两个任务列表，分别表示 map, reduce。

此外，给自己构造三个阶段：map 阶段，reduce 阶段，退出阶段。

##### 超时检测

10秒算超时。构造阶段设置一个 `deadWorkerChecker` 进程，隔段时间(如1s或5s)检测一次全体任务。对超时任务，直接重新设置为 IDLE。

*理由：如果后面发现 worker 没挂，让该任务被执行多次，因幂等不影响结果。可以给读写同一文件的过程加锁，避免 reducer 执行读到一半被覆盖*

由于超时检测会修改任务，而正常执行也会，避免竞态，在修改任务时加 mutex 锁

##### 分配任务

定义 Allocate RPC方法，对 worker 发送的请求，分配任务给 worker。

- 如果当前是退出阶段，发送信息给 worker 报告它退出。

- 否则，对当前任务列表(根据阶段选择 map 列表或 reduce 列表)，找到任意一个空闲任务分配给 worker

  简单起见，直接暴力遍历查找。后续需要的话可以用 bitset / map 等优化。

- 如果没有空闲任务，证明当前全部 map / reduce 都在执行，让 worker 继续等待。

加 mutex，下面结果收集同理。也是防止多个 RPC 竞态，不只是和 `deadWorkerChecker`。

##### 结果收集

定义 Report RPC方法，worker 完成任务后调用

- 如果当前状态和 worker 汇报的状态不一样，taskid 无意义，丢弃该饭会报错并告诉 worker。这证明 worker 挂掉了导致一个任务被执行多次。
- 否则，如果当前任务状态不是 RUNNING，报错。
- 否则，记录当前任务完成，更改状态。如果当前任务都完成了，进入下一阶段。

#### worker.go

##### 文件I/O

按要求进行文件读写。

- 除了 map 必须返回完整 content 外；对 KV 的读写使用流。
- 写文件，先写到一个临时文件(随机创建到当前目录)，写完后原子重命名为目标文件。
- 读文件，由于写操作 os.Rename 原子，不需要对读加锁，不会出现读写竞态等问题。

##### 主循环

由于在 `main` 里，它直接启动 `mrWorker`，不能像 `mrcoordinator` 那样 Done 轮询来结束，所以需要 worker 自己控制是否结束。

在这里设计结束主循环的情况只有两种：在发送 Allocate 时返回报错(coordinator挂了)、或者返回已完成。

主循环为：

1. 发送 Allocate 调用

   - 如果结果是等待，随机等待一段时间。

     *也可以设计指数退避*，这里从简考虑。

   - 否则，如果是 mapping / reducing，分别执行 map/reduce 任务。

2. 根据执行结果，发送 Report 调用。

##### mapping

过程：

1. 读文件

2. 执行 mapf 函数，得到 KV 列表

3. 将 KV 按 ihash 分桶

4. 每个桶的 KV 子列表直接写入中间文件 `mr-X-Y`，

   > X 是当前 map 号，Y 是全体 [0, nReduce)。这里 X,Y 0-indexed

如果写入报错，返回执行失败，否则都返回执行成功。

##### reducing

过程：

1. 读所有 `mr-X-Y` 文件，其中 Y 是当前 reduce 号，X 是全体任意。

   > 修复了 bugs，注意一定要严格是 X,Y 是整数，测试过程存在 `mr-??-Y`，其中 `??` 是字符串。避免匹配失败报错。

2. 合并所有 KV，然后排序

3. 排序后，同 K 的 V 分组，传入 reducef，然后记录该组答案

4. 写入该 reduce 任务的结果到 `mr-out-Y`

如果读写报错，返回执行失败，否则都返回执行成功。

## Lab2

### 任务描述

[lab2描述文档](http://nil.csail.mit.edu/6.5840/2025/labs/lab-kvsrv1.html)

##### 概述

实现 Key/value 服务器，单机，每个 Put 最多执行一次，保证 Linearizable 线性一致性。

每个用户与 KV 服务器用 Clerk 交互，它发送 RPC：`Put(key, value, version)` 和 `Get(key)`。服务器维护 in-memory map 记录每个 key 一个 (key, version) 元组。KV 都是字符串。版本数字记录 key 被写的次数。

Put 添加或覆盖当且仅当服务器版本与key版本一样，然后增加key的版本。不匹配返回 `rpc.ErrVersion`。客户端用版本0新建key，此时保存到服务器版本是1。若Put版本>0但key不存在，返回 `rpc.ErrNoKey`。

Get 获取 key 的当前值和版本，若不存在 key，返回 `rpc.ErrNoKey`。

版本对锁有意义，使得 at-most-one 语义有效，考虑到网络不可靠导致的重传。

`src/kvsrv1` 有框架和测试。`client.go` 实现 Clerk，使客户端用 RPC 与服务器交互，提供 Put, Get 方法。`server.go` 包含服务代码，如 Put, Get 处理器。`kvsrc1/rpc` 包里定义了 RPC 请求，回答，错误值。可用而不需要修改。测试；

```sh
cd src/kvsrv1
go test -v
```

##### 任务1：可靠网络

假设消息没有 drop，实现 RPC 发送和处理器。完成可靠性测试。

```sh
go test -v -run Reliable
```

Passed 行输出里的数字是：①花费的秒数；②1；③发送的RPC数(含客户端)；④执行的 KV 操作(Clert, Get, Put)。

检查代码竞态，需要使用 `go test -race`。

##### 任务2：锁

> 分布式应用的客户端跑在不同机器，使用 KV 服务器协调活动，如 ZooKeeper, Etcd 允许分布式锁。类似于Go程序中的线程可以通过锁（即sync.Mutex）进行协调。ZooKeeper和Etcd通过条件写入（conditional put）机制实现这种锁功能。

实现锁，支持 `Acquire` and `Release`。只有一个客户端可以成功获取锁，其他必须等它释放。在 `src/kvsrv1/lock/` 里有框架和测试，修改 `src/kvsrv1/lock/lock.go`，用 `lk.ck.Put/Get()` 与 KV 服务器沟通。

> 若客户端崩了，它的锁无法释放。所以一般锁有[租期](https://en.wikipedia.org/wiki/Lease_(computer_science)#:~:text=Leases%20are%20commonly%20used%20in,to%20rely%20on%20the%20resource.)，过期后会释放锁。这里可以忽略。

必须通过 Reliable test (lock 目录下的)

```sh
go test -v -run Reliable
```

对每个锁客户端，需要唯一标识符，调用 `kvtest.RandValue(8)`。

锁服务用特殊 key 存储锁状态，定义锁状态。传入参数 l (MakeLock, `lock/lock.go`)

##### 任务3：传输失败

网络会重排序、延迟、丢弃 RPC 请求和回复。需要回复它们，让 Clerk 保持重试直到收到回答。

网络丢弃请求，客户端重发即可解决。但网络还会丢弃回答，如果重发，Get 没问题，但 Put 不行，需要对 Put，若之前已接收并执行过该Put请求，服务端会向重传的副本返回rpc.ErrVersion错误而非重复执行操作，因此携带相同版本号重传Put RPC也是安全的。此时客户端不知道执行没有，可能执行了、可能没执行而是其他Clerk更新了。故，若重传Put收到了rpc.ErrVersion，它必须向应用程序返回rpc.ErrMaybe而非rpc.ErrVersion。后续应由应用程序处理此情况。若服务端对首次（非重传）Put RPC返回rpc.ErrVersion，则Clerk应向应用程序返回rpc.ErrVersion，因为此时可确定该RPC未被服务端执行。

> 若能使Put操作具备"精确一次"的执行语义（即避免出现rpc.ErrMaybe错误），应用程序开发者的体验将得到显著改善。但若不在服务端为每个Clerk维护状态信息，这一点将难以保证。在本实验的最后一项练习中，您将使用Clerk实现锁机制，以此探索如何基于"至多一次"的Clerk.Put语义进行编程。

修改 `kvsrv1/client.go`，`ck.clnt.Call()` 返回 true 表示收到 RPC 回答，false 表示收不到(超时)，此时要不断重发直到回答。该任务不应该改变服务器。

通过所有 `kvsrv1/` 的测试，就是过了：

```sh
go test -v
```

Client 重试之前，应当等待，如 100 ms 睡眠。

##### 任务4：传输失败锁

修改锁的实现，使得它可以通过所有 `kvsrv1/lock` 的测试。

##### 代码梳理

`rpc/rpc.go` 略，可自行观看。

> `client.go` 里使用到了：`src/tester1/Clnt`，它又基于 `src/labrpc/labrpc.go` 实现了基于 channel 的 RPC。模拟网络实现失去请求、延迟、断连。这些具体暂时略。感觉已经屏蔽了细节，所以我不需要考虑。

`client.go` 里 `Clert` 是结构体，定义了构造方法，Get 方法和 Put 方法。

`server.go` 里实现了 debug 方法，`KVServer` 结构体，它的构造方法，以及 `Get, Put` 方法。还有可忽略的 `Kill()` 方法和 `StartKVServer`。

##### 线性一致性

[QA](http://nil.csail.mit.edu/6.5840/2025/papers/linearizability-faq.txt) 的直接翻译

> 线性一致性：如果客户端操作不是并发的，那么每个客户端的 Clerk.Get 和 Clerk.Put 操作都将观察到前序操作序列对系统状态所做的修改。对于并发操作，其返回值和最终的系统状态，将等同于这些操作以某种顺序依次执行（一次只执行一个）所得到的结果。，假设客户端 X 调用了 Clerk.Put()，随后客户端 Y 也调用了 Clerk.Put()，然后客户端 X 的调用先返回。在这种情况下，任何一个操作都必须能够观察到在该操作开始之前已经完成的所有操作所产生的效果。
>
> 线性一致性对应用程序非常便利，因为它呈现的行为就如同单个服务器逐次处理请求时的表现。例如，若某客户端从服务器获得了更新请求的成功响应，那么其他客户端后续发起的读取操作就能确保看到该更新的效果。对于单台服务器而言，实现线性一致性相对容易。

问：什么是可线性化？

答：可线性化是一种定义服务在面临并发客户端请求时行为正确性的方式。粗略来说，它要求服务的行为必须表现得像是按照客户端操作到达的顺序逐个执行这些操作。

可线性化是基于“历史记录”来定义的：即客户端操作的轨迹，标注了客户端开始每个操作的时间点以及客户端感知到操作完成的时间点。可线性化用于判断单个历史记录是否合法；如果一个服务能够生成的所有历史记录都是可线性化的，我们就可以说该服务是符合可线性化的。

历史记录中包含客户端开始操作的事件，以及客户端判定操作已完成的事件。因此，历史记录明确体现了客户端之间的并发性以及网络延迟的影响。通常，开始和结束事件对应于客户端与服务器之间交换的请求和响应消息。

一个历史记录是可线性化的，前提是你能为每个操作分配一个“线性化点”（即一个时间点），使得每个操作的线性化点位于其开始和结束事件的时间点之间，并且历史记录的响应值与按照线性化点顺序逐个执行这些操作时得到的响应值一致。如果无法找到满足这两个要求的线性化点分配方案，则该历史记录不符合可线性化。

可线性化的一个重要推论是：服务在执行并发（时间上重叠的）操作时拥有顺序上的自由度。具体来说，如果客户端C1和C2的操作是并发的，即使C1的操作早于C2开始，服务器也可以先执行C2的操作。另一方面，如果C1的操作在C2开始之前已经结束，可线性化要求服务的行为必须像是先执行了C1的操作再执行C2的操作（即C2的操作需要能观察到C1的操作所产生的任何效果）。



问：可线性化检查器是如何工作的？

答：一个简单的可线性化检查器会尝试所有可能的顺序（或线性化点的选择），以判断是否存在一种顺序符合可线性化定义中的规则。由于在大规模历史记录上这种操作会非常缓慢，智能检查器会通过以下方式优化：避免检查明显不可能的排序（例如，若某提议的线性化点位于操作开始时间之前）、将历史记录分解为可独立检查的子历史记录（在可能的情况下），以及使用启发式方法优先尝试更可能的顺序。

以下论文描述了相关技术；我认为 Knossos 基于第一篇论文，而 Porcupine 则加入了第二篇论文的思想。

```
http://www.cs.ox.ac.uk/people/gavin.lowe/LinearizabiltyTesting/paper.pdf
https://arxiv.org/pdf/1504.00204.pdf
```



问：服务会使用可线性化检查器来实现可线性化吗？

答：不会；检查器仅用于测试环节。



问：那么服务如何实现可线性化呢？

答：如果服务以单服务器形式实现，且没有复制、缓存或内部并行机制，那么服务基本上只需按照客户端请求到达的顺序逐个执行即可。主要的复杂性来自因认为网络丢包而重发请求的客户端：对于具有副作用（side-effects）的请求，服务必须确保每个客户端请求仅执行一次。如果服务涉及复制或缓存，则需要更复杂的设计。

> 我认为复制就是多台机子。



我无法读取《p2201-zare》文件的内容。其他文件已阅读并为你总结如下：

问：是否有使用 Porcupine 或类似测试框架测试真实系统的案例？

答：此类测试很常见——例如，可以查看 [https://jepsen.io/analyses](https://jepsen.io/analyses)；Jepsen 是一个组织，已对许多存储系统的正确性（以及在适用情况下的可线性化）进行了测试。

具体到 Porcupine，这里有一个示例：[https://www.vldb.org/pvldb/vol15/p2201-zare.pdf](https://www.vldb.org/pvldb/vol15/p2201-zare.pdf)



问：还有哪些其他一致性模型？

答：可以参考以下模型：

- 最终一致性（eventual consistency）
- 因果一致性（causal consistency）
- 分支一致性（fork consistency）
- 可串行化（serializability）
- 顺序一致性（sequential consistency）
- 时间线一致性（timeline consistency）

此外，数据库、CPU 内存/缓存系统和文件系统领域还有其他一致性模型。

一般来说，不同模型的区别在于：

1. 对应用程序开发者而言的直观性；
2. 能实现的性能水平。

例如，最终一致性允许许多异常结果（例如，即使写入已完成，后续读取也可能看不到该写入），但在分布式/复制场景中，它可以比可线性化实现更高的性能。



问：为什么可线性化被称为强一致性模型？

答：它的"强"体现在禁止了许多可能让应用程序开发者感到意外的场景。

例如：

- 若我调用 `put(x, 22)`且该写入操作已完成，且期间没有其他对 `x`的写入操作，随后你调用 `get(x)`，可线性化保证你看到的值一定是 22。即读取总能获取最新数据。
- 若没有人写入 `x`，且我调用 `get(x)`后你也调用 `get(x)`，我们不会看到不同的值。

这些特性在其他一致性模型（如最终一致性和因果一致性）中并不成立。后者通常被称为"弱一致性"模型。



问：在实践中，人们如何确保分布式系统的正确性？

答：全面的测试是常见的方案。

形式化方法也经常被采用；可以查看以下示例：

```
https://arxiv.org/pdf/2210.13661.pdf
https://assets.amazon.science/67/f9/92733d574c11ba1a11bd08bfb8ae/how-amazon-web-services-uses-formal-methods.pdf
https://dl.acm.org/doi/abs/10.1145/3477132.3483540
https://www.ccs.neu.edu/~stavros/papers/2022-cpp-published.pdf
https://www.cs.purdue.edu/homes/pfonseca/papers/eurosys2017-dsbugs.pdf
https://www.andrew.cmu.edu/user/bparno/papers/ironfleet.pdf
```



问：为什么选择可线性化作为一致性模型，而不是最终一致性等其他模型？

答：人们确实经常构建提供弱于可线性化一致性（如最终一致性和因果一致性）的存储系统。

可线性化对应用开发者具有以下优势：

- 读取总能观察到最新数据。
- 若无并发写入，所有读取者会看到相同的数据。
- 在大多数可线性化系统上，可实现小型事务（如测试并设置），因为可线性化设计通常要求对每个数据项逐个执行操作。

而最终一致性等弱一致性方案能提供更高性能，因为它们不要求立即更新所有数据副本。这种性能优势往往是决定性因素。对于某些应用（如存储从不更新的图像或视频），弱一致性不会引发问题。

但弱一致性会给应用开发者带来复杂性：

- 读取可能观察到过时（陈旧）数据。
- 读取可能看到乱序的写入结果。
- 写入后立即读取可能看不到自己的写入，反而看到旧数据。
- 对同一项的并发更新无法逐个执行，因此难以实现测试并设置、原子递增等小型事务。



问：如何确定线性化操作中的橙色小线段（即线性化点）的位置？在图表中，它看起来像是随机画在请求主体内的某个位置。

答：其核心思想是，为了证明某个执行过程是线性化的，需要（由人工）找到放置这些橙色小线段（线性化点）的位置。也就是说，要证明某个历史记录是线性化的，需要找到满足以下要求的线性化点分配方案（从而确定操作顺序）：

- 所有函数调用都必须在其调用时刻和返回时刻之间的某个瞬间拥有一个线性化点。
- 所有函数在各自的线性化点必须瞬间完成，且行为符合顺序规范的要求。

因此，某些线性化点的放置位置是无效的——可能因为它们超出了请求的时间范围，也可能因为它们违反了顺序规范（对于键值存储而言，违反规范意味着读取操作未能观察到最近写入的值，这里的“最近”指的是线性化点的顺序）。

对于复杂的历史记录，可能需要尝试多种线性化点分配方案，才能找到能证明该历史记录具备线性化的方案。如果尝试了所有方案都无效，则该历史记录不具备线性化特性。



问：是否存在这种情况：当两个命令同时执行时，我们能够强制实施某种特定行为，使得某个命令总是先执行（即始终拥有更早的线性化点）？

答：在线性化存储服务（例如GFS或实验四）中，如果来自多个客户端的请求是并发的，服务可以自由选择执行这些请求的顺序。实际上，大多数服务会按照请求恰好到达网络的顺序来执行。实际的实现通常并不涉及明确的线性化点概念。



问：我们能否进行更强的一致性检查？从直觉上说，线性化似乎并不太实用，因为即使同时执行两个命令，读取的数据也可能不同。

答：确实，线性化让人联想到在程序中不使用锁的多线程编程。这种方式虽然能编写出正确程序，但需要格外谨慎。

更强一致性概念的典型例子是数据库中的事务机制，它能有效锁定所有使用的数据。对于需要读写多个数据项的程序而言，事务编程比线性化更简单。"可串行化"就是一种提供事务支持的一致性模型。

然而，事务系统相比线性化系统要复杂得多，速度更慢，且容错实现难度也更大。

> 线性一致性 关注的是 单个数据对象 在 时间线上的即时可见性。它回答了“现在这个数据的最新值是什么？”这个问题。
>
> 可串行化（事务） 关注的是 多个数据对象 上 一组操作执行的原子性和隔离性。它回答了“这组操作是否像没有并发一样执行？”这个问题。



问：为什么验证现实系统需要"巨大努力"？

答：验证是指证明程序是正确的，保证其符合某些规范。事实证明，证明复杂程序的重要定理是困难的——比普通编程要困难得多。

通过尝试本课程的实验，你可以体会到这一点：[https://6826.csail.mit.edu/2020/](https://6826.csail.mit.edu/2020/)



问：根据指定阅读材料，大多数分布式系统并未经过形式化验证。那么团队如何判定产品已充分测试完毕、可交付客户？

答：明智的做法是在公司资金耗尽破产前就开始交付产品并获取收入。在此之前团队会尽可能充分测试，通常还会争取让部分早期客户在知悉产品可能存在缺陷的情况下先行使用（协助发现漏洞）。当产品功能足以满足多数客户需求且已知重大缺陷均已修复时，或许就是合适的交付时机。

除此之外，理性的客户也会对自己依赖的软件进行测试。任何严谨的组织都不会指望软件毫无缺陷。



问：为何不采用客户端发送命令的时间作为线性化点？即让系统按照客户端发送操作的顺序来执行？

答：要构建能保证这种行为的系统非常困难——开始时间是指客户端代码发出请求的时刻，但由于网络延迟，服务端可能很久之后才收到请求。也就是说，请求到达服务端的顺序可能与开始时间的顺序截然不同。理论上服务端可以延迟处理每个到达的请求，以等待更早发出的请求抵达，但网络延迟可能无限长，很难确定应该等待多久。这还会增加每个请求的延迟，甚至可能大幅增加。不过我们后续将学习的Spanner系统就采用了相关技术。

像线性一致性这样的正确性规范需要在两个维度间取得平衡：既要足够宽松以实现高效执行，又要足够严格以便为应用程序提供有用保证。"看似按照调用顺序执行操作"的要求过于严格，难以高效实现；而线性一致性的"看似在调用和响应之间的某个时间点执行"虽然对应用程序开发者不够直观，却是可实现的方案。



问：在哪些应用场景下，使用线性一致性比弱一致性模型更易于编程？

答：假设应用程序的某个模块计算出一个值，将其写入存储系统，随后设置标志位表明计算值已就绪：

v = compute...

put("value", v)

put("done", true)

在另一台计算机上，程序检查"done"标志判断数值是否可用，若可用则进行调用：

if get("done") == true:

v = get("value")

print v

若实现put()和get()的存储系统具备线性一致性，上述程序将按预期运行。

而对于许多弱一致性模型，上述程序可能无法达到预期效果。例如，提供"最终一致性"的存储系统可能会重排两个put操作的顺序（导致"done"已设为true而"value"尚未就绪），或者可能返回陈旧的get()查询结果。



问：现实中有哪些线性化存储系统的例子？弱一致性存储系统又有哪些？

答：谷歌的Spanner和亚马逊的S3是提供线性一致性的存储系统。

而谷歌的GFS、亚马逊的Dynamo以及Cassandra则提供较弱的一致性保障，它们通常被归类为最终一致性系统。

### 实现思路

#### KV system

按题意模拟即可，所有都给好了提示。非常简单。

1. 对 `server.go` 实现一个 map，key 是字符串，value 是结构体 (string, uint64 (rpc.TVersion) )，按提示 CRUD 和处理错误情况即可。用给定的 mutex 加锁。
2. 对 `client.go` 死循环调用 RPC，返回不是 ok 继续调用。对 Put，如果不是第一次调用而且返回 error，按题意特判即可

按题意失败就等100ms再试。不断试。

#### 锁

参数 l 是锁的名字，Lock 的 ck 是锁代表的客户端。多个客户端的锁名字一样的话，它们竞争这个锁。为了区分是谁可以占有和释放锁，键就是锁名，值就是这个客户端代表的随机字符串。由于不能删除 Key，用字符串 `0` 代表空锁，随机的字符串一定取不到 `0`。

循环过程，执行下面操作：

1. 取当前 key 为 l 的锁的值和版本(版本为了进行修改)
2. 取的过程不应该有任何错误，只可能是 OK 或 NOKEY；上面屏蔽掉了网络情况。如果是 NOKEY，设置版本为 0 (不设置也行)
3. 检查取出来的 value，如果不是空，也不是自己，那么无权操作，忙等睡眠 100ms
4. 否则，允许操作，如果是 Acquire，把 value 设置为自己的标识符；否则设置为空锁标识符，并退出循环；不然继续

> 难度不大，理解流程设计出来即可

## Lab3

### 任务描述

#### 概述

实现 Raft，一个复制状态机协议。复制服务通过存储完整副本到多个服务器达到错误容忍。它允许服务器崩溃的情况下继续服务，如崩溃或网络问题。在这种情况下，不同副本数据可能不一致。

Raft 组织客户端请求为序列，即 log，确保所有副本服务器(replica server)看到相同的 log。每个replica按log顺序执行请求，更新到本地服务器状态。所有replica都执行，得到一致的状态。如果服务器挂了，只需要更新log就可以恢复，只要大部分服务器alive，可以相互沟通，就可以恢复。但大部分挂了就只能等恢复了。

一个Raft实例集用RPC相互沟通，维护副本log。Raft交互支持无限序列的编号命令，即log entries。这些entries用index numbers编号，最终提交，随后Raft发送log entry。

依照 [论文](http://nil.csail.mit.edu/6.5840/2025/papers/raft-extended.pdf) 设计，特别是图2。需要实现大部分论文内容，如保存持续状态、节点重启后读。不需要实现级群成员关系改变（第六章）。

代码在 `src/raft/raft.go`，提供一系列测试 `src/raft/raft_test.go`。请自行测试 `-race`。可以在 `src/raft1` 执行 `go test`。

通过在 `raft/raft.go` 添加代码实现。必须支持下面的接口：

```go
// create a new Raft server instance:
rf := Make(peers, me, persister, applyCh)

// start agreement on a new log entry:
rf.Start(command interface{}) (index, term, isleader)

// ask a Raft for its current term, and whether it thinks it is leader
rf.GetState() (term, isLeader)

// each time a new entry is committed to the log, each Raft peer should send an ApplyMsg to the service (or tester).
type ApplyMsg
```

- 服务器调用 `Make` 来创建 Raft peer，参数是含自己的peer的网络标识符数组，`me` 是自己在 peer 数组里的下标。
- `Start` 请求 Raft 启动添加命令到replicated log。需要立刻返回，不等待添加完毕。期望发送 `ApplyMsg` 给每个新提交log entry导`applyCh`管道给`Make()`。
- 提供样例 `sendRequestVote()` 和 `RequestVote()` 例子 RPC。使用 `src/labrpc` 包，测试器可以模拟错误情况。只使用 RPC 进行 Raft 实例互动，不允许使用共享 Go 变量/文件沟通。

#### 领导选举

实现领导选举和心跳(`AppendEntries` RPC，无 log entries)，单领导选举，领导维持和接管。测试：`go test -run 3A`。

提示：

1. 不要自己运行，要用 test
2. 根据图2，完成收发 RequestVote，遵守选举规则和状态
3. 把选举的状态添加到 `Raft` 结构体 (`raft.go`)，定义结构体维护 log entry 信息
4. 补充 `RequestVoteArgs` 和 `RequestVoteReply` 结构体，修改 `Make()` 创建一个后台 goroutine，它负责在没收到 `RequestVote` 答复时开启选举
5. 心跳实现，`AppendEntries` RPC 结构体定义，定期发送。写 `AppendEntries` 处理方法
6. 测试要求 Raft 在旧领导挂了的5秒内选举出新领导
7. 文章 5.2 方法超时是 150-300ms，它有用只当领导发送心跳远比150ms频繁，如10ms。测试限制每秒10个心跳，所以必须用笔150-300更大的时间，但不要太大，否则无法5秒通过
8. 使用 go [rand](https://golang.org/pkg/math/rand/)
9. 不要使用 `time.Timer / time.Ticker`，使用 [`time.Sleep()`](https://golang.org/pkg/time/#Sleep)，如 `ticker()` goroutine，为 `Make()` 创建。执行周期任务。
10. 选举全逻辑在图2贯穿多部分
11. 实现 `GetState()`
12. 调用 `rf.Kill()` 永远关闭一个实例，使用 `rf.killed()` 检测是否已调用，防止死 Raft 进程混淆
13. `labgob` 会检测是否 RPC 没用大写字段
14. 最难的是 debug，参考[debug技巧](http://nil.csail.mit.edu/6.5840/2025/labs/guidance.html)
15. 测试失败，会产生一个文件，可视化时间线表示事件，[例子]([Porcupine](http://nil.csail.mit.edu/6.5840/2025/labs/vis.html))。也可以自己添加注释，如 `tester.Annotate("Server 0", "short description", "details")`

每个通过包含五个信息：用时、Raft 数、RPC 发送数、RPC 消息总字节、提交总 log entries 数。可以帮助 debug。如果总用时超过 600s / 单个超过 120s，会测试失败。

#### log

测试 `go test -run 3B`。

提示：

1. log 1-indexed。但可以视为 0-indexed，即做一个 0 的 entry，允许 `AppendEntries` 的 `PrevLogIndex` 有效
2. 先通过 `TestBasicAgree3B()`，实现 `Start()`，用 `AppendEntries` 收发新 log entries，如图2，发送提交的 entry 在 `applyCh`
3. 实现选举限制，如论文 5.4.1
4. 循环检测事件，它们需要暂停如 10ms，或者使用 [`Cond`](https://golang.org/pkg/sync/#Cond) condition variables
5. 为后续任务顺利实现，看 guidance 查看开发和debug技巧
6. 查看 `raft_test.go` 检查究竟什么错误

检查实际上使用的 CPU 时间和实际时间，可以查看测试输出

```
go test -run 3B  1.37s user 0.74s system 4% cpu 48.865 total
```

user CPU 时是执行指令而不是等待/睡眠的时间，如果超过 1 分钟 real time，或 5 秒 CPU 时，可能出现问题

##### 持久化

Raft 重启，恢复，如论文图 2。

实际实现会把持久化状态写盘，该实验使用 `persister.go` 的 `Persister` 对象，调用 `Raft.Make()` 会提供该对象，获得最近的持续状态。用它的 `ReadRaftState(), Save()` 方法

实现 `persist(), readPersist()`，需要序列化/编码状态为字节数组，传给 `Persister`。用 `labgob` 编码器，查看注释。对 `Save()`，第二个参数传递 nil。当改变持久状态时，调用 `persist()`，通过 3C 测试。

可能需要优化，查看论文 P7-P8 (灰色线)，当超过一个 entry 一次时。一个可能是提供拒绝信息。

```
XTerm:  term in the conflicting entry (if any)
XIndex: index of first entry with that term (if any)
XLen:   log length
```

```
Case 1: leader doesn't have XTerm:
	nextIndex = XIndex
Case 2: leader has XTerm:
	nextIndex = (index of leader's last entry for XTerm) + 1
Case 3: follower's log is too short:
	nextIndex = XLen
```

提示：3A, 3B 的故障可能导致 3C 的错误。

可以做多次测试 `go test -run 3C`

##### 压缩

持久化存储快照，时不时存储，它丢弃先前在快照的 log entries，让重启更快，持久数据更少。论文第7章提供方案，当follower离leader很远，以至于leader丢弃了log entries，则leader必须发送快照+对应log。

要求提供序列化快照方法 `Snapshot(index int, snapshot []byte)`，测试会定期调用。后期的 Lab4 也会调用，快照包含完整的 KV 表。服务层每个 peer 都调用。

index 参数是最高 log index 的，Raft 要丢弃该点之前的log entries，修改代码，存储 log 尾部即可

实现 `InstallSnapshot` RPC，让 leader 给延迟的 peer 替换它的 snapshot 和 state，如图2；follower 接收到时，使用 `applyCh` 发送快照给 `ApplyMsg` 服务，该结构体定义了需要的字段，不应该让服务状态向后退

若服务器崩了，重启带持久化数据，要储存 state 和对应的 snapshot，用 `persister.Save()` 第二个参数存储快照，如果无快照，传 nil。

服务器重启，应用层读取 persisted 快照，恢复保存状态。

提示：

1. 存储 log 从某 index X 开始，初始化 X=0；让 `Snapshot(index)` 丢弃 index 之前的，然后令 X=index，则可以通过 3D
2. 常见WA的原因：follower 花费太长时间去跟上 leader
3. leader 没有需要的 log entries，就请求 `InstallSnapshot`
4. 单一 RPC 发送整个 snapshot，不要实现图13的offset机制分裂snapshot
5. Raft必须丢弃旧log entries，让Go CV可以复用内存，让它们没有引用指针即可
6. 没有 `-race`，所有测试 6 分钟以内，其中1分钟 CPU time。如果`-race`，测试10分钟以内，2分钟 CPU time。

通过所有测试

# 过程记录

- 2025/9/29 2h 阅读Lab1代码，学习go语法
- 2025/10/1 2h 阅读Lab1代码、要求与指示，学习go RPC等
- 2025/10/2 4h 设计Lab1代码，学习go语法、map-reduce流程等
- 2025/10/3 5h 完成Lab1代码，学习go语法，debug，文档
- 2025/10/4 3h 完成Lab2代码，并学习概念与相关知识原理
- 2025/10/5 2h 阅读Lab1要求，Raft 概念学习