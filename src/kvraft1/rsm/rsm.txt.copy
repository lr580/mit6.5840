package rsm

import (
	"sync"
	"sync/atomic"
	"time"

	"6.5840/kvsrv1/rpc"
	"6.5840/labrpc"
	raft "6.5840/raft1"
	"6.5840/raftapi"
	tester "6.5840/tester1"
)

var useRaftStateMachine bool // to plug in another raft besided raft1

type Op struct {
	// Your definitions here.
	// Field names must start with capital letters,
	// otherwise RPC will break.
	Id  int64 // id+请求=操作
	Req any
}

// Op 操作的结果
type OpResult struct {
	err   rpc.Err
	reply any
}

// 等待提交的 Op 操作
type waitingOp struct {
	opId    int64
	term    int
	replyCh chan OpResult
}

// A server (i.e., ../server.go) that wants to replicate itself calls
// MakeRSM and must implement the StateMachine interface.  This
// interface allows the rsm package to interact with the server for
// server-specific operations: the server must implement DoOp to
// execute an operation (e.g., a Get or Put request), and
// Snapshot/Restore to snapshot and restore the server's state.
type StateMachine interface {
	DoOp(any) any
	Snapshot() []byte
	Restore([]byte)
}

type RSM struct {
	mu           sync.Mutex
	me           int
	rf           raftapi.Raft
	applyCh      chan raftapi.ApplyMsg
	maxraftstate int // snapshot if log grows this big
	sm           StateMachine
	// Your definitions here.
	nextOpId    int64
	waiting     map[int]*waitingOp // 按Raft index跟踪等待的操作
	dead        int32              // 服务器是否已关闭
	lastApplied int                // 上次应用的索引
}

// servers[] contains the ports of the set of
// servers that will cooperate via Raft to
// form the fault-tolerant key/value service.
//
// me is the index of the current server in servers[].
//
// the k/v server should store snapshots through the underlying Raft
// implementation, which should call persister.SaveStateAndSnapshot() to
// atomically save the Raft state along with the snapshot.
// The RSM should snapshot when Raft's saved state exceeds maxraftstate bytes,
// in order to allow Raft to garbage-collect its log. if maxraftstate is -1,
// you don't need to snapshot.
//
// MakeRSM() must return quickly, so it should start goroutines for
// any long-running work.
func MakeRSM(servers []*labrpc.ClientEnd, me int, persister *tester.Persister, maxraftstate int, sm StateMachine) *RSM {
	rsm := &RSM{
		me:           me,
		maxraftstate: maxraftstate,
		applyCh:      make(chan raftapi.ApplyMsg),
		sm:           sm,

		waiting:  make(map[int]*waitingOp),
		nextOpId: 1,
	}
	if !useRaftStateMachine {
		rsm.rf = raft.Make(servers, me, persister, rsm.applyCh)
	}

	go rsm.reader()
	return rsm
}

func (rsm *RSM) Raft() raftapi.Raft {
	return rsm.rf
}

// Submit a command to Raft, and wait for it to be committed.  It
// should return ErrWrongLeader if client should find new leader and
// try again.
func (rsm *RSM) Submit(req any) (rpc.Err, any) {

	// Submit creates an Op structure to run a command through Raft;
	// for example: op := Op{Me: rsm.me, Id: id, Req: req}, where req
	// is the argument to Submit and id is a unique id for the op.

	// your code here
	if atomic.LoadInt32(&rsm.dead) == 1 {
		return rpc.ErrWrongLeader, nil
	}
	rsm.mu.Lock()
	opId := rsm.nextOpId
	rsm.nextOpId++
	op := Op{opId, req}
	index, term, isLeader := rsm.rf.Start(op)
	if !isLeader {
		rsm.mu.Unlock()
		return rpc.ErrWrongLeader, nil
	}
	replyCh := make(chan OpResult, 1)
	rsm.waiting[index] = &waitingOp{opId, term, replyCh}
	rsm.mu.Unlock()

	// 再次检查 dead 标志，防止竞态条件
	// if atomic.LoadInt32(&rsm.dead) == 1 {
	// 	delete(rsm.waiting, index)
	// 	rsm.mu.Unlock()
	// 	return rpc.ErrWrongLeader, nil
	// }

	// 等待结果，带超时检查
	ticker := time.NewTicker(100 * time.Millisecond)
	defer ticker.Stop()

	for {
		select {
		case result := <-replyCh:
			return result.err, result.reply
		case <-ticker.C:
			// 优化：Ticker 触发时，再次非阻塞尝试读取 channel
			// 避免因为 select 的随机性错过已经到达的结果
			select {
			case result := <-replyCh:
				return result.err, result.reply
			default:
			}

			if atomic.LoadInt32(&rsm.dead) == 1 {
				return rpc.ErrWrongLeader, nil
			}

			// 检查 Term 是否改变或失去 Leadership
			curTerm, isLeader := rsm.rf.GetState()
			if !isLeader || curTerm != term {
				rsm.mu.Lock()
				// 再次确认 map 中是否还有这个 index 的等待
				// (虽然通常不需要，但为了严谨防止意外删除别的)
				if w, ok := rsm.waiting[index]; ok && w.opId == opId {
					delete(rsm.waiting, index)
				}
				rsm.mu.Unlock()
				return rpc.ErrWrongLeader, nil
			}
		}
	}
}

func (rsm *RSM) Kill() {
	atomic.StoreInt32(&rsm.dead, 1)
	if rsm.rf != nil {
		rsm.rf.Kill()
	}

	// shutdown waiter do
}

// 从applyCh读取已提交的命令并应用
func (rsm *RSM) reader() {
	for msg := range rsm.applyCh {
		if msg.CommandValid {
			rsm.applyCommand(msg)
		} else if msg.SnapshotValid {
			rsm.applySnapshot(msg)
		}
	}
	rsm.shutdownWaiters()
}

func (rsm *RSM) applyCommand(msg raftapi.ApplyMsg) {
	rsm.mu.Lock()
	defer rsm.mu.Unlock()

	if msg.CommandIndex <= rsm.lastApplied {
		return //做过了
	}
	rsm.lastApplied = msg.CommandIndex

	op, ok := msg.Command.(Op)
	if !ok {
		if waitOp, exists := rsm.waiting[msg.CommandIndex]; exists {
			waitOp.replyCh <- OpResult{err: rpc.ErrWrongLeader, reply: nil}
			delete(rsm.waiting, msg.CommandIndex)
		}
		return
	}

	reply := rsm.sm.DoOp(op.Req)

	if waitOp, exists := rsm.waiting[msg.CommandIndex]; exists {
		if waitOp.opId == op.Id {
			// 这是我们等待的操作，检查我们是否仍然是 leader
			curTerm, isLeader := rsm.rf.GetState()
			if isLeader && curTerm == waitOp.term {
				waitOp.replyCh <- OpResult{err: rpc.OK, reply: reply}
			} else {
				waitOp.replyCh <- OpResult{err: rpc.ErrWrongLeader, reply: nil}
			}
		} else {
			// 索引位置被不同的操作占用了（来自新 leader）
			waitOp.replyCh <- OpResult{err: rpc.ErrWrongLeader, reply: nil}
		}
		delete(rsm.waiting, msg.CommandIndex)
	}

	if rsm.maxraftstate > 0 && rsm.rf.PersistBytes() >= rsm.maxraftstate {
		snapshot := rsm.sm.Snapshot()
		rsm.rf.Snapshot(msg.CommandIndex, snapshot)
	}
}

func (rsm *RSM) applySnapshot(msg raftapi.ApplyMsg) {
	rsm.mu.Lock()
	defer rsm.mu.Unlock()

	// 只有在快照索引大于当前 lastApplied 时才应用
	if msg.SnapshotIndex <= rsm.lastApplied {
		return
	}

	rsm.sm.Restore(msg.Snapshot)
	rsm.lastApplied = msg.SnapshotIndex

	// 清理所有等待中的操作（它们都已过时）
	for index, waitOp := range rsm.waiting {
		if index <= msg.SnapshotIndex {
			waitOp.replyCh <- OpResult{err: rpc.ErrWrongLeader, reply: nil}
			delete(rsm.waiting, index)
		}
	}
}

func (rsm *RSM) shutdownWaiters() {
	atomic.StoreInt32(&rsm.dead, 1)
	rsm.mu.Lock()
	defer rsm.mu.Unlock()
	for idx, w := range rsm.waiting {
		select {
		case w.replyCh <- OpResult{err: rpc.ErrWrongLeader}:
		default:
		}
		delete(rsm.waiting, idx)
	}
}
