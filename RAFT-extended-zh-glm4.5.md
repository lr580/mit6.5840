一种可理解的共识算法探索
（扩展版）
Diego Ongaro and John Ousterhout
斯坦福大学

摘要

Raft 是一种用于管理复制日志的共识算法。它产生的结果等价于（多）Paxos，并且效率与 Paxos 相当，但其结构不同于 Paxos；这使得 Raft 比 Paxos 更易于理解，并为构建实际系统提供了更好的基础。为了增强可理解性，Raft 将共识的关键元素（如领导者选举、日志复制和安全性）分离开来，并强制执行更高程度的连贯性，以减少必须考虑的状态数量。一项针对用户的研究结果表明，Raft 比学生更容易学习。Raft 还包括一种用于更改集群成员身份的新机制，该机制使用重叠多数派来保证安全性。

1 引言

共识算法允许一组机器作为一个整体协同工作，即使其中一些成员发生故障也能存活。因此，它们在构建可靠的大型软件系统中发挥着关键作用。在过去的十年中，Paxos [15, 16] 主导了共识算法的讨论：大多数共识实现都基于 Paxos 或受其影响，Paxos 已成为向学生教授共识的主要工具。

不幸的是，尽管有许多尝试使其更易于理解，但 Paxos 仍然相当难以理解。此外，其架构需要复杂的更改来支持实际系统。因此，系统构建者和学生在使用 Paxos 时都遇到了困难。

在亲身经历了 Paxos 的困难之后，我们着手寻找一种新的共识算法，它能为系统构建和教育提供更好的基础。我们的方法非同寻常，因为我们的主要目标是可理解性：我们能否为实际系统定义一种共识算法，并以比 Paxos 更容易学习的方式进行描述？此外，我们希望该算法能够促进系统构建者所必需的直觉的形成。算法不仅要能工作，而且其工作原理也必须是显而易见的。

这项工作的成果是一种名为 Raft 的共识算法。在设计 Raft 时，我们采用了特定技术来提高可理解性，包括分解（Raft 将领导者选举、日志复制和安全性分离开）和状态空间简化（与 Paxos 相比，Raft 减少了非确定性和服务器之间不一致的程度）。一项涉及两所大学 43 名学生的用户研究表明，Raft 比 Paxos 更容易理解：在学习了两种算法后，这些学生中有 33 人能够更好地回答关于 Raft 的问题，而不是关于 Paxos 的问题。

Raft 在许多方面与现有的共识算法（最著名的是 Oki 和 Liskov 的 Viewstamped Replication [29, 22]）相似，但它有几个新颖的特性：

• 强领导者：Raft 比其他共识算法使用更强形式的领导权。例如，日志条目仅从领导者流向其他服务器。这简化了复制日志的管理，并使 Raft 更易于理解。

• 领导者选举：Raft 使用随机定时器来选举领导者。这仅为任何共识算法已经必需的心跳机制增加了一点机制，同时能够简单快速地解决冲突。

• 成员身份变更：Raft 用于更改集群中服务器集合的机制使用了一种新的联合共识方法，其中在不同配置转换期间，两个不同配置的多数派会重叠。这使得集群能够在配置更改期间继续正常运行。

我们相信，无论是出于教育目的还是作为实现的基础，Raft 都优于 Paxos 和其他共识算法。它比其他算法更简单、更易于理解；它的描述足够完整，以满足实际系统的需求；它有几个开源实现并被几家公司使用；其安全属性已被形式化规范和证明；其效率与其他算法相当。

本文的其余部分介绍了复制状态机问题（第 2 节），讨论了 Paxos 的优缺点（第 3 节），描述了我们提高可理解性的通用方法（第 4 节），介绍了 Raft 共识算法（第 5-8 节），评估了 Raft（第 9 节），并讨论了相关工作（第 10 节）。

2 复制状态机

共识算法通常出现在复制状态机 [37] 的上下文中。在这种方法中，一组服务器上的状态机计算相同状态的相同副本，即使其中一些服务器宕机也能继续运行。复制状态机用于解决分布式系统中的各种容错问题。例如，具有单个集群领导者的大规模系统，如 GFS [8]、HDFS [38] 和 RAMCloud [33]，通常使用一个独立的复制状态机来管理领导者选举并存储必须在领导者崩溃时幸存的配置信息。复制状态机的示例包括 Chubby [2] 和 ZooKeeper [11]。

复制状态机通常使用如图 1 所示的复制日志来实现。每台服务器存储一个包含一系列命令的日志，其状态机按顺序执行这些命令。每个日志都包含相同顺序的相同命令，因此每个状态机都处理相同的命令序列。由于状态机是确定性的，每个状态机都会计算相同的状态和相同的输出序列。

保持复制日志的一致性是共识算法的工作。服务器上的共识模块从客户端接收命令并将其添加到其日志中。它与其他服务器上的共识模块进行通信，以确保即使某些服务器发生故障，每个日志最终也包含相同顺序的相同请求。一旦命令被正确复制，每台服务器的状态机就会按日志顺序处理它们，并将输出返回给客户端。因此，这些服务器似乎形成了一个单一的高度可靠的状态机。

实际系统的共识算法通常具有以下特性：

• 在所有非拜占庭条件下，包括网络延迟、分区以及数据包丢失、重复和重排序，它们都能确保安全性（永不返回错误结果）。

• 只要任何多数派的服务器能够相互通信并与客户端通信并正常运行，它们就是完全功能性的（可用的）。因此，一个典型的五台服务器集群可以容忍任何两台服务器的故障。服务器被假定为通过停止来发生故障；它们之后可以从稳定存储中的状态恢复并重新加入集群。

• 它们不依赖于计时来确保日志的一致性：有故障的时钟和极度的消息延迟在最坏的情况下只会导致可用性问题。

• 在正常情况下，一旦多数派集群对一轮远程过程调用做出响应，命令即可完成；少数慢速服务器不必影响整体系统性能。

3 Paxos 的弊端是什么？

在过去的十年里，Leslie Lamport 的 Paxos 协议 [15] 几乎已经成为共识的同义词：它是课程中最常教授的协议，大多数共识实现都将其作为起点。Paxos 首先定义了一个能够就单个决策（例如单个复制日志条目）达成一致的协议。我们将此子集称为单决策 Paxos。然后，Paxos 将该协议的多个实例组合起来，以支持一系列决策，例如日志（多 Paxos）。Paxos 既确保了安全性，也确保了活性，并支持集群成员身份的更改。其正确性已被证明，并且在正常情况下是高效的。

不幸的是，Paxos 有两个显著的缺点。第一个缺点是 Paxos 异常难以理解。完整的解释 [15] 以其晦涩而臭名昭著；很少有人能成功理解它，而且只有付出巨大的努力。因此，有人尝试用更简单的术语来解释 Paxos [16, 20, 21]。这些解释侧重于单决策子集，但仍然具有挑战性。在 2012 年 NSDI 与会者的非正式调查中，我们发现很少有人熟悉 Paxos，即使在经验丰富的研究人员中也是如此。我们自己也曾为 Paxos 所困；在阅读了几个简化的解释并设计了我们自己的替代协议（这个过程花了将近一年时间）之后，我们才得以理解完整的协议。

我们假设 Paxos 的晦涩源于其选择单决策子集作为基础。单决策 Paxos 密集而微妙：它分为两个阶段，这两个阶段没有简单的直观解释，也无法独立理解。因此，很难发展出关于单决策协议为何工作的直觉。多 Paxos 的组合规则增加了显著的额外复杂性和微妙性。我们认为，就多个决策（即日志而不是单个条目）达成共识的总体问题，可以通过其他更直接和明显的方式进行分解。

Paxos 的第二个问题是它没有为构建实际的实现提供良好的基础。一个原因是没有广泛公认的多 Paxos 算法。Lamport 的描述大多是关于单决策 Paxos；他概述了多 Paxos 的可能方法，但许多细节缺失。已有一些尝试来充实和优化 Paxos，例如 [26]、[39] 和 [13]，但它们彼此不同，也与 Lamport 的概要不同。诸如 Chubby [4] 之类的系统实现了类似 Paxos 的算法，但在大多数情况下，其细节尚未公布。

此外，Paxos 的架构对于构建实际系统来说是一个糟糕的架构；这是单决策分解的另一个后果。例如，独立选择一组日志条目然后将它们融合成一个顺序日志几乎没有好处；这只是增加了复杂性。围绕日志设计一个系统，其中新条目以受限的顺序顺序追加，这样更简单、更高效。另一个问题是 Paxos 在其核心采用对称的对等方法（尽管它最终建议使用一种弱形式的领导权作为性能优化）。在一个只做一个决策的简化世界中，这是有道理的，但很少有实际系统使用这种方法。如果必须做出一系列决策，首先选举一个领导者，然后让领导者协调决策，会更简单、更快。

因此，实际系统与 Paxos 几乎没有相似之处。每个实现都从 Paxos 开始，发现实现它的困难，然后开发出显著不同的架构。这既耗时又容易出错，而理解 Paxos 的困难加剧了这个问题。Paxos 的表述可能很适合为其正确性证明定理，但实际的实现与 Paxos 相差太大，以至于这些证明几乎没有价值。Chubby 实现者下的典型评论如下：

Paxos 算法的描述与真实世界系统的需求之间存在显著差距……最终系统将基于一个未经证明的协议 [4]。

由于这些问题，我们得出结论，Paxos 既没有为系统构建也没有为教育提供良好的基础。鉴于共识在大型软件系统中的重要性，我们决定尝试设计一种替代共识算法，其特性优于 Paxos。Raft 就是那个实验的结果。

4 为可理解性而设计

在设计 Raft 时，我们有几个目标：它必须为系统构建提供一个完整且实用的基础，从而显著减少开发者的设计工作量；它必须在所有条件下都是安全的，并且在典型运行条件下是可用的；它必须对常见操作高效。但我们最重要的目标——也是最具挑战性的目标——是可理解性。必须让广大受众能够轻松理解该算法。此外，还必须能够发展出对该算法的直觉，以便系统构建者能够做出在实际实现中不可避免的扩展。

在 Raft 设计的许多地方，我们必须在多种方法之间进行选择。在这些情况下，我们根据可理解性来评估这些替代方案：解释每种替代方案有多困难（例如，其状态空间有多复杂，是否有微妙的含义？），以及读者完全理解该方法及其含义将有多容易？

我们认识到这种分析具有高度的主观性；尽管如此，我们使用了两种通常适用技术。第一种技术是众所周知的问题分解方法：在可能的情况下，我们将问题分解为可以相对独立解决、解释和理解的独立部分。例如，在 Raft 中，我们将领导者选举、日志复制、安全性和成员身份变更分离开来。我们的第二种方法是简化状态空间，通过减少需要考虑的状态数量，使系统更加连贯，并在可能的情况下消除非确定性。具体来说，日志不允许有空洞，并且 Raft 限制了日志之间可能不一致的方式。尽管在大多数情况下我们试图消除非确定性，但在某些情况下，非确定性实际上可以提高可理解性。特别是，随机方法引入了非确定性，但它们倾向于以类似的方式处理所有可能的选择（“随便选哪个；都行”）。我们使用随机化来简化 Raft 领导者选举算法。

5 Raft 共识算法

Raft 是一种管理第 2 节所述的复制日志的算法。图 2 以摘要形式总结了该算法以供参考，图 3 列出了该算法的关键属性；这些图的元素将在本节其余部分分段讨论。

Raft 通过首先选举一个杰出的领导者，然后赋予领导者管理复制日志的全部责任，来实现共识。领导者接受来自客户端的日志条目，在其他服务器上复制它们，并告诉服务器何时可以将日志条目安全地应用到它们的状态机上。拥有领导者简化了复制日志的管理。例如，领导者可以在不咨询其他服务器的情况下决定将新条目放在日志的什么位置，并且数据以简单的方式从领导者流向其他服务器。领导者可能会发生故障或与其他服务器断开连接，在这种情况下，会选举一个新的领导者。

基于领导者方法，Raft 将共识问题分解为三个相对独立的子问题，将在以下小节中讨论：
• 领导者选举：当现有领导者发生故障时，必须选择一个新的领导者（第 5.2 节）。

• 日志复制：领导者必须接受来自客户端的日志条目，并在整个集群中复制它们，强制其他日志与自己的日志保持一致（第 5.3 节）。

• 安全性：Raft 的关键安全属性是图 3 中的状态机安全属性：如果任何服务器已将特定的日志条目应用到其状态机，则任何其他服务器都不得为相同的日志索引应用不同的命令。第 5.4 节描述了 Raft 如何确保此属性；该解决方案涉及对第 5.2 节描述的选举机制的额外限制。

在介绍了共识算法之后，本节讨论了可用性问题以及计时在系统中的作用。

5.1 Raft 基础

一个 Raft 集群包含多台服务器；五个是典型的数量，这使得系统能够容忍两次故障。在任何给定时间，每台服务器都处于三种状态之一：领导者、追随者或候选人。在正常操作中，恰好有一个领导者，所有其他服务器都是追随者。追随者是被动式的：它们不自行发出请求，只是响应来自领导者和候选人的请求。领导者处理所有客户端请求（如果客户端联系追随者，追随者会将其重定向到领导者）。第三种状态——候选人——用于选举新的领导者，如第 5.2 节所述。图 4 显示了状态及其转换；转换将在下面讨论。

Raft 将时间划分为任意长度的任期，如图 5 所示。任期用连续整数编号。每个任期以选举开始，其中一个或多个候选人尝试成为领导者，如第 5.2 节所述。如果候选人赢得选举，那么它将在该剩余的任期内担任领导者。在某些情况下，选举会导致平票。在这种情况下，任期将以没有领导者结束；一个新的任期（包含新的选举）将很快开始。Raft 确保在任何给定的任期内最多有一个领导者。

不同的服务器可能在不同的时间观察到任期间的转换，在某些情况下，服务器可能不会观察到选举甚至整个任期。任期在 Raft 中充当逻辑时钟 [14]，它们使服务器能够检测过时的信息，例如过期的领导者。每台服务器存储一个当前任期号，该号码随时间单调递增。每当服务器通信时，都会交换当前任期；如果一台服务器的当前任期小于另一台服务器的当前任期，则它会将其当前任期更新为较大的值。如果候选人或领导者发现其任期已过时，它会立即恢复为追随者状态。如果服务器收到带有过时任期号的请求，它会拒绝该请求。

Raft 服务器使用远程过程调用 (RPC) 进行通信，基本的共识算法只需要两种类型的 RPC。RequestVote RPC 在选举期间由候选人发起（第 5.2 节），AppendEntries RPC 由领导者发起，用于复制日志条目并提供一种心跳形式（第 5.3 节）。第 7 节添加了第三种 RPC，用于在服务器之间传输快照。如果服务器没有及时收到响应，它们会重试 RPC，并且为了获得最佳性能，它们会并行发出 RPC。

5.2 领导者选举

Raft 使用心跳机制来触发领导者选举。当服务器启动时，它们以追随者身份开始。只要它从领导者或候选人那里收到有效的 RPC，服务器就保持在追随者状态。领导者定期向所有追随者发送心跳（AppendEntries RPC，不携带日志条目），以维护其权威性。如果追随者在称为选举超时的一段时间内没有收到任何通信，那么它会假定没有可用的领导者，并开始选举以选择新的领导者。要开始选举，追随者会增加其当前任期并转换为候选人状态。然后，它为自己投票，并向集群中的每台其他服务器并行发出 RequestVote RPC。候选人会一直保持这种状态，直到发生以下三件事之一发生：(a) 它赢得选举，(b) 另一台服务器将自己确立为领导者，或 (c) 一段时间过去了，没有产生获胜者。这些结果将在下面的段落中单独讨论。

如果候选人获得了整个集群中多数服务器的投票（在同一任期内），则候选人赢得选举。每台服务器在一个给定的任期内最多只能投票给一位候选人，遵循先到先得的原则（注意：第 5.4 节对投票增加了额外的限制）。多数规则确保在特定任期内最多只能有一位候选人赢得选举（图 3 中的选举安全属性）。一旦候选人赢得选举，它就成为领导者。然后，它向所有其他服务器发送心跳消息以建立其权威性并防止新的选举。

在等待投票时，候选人可能会收到来自另一台声称是领导者的服务器的 AppendEntries RPC。如果领导者的任期（包含在其 RPC 中）至少与候选人的当前任期一样大，那么候选人会将领导者视为合法的，并返回追随者状态。如果 RPC 中的任期小于候选人的当前任期，则候选人会拒绝该 RPC 并保持候选人状态。

第三种可能的结果是候选人既没有赢也没有输选举：如果许多追随者同时成为候选人，投票可能会分散，以至于没有候选人获得多数。当这种情况发生时，每个候选人都会超时，并通过增加其任期并启动另一轮 RequestVote RPC 来开始新的选举。但是，如果没有额外的措施，分散的投票可能会无限期地重复。

Raft 使用随机化的选举超时来确保分散的投票很少，并且能够迅速解决。为了首先防止分散的投票，选举超时是从一个固定的时间间隔内随机选择的（例如，150-300ms）。这使得服务器分开了，因此在大多数情况下只有一台服务器会超时；它在任何其他服务器超时之前赢得选举并发送心跳。相同的机制用于处理分散的投票。每个候选人在选举开始时重新启动其随机化的选举超时，并且它会等待该超时过去后再开始下一次选举；这减少了在新选举中再次发生分散投票的可能性。第 9.3 节表明，这种方法能够迅速选举出领导者。

选举是可理解性如何指导我们在设计替代方案之间做出选择的一个例子。最初，我们计划使用一个排名系统：为每个候选人分配一个唯一的排名，用于在相互竞争的候选人之间进行选择。如果候选人发现另一个排名更高的候选人，它会返回追随者状态，以便排名更高的候选人可以更容易地赢得下一次选举。我们发现这种方法在可用性方面造成了微妙的问题（如果排名较低的服务器发生故障，它可能需要超时并再次成为候选人，但如果它这样做得太快，它可能会重置选举领导者的进度）。我们多次调整了算法，但每次调整后都会出现新的边界情况。最终，我们得出结论，随机重试方法更明显、更易于理解。

5.3 日志复制

一旦选举出领导者，它就开始处理客户端请求。每个客户端请求包含一个由复制状态机执行的命令。领导者将命令作为新条目附加到其日志中，然后并行向其他服务器发出 AppendEntries RPC 以复制该条目。当该条目被安全复制（如下所述）后，领导者将该条目应用到其状态机，并将执行结果返回给客户端。如果追随者崩溃或运行缓慢，或者网络数据包丢失，领导者会无限次重试 AppendEntries RPC（即使在向客户端响应之后），直到所有追随者最终存储所有日志条目。

日志的组织方式如图 6 所示。每个日志条目存储一个状态机命令以及领导者接收该条目时的任期号。日志条目中的任期号用于检测日志之间的不一致性，并确保图 3 中的一些属性。每个日志条目还有一个整数索引，用于标识其在日志中的位置。

领导者决定何时将日志条目安全地应用到状态机；这样的条目被称为已提交。Raft 保证已提交的条目是持久的，并且最终将由所有可用的状态机执行。一旦领导者创建的条目在多数派服务器上复制，该日志条目就被提交（例如，图 6 中的条目 7）。这也提交了领导者日志中所有先前的条目，包括由先前领导者创建的条目。第 5.4 节讨论了在领导者更改后应用此规则时的一些微妙之处，并表明这种提交定义是安全的。领导者跟踪它知道的最高已提交索引，并在未来的 AppendEntries RPC（包括心跳）中包含该索引，以便其他服务器最终能够知道。一旦追随者知道一个日志条目已提交，它就会将该条目应用到其本地状态机中（按日志顺序）。

我们设计了 Raft 日志机制，以维护不同服务器日志之间的高度一致性。这不仅简化了系统的行为并使其更具可预测性，而且是确保安全性的重要组成部分。Raft 维护以下属性，这些属性共同构成了图 3 中的日志匹配属性：

• 如果不同日志中的两个条目具有相同的索引和任期，那么它们存储相同的命令。

• 如果不同日志中的两个条目具有相同的索引和任期，那么在该索引之前的所有条目中，日志是相同的。

第一个属性源于这样一个事实：领导者在一个给定的任期内最多为给定的日志索引创建一个条目，并且日志条目永远不会改变其在日志中的位置。第二个属性由 AppendEntries 执行的简单一致性检查来保证。当发送 AppendEntries RPC 时，领导者包含其日志中紧跟在新条目之前的条目的索引和任期。如果追随者在其日志中没有找到具有相同索引和任期的条目，那么它会拒绝新条目。一致性检查充当归纳步骤：日志的初始空状态满足日志匹配属性，并且每当日志扩展时，一致性检查都会保留日志匹配属性。因此，每当 AppendEntries 成功返回时，领导者就知道追随者的日志与它自己的日志在新条目之前是相同的。

在正常操作期间，领导者与追随者的日志保持一致，因此 AppendEntries 一致性检查永远不会失败。但是，领导者崩溃可能会使日志不一致（旧领导者可能没有完全复制其日志中的所有条目）。这些不一致性可能在一系列领导者与追随者崩溃中复合。图 7 说明了追随者的日志可能与新领导者日志不同的方式。追随者可能缺少领导者上存在的条目，它可能有额外的未提交条目，或者两者都有。日志中缺失和多余的条目可能跨越多个任期。

在 Raft 中，领导者通过强制追随者的日志复制自己的日志来处理不一致。这意味着追随者日志中的冲突条目将被领导者日志中的条目覆盖。第 5.4 节将表明，结合一个额外的限制，这是安全的。

为了将追随者的日志与自己的日志保持一致，领导者必须找到两个日志一致的最新日志条目，删除追随者日志中该点之后的所有条目，并将领导者在该点之后的所有条目发送给追随者。所有这些操作都是响应 AppendEntries RPC 执行的一致性检查而发生的。领导者为每个追随者维护一个 nextIndex，即领导者将发送给该追随者的下一个日志条目的索引。当领导者首次上任时，它将所有 nextIndex 值初始化为其日志中最后一个条目之后的索引（图 7 中的 11）。如果追随者的日志与领导者的日志不一致，下一个 AppendEntries RPC 中的一致性检查将失败。在被拒绝后，领导者会减少 nextIndex 并重试 AppendEntries RPC。最终 nextIndex 将达到领导者与追随者日志匹配的点。当这种情况发生时，AppendEntries 将成功，这会删除追随者日志中的任何冲突条目并附加来自领导者日志的条目（如果有的话）。一旦 AppendEntries 成功，追随者的日志就与领导者的日志一致，并且在该剩余的任期内将保持这种状态。

如果需要，可以优化协议以减少被拒绝的 AppendEntries RPC 的数量。例如，在拒绝 AppendEntries 请求时，追随者可以包含冲突条目的任期以及它为该任期存储的第一个索引。有了这些信息，领导者可以将 nextIndex 减少以绕过该任期中所有冲突的条目；对于每个具有冲突条目的任期，只需要一个 AppendEntries RPC，而不是每个条目一个 RPC。在实践中，我们怀疑这种优化是必要的，因为故障很少发生，并且不太可能有多个不一致的条目。

通过这种机制，领导者在上任时无需采取任何特殊操作来恢复日志一致性。它只是开始正常操作，并且日志会自动响应 AppendEntries 一致性检查的故障而收敛。领导者永远不会覆盖或删除其日志中的条目（图 3 中的领导者仅追加属性）。

这种日志复制机制表现出第 2 节中描述的理想的共识属性：Raft 可以接受、复制和应用新的日志条目，只要多数派服务器在线；在正常情况下，新的日志条目可以通过一轮到集群多数派的 RPC 来复制；单个慢速追随者不会影响性能。

5.4 安全性

前面的几节描述了 Raft 如何选举领导者并复制日志条目。然而，迄今为止描述的机制还不足以确保每个状态机都以完全相同的顺序执行完全相同的命令。例如，追随者可能在领导者提交几个日志条目时不可用，然后它可能被选为领导者并用新的条目覆盖这些条目；结果，不同的状态机可能会执行不同的命令序列。

本节通过增加一个关于哪些服务器可以被选举为领导者的限制来完成 Raft 算法。该限制确保任何给定任期的领导者包含先前所有已提交的条目（图 3 中的领导者完整性属性）。鉴于选举限制，我们然后使提交规则更加精确。最后，我们提出了领导者完整性属性的证明概要，并展示了它如何导致复制状态机的正确行为。

5.4.1 选举限制

在任何基于领导者的共识算法中，领导者最终必须存储所有已提交的日志条目。在某些共识算法中，例如 Viewstamped Replication [22]，领导者即使最初不包含所有已提交的条目也可以被选举。这些算法包含额外的机制来识别缺失的条目并将其传输到新的领导者中，无论是在选举过程中还是在选举之后不久。不幸的是，这导致了相当多的额外机制和复杂性。Raft 使用了一种更简单的方法，它保证所有先前任期中的已提交条目都存在于每个新领导者身上，而无需将这些条目传输给领导者。这意味着日志条目只单向流动，从领导者到追随者，并且领导者永远不会覆盖其日志中的现有条目。

Raft 使用投票过程来防止候选人在其日志不包含所有已提交条目的情况下赢得选举。候选人必须联系集群中的多数派才能被选举，这意味着每个已提交的条目必须存在于这些服务器中的至少一个中。如果候选人的日志至少与该多数派中任何其他日志一样最新（“最新”的定义将在下面精确说明），那么它将拥有所有已提交的条目。RequestVote RPC 实现了此限制：RPC 包含有关候选人日志的信息，如果选民的日志比候选人的日志更最新，则选民拒绝投票。

Raft 通过比较日志中最后一个条目的索引和任期来确定哪两个日志更最新。如果日志具有不同任期的最后一个条目，则具有较晚任期的日志更最新。如果日志以相同的任期结束，则较长的日志更最新。

5.4.2 提交先前任期的条目

如第 5.3 节所述，领导者知道来自其当前任期的条目一旦存储在多数派服务器上就被提交。如果领导者在提交条目之前崩溃，未来的领导者将尝试完成复制该条目。然而，领导者不能立即得出结论：一旦先前任期的条目存储在多数派服务器上，它就被提交。图 8 说明了一种情况，旧的日志条目存储在多数派服务器上，但仍然可能被未来的领导者覆盖。

为了消除图 8 中的问题，Raft 决不通过计算副本来提交先前任期的日志条目。只有来自领导者当前任期的日志条目才通过计算副本来提交；一旦当前任期的条目以这种方式提交，那么所有先前的条目都会由于日志匹配属性而被间接提交。在某些情况下，领导者可以安全地得出结论认为旧的日志条目已提交（例如，如果该条目存储在每台服务器上），但 Raft 采用了更保守的方法以简单性为重。

Raft 在提交规则中承担了这种额外的复杂性，因为当领导者复制先前任期的条目时，日志条目会保留其原始的任期号。在其他共识算法中，如果新的领导者重新复制先前“任期”中的条目，它必须使用其新的“任期号”。Raft 的方法使得推理日志条目更容易，因为它们随时间推移和在跨日志时保持相同的任期号。此外，Raft 中的新领导者发送的先前任期日志条目比其他算法中的少（其他算法必须发送冗余的日志条目，以便重新编号它们才能被提交）。

5.4.3 安全性论证

鉴于完整的 Raft 算法，我们现在可以更精确地论证领导者完整性属性成立（此论证基于安全性证明；参见第 9.2 节）。我们假设领导者完整性属性不成立，然后我们证明一个矛盾。假设任期 T 的领导者（leaderT）提交了其任期中的一个日志条目，但该日志条目未被某个未来任期的领导者存储。考虑最小的任期 U > T，其领导者（leaderU）不存储该条目。

1. 已提交的条目在选举时必须不在 leaderU 的日志中（领导者从不删除或覆盖条目）。

2. leaderT 在集群的多数派上复制了该条目，并且 leaderU 从集群的多数派那里获得了投票。因此，至少有一台服务器（“选民”）既接受了来自 leaderT 的条目，又投票给了 leaderU，如图 9 所示。选民是达到矛盾的关键。

3. 选民必须在投票给 leaderU 之前接受来自 leaderT 的已提交条目；否则它会拒绝来自 leaderT 的 AppendEntries 请求（其当前任期将高于 T）。

4. 当选民投票给 leaderU 时，它仍然存储该条目，因为每个中间领导者都包含该条目（根据假设），领导者从不删除条目，并且追随者只有在与领导者冲突时才会删除条目。

5. 选民将其投票授予了 leaderU，因此 leaderU 的日志必须至少与选民的日志一样最新。这导致了两种矛盾之一。

6. 首先，如果选民和 leaderU 共享相同的最后一个日志任期，那么 leaderU 的日志必须至少与选民的日志一样长，因此它的日志包含选民日志中的每个条目。这是一个矛盾，因为选民包含已提交的条目，而 leaderU 被假设不包含。

7. 否则，leaderU 的最后一个日志任期必须大于选民的。此外，它大于 T，因为选民的最后一个日志任期至少为 T（它包含来自任期 T 的已提交条目）。创建 leaderU 最后一个日志条目的早期领导者必须在其日志中包含已提交的条目（根据假设）。然后，根据日志匹配属性，leaderU 的日志也必须包含已提交的条目，这是一个矛盾。

8. 这完成了矛盾。因此，所有大于 T 的任期的领导者都必须包含在任期 T 中提交的所有来自任期 T 的条目。

9. 日志匹配属性保证未来的领导者也将包含间接提交的条目，如图 8(d) 中的索引 2。

鉴于领导者完整性属性，我们可以证明图 3 中的状态机安全属性，该属性指出，如果一台服务器已将给定索引的日志条目应用到其状态机，则任何其他服务器都绝不能为相同的索引应用不同的日志条目。在服务器将日志条目应用到其状态机时，其日志必须与领导者的日志在该条目之前相同，并且该条目必须已提交。现在考虑在任何服务器应用给定日志索引的最低任期；领导者完整性属性保证所有更高任期的领导者将存储相同的日志条目，因此在后续任期中应用该索引的服务器将应用相同的值。因此，状态机安全属性成立。

最后，Raft 要求服务器按日志索引顺序应用条目。结合状态机安全属性，这意味着所有服务器将完全相同的一组日志条目应用到它们的状态机中，顺序也相同。

5.5 追随者和候选人崩溃

到此为止，我们一直关注领导者故障。追随者和候选人崩溃比领导者崩溃更容易处理，并且它们都以相同的方式处理。如果追随者或候选人崩溃，那么发送给它们的未来的 RequestVote 和 AppendEntries RPC 将会失败。Raft 通过无限次重试来处理这些故障；如果崩溃的服务器重新启动，则 RPC 将成功完成。如果服务器在完成 RPC 但在响应之前崩溃，那么它在重新启动后将再次收到相同的 RPC。Raft RPC 是幂等的，因此这不会造成任何伤害。例如，如果追随者收到一个 AppendEntries 请求，其中包含其日志中已存在的日志条目，它会忽略新请求中的这些条目。

5.6 计时与可用性

我们对 Raft 的要求之一是安全性不能依赖于计时：系统不能仅仅因为某些事件发生得比预期更快或更慢而产生错误结果。然而，可用性（系统及时响应客户端的能力）不可避免地必须依赖于计时。例如，如果消息交换的时间超过服务器崩溃之间的典型时间，候选人将无法保持足够长的时间来赢得选举；如果没有稳定的领导者，Raft 无法取得进展。

领导者选举是 Raft 中计时最关键的部分。只要系统满足以下时间要求，Raft 就能够选举并维持一个稳定的领导者：

broadcastTime ≪ electionTimeout ≪ MTBF
在这个不等式中，broadcastTime 是服务器并行向集群中每台服务器发送 RPC 并接收其响应所需的平均时间；electionTimeout 是第 5.2 节中描述的选举超时；MTBF 是单个服务器的平均故障间隔时间。广播时间应该比选举超时小一个数量级，以便领导者能够可靠地发送心跳消息以防止追随者开始选举；鉴于用于选举超时的随机化方法，这个不等式也使得分散的投票不太可能发生。选举超时应比 MTBF 小几个数量级，以便系统能够稳步前进。当领导者崩溃时，系统将在大约选举超时的时间内不可用；我们希望这仅代表总时间的一小部分。

广播时间和 MTBF 是底层系统的属性，而选举超时是我们必须选择的。Raft 的 RPC 通常要求接收方将信息持久化到稳定存储中，因此广播时间可能从 0.5ms 到 20ms 不等，具体取决于存储技术。因此，选举超时可能在 10ms 到 500ms 之间。典型的服务器 MTBF 为几个月或更长，这很容易满足时间要求。

6 集群成员身份变更

到目前为止，我们一直假设集群配置（参与共识算法的服务器集合）是固定的。在实践中，偶尔需要更改配置，例如在服务器故障时替换它们或更改复制程度。虽然这可以通过将整个集群离线、更新配置文件然后重新启动集群来完成，但这会在更改期间使集群不可用。此外，如果存在任何手动步骤，则存在操作人员错误的风险。为了避免这些问题，我们决定自动化配置更改并将其纳入 Raft 共识算法。

为了使配置更改机制安全，在转换过程中不可能存在两个领导者被选举为同一任期的任何一点。不幸的是，任何让服务器直接从旧配置切换到新配置的方法都是不安全的。不可能一次性原子地切换所有服务器，因此在转换过程中，集群可能会分裂成两个独立的多数派（见图 10）。

为了确保安全性，配置更改必须使用两阶段方法。实现这两个阶段有多种方式。例如，一些系统（例如 [22]）在第一阶段禁用旧配置，使其无法处理客户端请求；然后第二阶段启用新配置。在 Raft 中，集群首先切换到一个我们称为联合共识的过渡配置；一旦联合共识被提交，系统就会切换到新配置。联合共识结合了旧配置和新配置：

• 日志条目会复制到两个配置中的所有服务器。

• 任何配置中的任何服务器都可以担任领导者。

• 协议（用于选举和条目提交）需要分别来自旧配置和新配置的多数派。

联合共识允许单个服务器在不同时间在配置之间切换，而不会损害安全性。此外，联合共识允许集群在整个配置更改期间继续处理客户端请求。

集群配置使用复制日志中的特殊条目进行存储和通信；图 11 说明了配置更改过程。当领导者收到将配置从 Cold 更改为 Cnew 的请求时，它将联合共识的配置（图中的 Cold,new）作为日志条目存储，并使用前面描述的机制复制该条目。一旦给定服务器将新的配置条目添加到其日志中，它就使用该配置进行所有未来的决策（服务器始终使用其日志中最新的配置，无论该条目是否已提交）。这意味着领导者将使用 Cold,new 的规则来确定 Cold,new 日志条目何时被提交。如果领导者崩溃，可能会根据获胜的候选人是否收到 Cold,new 而在 Cold 或 Cold,new 下选择新的领导者。在任何情况下，在此期间 Cnew 都不能单方面做出决策。

一旦 Cold,new 被提交，Cold 和 Cnew 都无法获得另一方的批准就做出决策，并且领导者完整性属性确保只有拥有 Cold,new 日志条目的服务器才能被选为领导者。现在，领导者可以安全地创建描述 Cnew 的日志条目并将其复制到集群中。同样，一旦服务器看到该配置，它就会在该服务器上生效。当新配置在 Cnew 的规则下被提交后，旧配置就无关紧要了，不在新配置中的服务器可以关闭。如图 11 所示，任何时候 Cold 和 Cnew 都不能单方面做出决策；这保证了安全性。

还有三个问题需要解决以进行重新配置。第一个问题是新服务器可能最初不存储任何日志条目。如果它们以这种状态添加到集群中，可能需要很长时间才能赶上，在此期间可能无法提交新的日志条目。为了避免可用性差距，Raft 在配置更改之前引入了一个额外的阶段，在该阶段中，新服务器作为非投票成员加入集群（领导者将日志条目复制到它们身上，但它们不计入多数派）。一旦新服务器赶上了集群的其余部分，就可以如上所述进行重新配置。

第二个问题是集群领导者可能不属于新配置。在这种情况下，一旦它提交了 Cnew 日志条目，领导者就会下台（返回追随者状态）。这意味着将有一段时间（在它提交 Cnew 时），领导者正在管理一个不包含其自身的集群；它复制日志条目，但在计算多数派时不计算自己。当 Cnew 被提交时会发生领导者转换，因为这是新配置可以独立运行的第一点（总是可以从 Cnew 中选择领导者）。在此之前，只有 Cold 中的服务器可以被选为领导者。

第三个问题是被移除的服务器（那些不在 Cnew 中的服务器）可能会破坏集群。这些服务器不会收到心跳，因此它们会超时并开始新的选举。然后，它们将发送带有新任期号的 RequestVote RPC，这将导致当前领导者恢复为追随者状态。最终会选举出新的领导者，但被移除的服务器会再次超时，这个过程会重复，从而导致可用性差。

为了防止此问题，服务器在认为存在当前领导者时会忽略 RequestVote RPC。具体来说，如果服务器在从当前领导者那里听到的最小选举超时内收到 RequestVote RPC，它不会更新其任期或授予其投票。这不影响正常的选举，在正常选举中，每台服务器在开始选举之前至少等待一个最小的选举超时。但是，它有助于避免被移除服务器的干扰：如果领导者能够将心跳发送到其集群，那么它就不会被更大的任期号罢免。

7 日志压缩

Raft 的日志在正常操作过程中增长以包含更多的客户端请求，但在实际系统中，它不能无限增长。随着日志变得越来越长，它会占用更多空间，并且需要更多时间来重放。如果没有某种机制来丢弃日志中积累的过时信息，最终将导致可用性问题。

快照是压缩最简单的方法。在快照中，整个当前系统状态被写入稳定存储上的快照，然后丢弃到该点为止的整个日志。Chubby 和 ZooKeeper 都使用了快照，本节的其余部分将描述 Raft 中的快照。

增量压缩方法，例如日志清理 [36] 和日志结构合并树 [30, 5]，也是可能的。这些方法一次处理一部分数据，因此它们将压缩的负载更均匀地分布在时间上。它们首先选择一个积累了大量已删除和覆盖对象的数据区域，然后以更紧凑的方式重写该区域中的活动对象，并释放该区域。与总是处理整个数据集的快照相比，这需要显著的额外机制和复杂性。虽然日志清理需要对 Raft 进行修改，但状态机可以使用与快照相同的接口实现 LSM 树。

图 12 显示了 Raft 中快照的基本思想。每台服务器独立地拍摄快照，仅覆盖其日志中的已提交条目。大部分工作由状态机将其当前状态写入快照组成。Raft 还在快照中包含少量元数据：最后包含的索引是快照替换的日志中最后一个条目的索引（状态机已应用的最后一个条目），最后包含的任期是该条目的任期。这些被保留下来以支持 AppendEntries 对快照后第一个日志条目的一致性检查，因为该条目需要前一个日志索引和任期。为了支持集群成员身份更改（第 6 节），快照还包括截至最后包含索引的日志中的最新配置。一旦服务器完成写入快照，它可以删除到最后包含索引为止的所有日志条目，以及任何先前的快照。

尽管服务器通常独立拍摄快照，但领导者偶尔必须向落后的追随者发送快照。当领导者已经丢弃了需要发送给追随者的下一个日志条目时，就会发生这种情况。幸运的是，在正常操作中这种情况不太可能发生：一直保持同步的追随者已经拥有领导者即将发送的下一个日志条目。然而，异常慢的追随者或加入集群的新服务器（第 6 节）则没有。使此类追随者保持最新状态的方法是领导者通过网络向其发送快照。

领导者使用一种称为 InstallSnapshot 的新 RPC 将快照发送给落后的追随者；参见图 13。当追随者通过此 RPC 接收快照时，它必须决定如何处理其现有的日志条目。通常，快照将包含接收方日志中尚未存在的新信息。在这种情况下，追随者会丢弃其整个日志；它全部被快照取代，并且可能有一些与快照冲突的未提交条目。如果追随者接收到描述其日志前缀的快照（由于重传或错误），则快照覆盖的日志条目将被删除，但快照之后的条目仍然有效且必须保留。

这种快照方法偏离了 Raft 的强领导者原则，因为追随者可以在不通知领导者的情况下拍摄快照。然而，我们认为这种偏离是合理的。虽然拥有领导者有助于在达成共识时避免冲突决策，但在拍摄快照时共识已经达成，因此没有决策冲突。数据仍然只从领导者流向追随者，只是追随者现在可以重组它们的数据。

我们考虑过另一种基于领导者的替代方法，其中只有领导者会创建快照，然后将其发送给其每个追随者。然而，这有两个缺点。首先，将快照发送给每个追随者会浪费网络带宽并减慢快照过程。每个追随者已经生产自己快照所需的信息，并且对于服务器来说，从本地状态生成快照通常比通过网络发送和接收一个快照要便宜得多。其次，领导者的实现会更复杂。例如，领导者需要并行向追随者发送快照，同时向它们复制新的日志条目，以便不会阻塞新的客户端请求。

还有两个影响快照性能的问题。首先，服务器必须决定何时拍摄快照。如果服务器拍摄快照太频繁，它会浪费磁盘带宽和能源；如果拍摄太不频繁，它有耗尽其存储容量的风险，并且增加了在重启期间重放日志所需的时间。一个简单的策略是当日志达到固定的大小时拍摄快照。如果这个大小设置得显著大于预期快照的大小，那么快照的磁盘带宽开销将会很小。

第二个性能问题是写入快照可能需要相当长的时间，我们不希望这延迟正常的操作。解决方案是使用写时复制技术，以便在接受新更新的同时不会影响正在写入的快照。例如，使用函数式数据结构构建的状态机自然支持这一点。或者，可以使用操作系统的写时复制支持（例如 Linux 上的 fork）来创建整个状态机的内存快照（我们的实现使用了这种方法）。

8 客户端交互

本节描述了客户端如何与 Raft 交互，包括客户端如何找到集群领导者以及 Raft 如何支持线性化语义 [10]。这些问题适用于所有基于共识的系统，而 Raft 的解决方案与其他系统类似。

Raft 的客户端将所有请求发送给领导者。当客户端首次启动时，它会连接到一个随机选择的服务器。如果客户端的首选不是领导者，该服务器将拒绝客户端的请求并提供有关它最近听到的领导者的信息（AppendEntries 请求包含领导者的网络地址）。如果领导者崩溃，客户端请求将超时；然后客户端会尝试使用随机选择的服务器再次尝试。我们对 Raft 的目标是实现线性化语义（每个操作看起来在其调用和响应之间的某个时刻瞬时执行，恰好一次）。然而，如迄今为止所描述，Raft 可能多次执行一个命令：例如，如果领导者在提交日志条目之后但在向客户端响应之前崩溃，客户端将使用新的领导者重试该命令，导致它被第二次执行。解决方案是为客户端为每个命令分配唯一的序列号。然后，状态机跟踪为每个客户端处理的最新序列号以及相关的响应。如果它收到一个序列号已被执行的命令，它会立即响应而无需重新执行该请求。

只读操作可以在不写入日志的情况下处理。然而，如果没有额外的措施，这将冒着返回过时数据的风险，因为响应请求的领导者可能已经被它不知道的更新领导者所取代。线性化读取不能返回过时数据，Raft 需要两个额外的预防措施来保证这一点而不使用日志。首先，领导者必须拥有关于哪些条目已提交的最新信息。领导者完整性属性保证领导者拥有所有已提交的条目，但在其任期开始时，它可能不知道哪些是已提交的。为了找出，它需要提交一个来自其任期的条目。Raft 通过让每个领导者在其任期开始时将一个空的无操作条目提交到日志中来处理这个问题。其次，领导者必须在处理只读请求之前检查自己是否已被罢免（如果有更新的领导者被选举，其信息可能已过时）。Raft 通过让领导者在响应只读请求之前与集群的多数派交换心跳消息来处理这个问题。或者，领导者可以依赖心跳机制来提供某种租约 [9] 的形式，但这将依赖于计时来保证安全（它假设有界的时钟偏差）。

9 实现与评估

我们已经实现了 Raft，作为复制状态机的一部分，用于为 RAMCloud [33] 存储配置信息并协助 RAMCloud 协调器的故障转移。Raft 实现包含大约 2000 行 C++ 代码，不包括测试、注释或空行。源代码可以免费获取 [23]。还有大约 25 个独立的第三方开源实现 [34]，处于不同的开发阶段，基于本文的草案。此外，各种公司正在部署基于 Raft 的系统 [34]。

本节的其余部分使用三个标准来评估 Raft：可理解性、正确性和性能。

9.1 可理解性

为了衡量 Raft 相对于 Paxos 的可理解性，我们进行了一项实验研究，研究对象是斯坦福大学高级操作系统课程和加州大学伯克利分校分布式计算课程中的高年级本科生和研究生。我们录制了一段关于 Raft 的视频讲座和另一段关于 Paxos 的视频，并创建了相应的测验。Raft 讲座涵盖了本文除日志压缩之外的所有内容；Paxos 讲座涵盖了足以创建等效复制状态机的材料，包括单决策 Paxos、多决策 Paxos、重新配置以及一些在实践中需要的优化（例如领导者选举）。测验测试了算法的基本理解，并要求学生推理边界情况。每位学生观看了其中一个视频，参加了相应的测验，观看了第二个视频，然后参加了第二个测验。大约一半的参与者先完成 Paxos 部分，另一半先完成 Raft 部分，以研究在研究的第一部分中表现和经验方面的个体差异。我们比较了参与者在每个测验中的得分，以确定参与者是否表现出对 Raft 更好的理解。

我们试图尽可能公平地比较 Paxos 和 Raft。该实验在两个方面偏向 Paxos：43 名参与者中有 15 名报告说之前有一些 Paxos 经验，并且 Paxos 视频比 Raft 视频长 14%。如表 1 所示，我们已经采取措施减轻潜在偏见来源。我们所有的材料都可以供审查 [28, 31]。

平均而言，参与者在 Raft 测验中的得分比在 Paxos 测验中高出 4.9 分（满分 60 分，Raft 的平均分是 25.7，Paxos 的平均分是 20.8）；图 14 显示了他们的个人得分。成对 t 检验表明，有 95% 的置信度，Raft 分数的真实分布的平均值至少比 Paxos 分数的真实分布的平均值大 2.5 分。

我们还创建了一个线性回归模型，根据三个因素预测新学生的测验得分：他们参加了哪个测验，他们之前对 Paxos 的经验程度，以及他们学习算法的顺序。该模型预测，测验的选择会产生 12.5 分的差异，有利于 Raft。这显著高于观察到的 4.9 分的差异，因为许多实际学生有 Paxos 经验，这极大地帮助了 Paxos，而对 Raft 的帮助则稍小一些。奇怪的是，该模型还预测，对于已经参加 Paxos 测验的人，Raft 的分数低 6.3 分；尽管我们不知道为什么，但这在统计学上似乎是显著的。

我们还在测验后对参与者进行了调查，看看他们认为哪种算法更容易实现或解释；这些结果如图 15 所示。绝大多数参与者报告说 Raft 更容易实现和解释（对于每个问题，41 人中有 33 人）。然而，这些自我报告的感觉可能不如参与者的测验得分可靠，并且参与者可能知道我们的假设即 Raft 更容易理解而产生偏见。

关于 Raft 用户研究的详细讨论可在 [31] 中找到。

9.2 正确性

我们已经为第 5 节中描述的共识机制开发了一个形式化规范和安全性证明。形式化规范 [31] 使用 TLA+ 规范语言 [17] 使图 2 中总结的信息完全精确。它大约有 400 行长，作为证明的主题。对于任何实现 Raft 的人来说，它本身也很有用。我们使用 TLA 证明系统 [7] 机械地证明了领导者完整性属性。然而，这个证明依赖于尚未机械检查的不变量（例如，我们没有证明规范的安全性）。此外，我们写了一个关于状态机安全属性的非正式证明 [31]，该证明是完整的（它仅依赖于规范）并且相对精确（大约 3500 字长）。

9.3 性能

Raft 的性能与其他共识算法（如 Paxos）相似。性能最重要的情况是当既定领导者正在复制新的日志条目时。Raft 使用最少数量的消息（领导者到集群一半的单次往返）来实现这一点。也可以进一步改进 Raft 的性能。例如，它轻松支持批处理和流水线请求，以实现更高的吞吐量和更低的延迟。文献中已经为其他算法提出了各种优化；其中许多可以应用于 Raft，但我们将其留给未来的工作。

我们使用我们的 Raft 实现来测量 Raft 领导者选举算法的性能并回答两个问题。首先，选举过程是否快速收敛？其次，领导者崩溃后可以实现的最小停机时间是多少？

为了测量领导者选举，我们反复崩溃一个五台服务器集群的领导者，并计时检测崩溃和选举新领导者需要多长时间（见图 16）。为了生成最坏情况，每次试验中的服务器具有不同的日志长度，因此一些候选人没有资格成为领导者。此外，为了鼓励分散的投票，我们的测试脚本在终止领导者进程之前触发领导者心跳 RPC 的同步广播（这近似于领导者在崩溃之前复制新日志条目的行为）。领导者在心跳间隔内均匀随机崩溃，这是所有测试中最小选举超时的一半。因此，最小的可能停机时间大约是最小选举超时的一半。

图 16 顶部的图表显示，选举超时中少量的随机化就足以避免选举中的分散投票。在没有随机性的情况下，在我们的测试中，领导者选举一直持续超过 10 秒，因为有许多分散的投票。仅增加 5ms 的随机化就非常有帮助，导致中位停机时间为 287ms。使用更多的随机化可以改善最坏情况：使用 50ms 的随机性，最坏情况完成时间（超过 1000 次试验）为 513ms。

图 16 底部的图表显示，通过减少选举超时可以减少停机时间。选举超时为 12-24ms 时，平均只需 35ms 就可以选举出领导者（最长的试验耗时 152ms）。然而，将超时降低到这个点以下违反了 Raft 的时间要求：领导者难以在其他服务器开始新选举之前广播心跳。这可能导致不必要的领导者变更并降低整体系统可用性。我们建议使用保守的选举超时，例如 150-300ms；这样的超时不太可能导致不必要的领导者变更，并且仍能提供良好的可用性。

10 相关工作

有许多与共识算法相关的出版物，其中许多属于以下类别之一：

• Lamport 对 Paxos 的原始描述 [15]，以及尝试更清晰地解释它的尝试 [16, 20, 21]。

• Paxos 的详细阐述，它填补了缺失的细节并修改了算法以提供更好的实现基础 [26, 39, 13]。

• 实现共识算法的系统，例如 Chubby [2, 4]、ZooKeeper [11, 12] 和 Spanner [6]。Chubby 和 Spanner 的算法尚未详细发表，尽管两者都声称基于 Paxos。ZooKeeper 的算法已发表更详细的内容，但它与 Paxos 大不相同。

• 可以应用于 Paxos 的性能优化 [18, 19, 3, 25, 1, 27]。

• Oki 和 Liskov 的 Viewstamped Replication (VR)，这是与 Paxos 大约同时开发的共识的另一种方法。原始描述 [29] 与分布式事务协议交织在一起，但核心共识协议在最近的更新 [22] 中被分离出来。VR 使用基于领导者的方法，与 Raft 有许多相似之处。

Raft 和 Paxos 最大的区别在于 Raft 的强领导者：Raft 将领导者选举作为共识协议的重要组成部分，并将尽可能多的功能集中在领导者身上。这种方法导致了更简单、更易于理解的算法。例如，在 Paxos 中，领导者选举与基本共识协议是正交的：它仅作为性能优化，并非达成共识所必需。然而，这导致了额外的机制：Paxos 包括基本共识的两阶段协议和用于领导者选举的单独机制。相比之下，Raft 将领导者选举直接纳入共识算法，并将其作为共识的两个阶段中的第一个。这比 Paxos 机制更少。

与 Raft 一样，VR 和 ZooKeeper 都是基于领导者的，因此与 Raft 一样共享许多优于 Paxos 的优势。然而，Raft 的机制比 VR 或 ZooKeeper 少，因为它最小化了非领导者的功能。例如，Raft 中的日志条目只单向流动：通过 AppendEntries RPC 从领导者向外流出。在 VR 中，日志条目双向流动（领导者可以在选举过程中接收日志条目）；这导致了额外的机制和复杂性。ZooKeeper 的已发表描述也将日志条目传输到领导者并从领导者传输，但实现显然更像 Raft [35]。

据我们所知，Raft 比任何其他基于共识的日志复制算法的消息类型都少。例如，我们统计了 VR 和 ZooKeeper 用于基本共识和成员身份更改的消息类型数量（不包括日志压缩和客户端交互，因为这些几乎独立于算法）。VR 和 ZooKeeper 每个都定义了 10 种不同的消息类型，而 Raft 只有 4 种消息类型（两种 RPC 请求及其响应）。Raft 的消息比其他算法的消息稍微密集一些，但它们总体上更简单。此外，VR 和 ZooKeeper 是以在领导者变更期间传输整个日志来描述的；需要额外的消息类型来优化这些机制以使其具有实用性。

Raft 的强领导者方法简化了算法，但它排除了一些性能优化。例如，平等主义 Paxos (EPaxos) 可以在某些条件下通过无领导者方法实现更高的性能 [27]。EPaxos 利用状态机命令的可交换性。只要其他并发提议的命令与它可交换，任何服务器都可以通过一次通信来提交命令。然而，如果并发提议的命令不可交换，EPaxos 需要额外的通信轮次。由于任何服务器都可以提交命令，EPaxos 在服务器之间很好地平衡了负载，并且能够在 WAN 环境中实现比 Raft 更低的延迟。然而，它为 Paxos 增加了显著的复杂性。

其他工作中已经提出或实现了几种不同的集群成员身份更改方法，包括 Lamport 的原始提议 [15]、VR [22] 和 SMART [24]。我们为 Raft 选择联合共识方法，因为它利用了共识协议的其余部分，因此成员身份更改所需的额外机制非常少。Lamport 的基于 α 的方法不适用于 Raft，因为它假设可以在没有领导者的情况下达成共识。与 VR 和 SMART 相比，Raft 的重新配置算法的优势在于，成员身份更改可以在不限制正常请求处理的情况下进行；相比之下，VR 在配置更改期间停止所有正常处理，并且 SMART 对未决请求数量施加了类似 α 的限制。Raft 的方法也比 VR 或 SMART 增加的机制更少。

11 结论

算法通常以正确性、效率和/或简洁性为主要目标设计。尽管这些都是值得追求的目标，但我们认为可理解性同样重要。在开发者将算法转化为实际实现之前，无法实现任何其他目标，而实际实现将不可避免地偏离并扩展已发表的形式。除非开发者对算法有深入的理解并能够对其产生直觉，否则他们将难以在实现中保留其理想的特性。

在本文中，我们解决了分布式共识的问题，在这里一个被广泛接受但难以理解的算法 Paxos 多年来一直挑战着学生和开发者。我们开发了一种新算法 Raft，我们已经证明它比 Paxos 更易于理解。我们还相信 Raft 为系统构建提供了更好的基础。将可理解性作为主要设计目标改变了我们设计 Raft 的方式；随着设计的进展，我们发现我们重复使用了一些技术，例如分解问题和简化状态空间。这些技术不仅提高了 Raft 的可理解性，也使我们更容易说服自己其正确性。

12 致谢

没有 Ali Ghodsi、David Mazi`eres 以及伯克利 CS 294-91 和斯坦福 CS 240 的学生的支持，用户研究就不可能进行。Scott Klemmer 帮助我们设计了用户研究，Nelson Ray 就统计分析向我们提供建议。用于用户研究的 Paxos 幻灯片大量借用了 Lorenzo Alvisi 最初创建的幻灯片集。特别感谢 David Mazi`eres 和 Ezra Hoch 发现了 Raft 中的微妙错误。许多人就论文和用户研究材料提供了宝贵的反馈，包括 Ed Bugnion、Michael Chan、Hugues Evrard、Daniel Gifﬁn、Arjun Gopalan、Jon Howell、Vimalkumar Jeyakumar、Ankita Kejriwal、Aleksandar Kracun、Amit Levy、Joel Martin、Satoshi Matsushita、Oleg Pesok、David Ramos、Robbert van Renesse、Mendel Rosenblum、Nicolas Schiper、Deian Stefan、Andrew Stone、Ryan Stutsman、David Terei、Stephen Yang、Matei Zaharia、24 位匿名会议审稿人（有重复），尤其是我们的指导者 Eddie Kohler。Werner Vogels 推文链接到一个早期草稿，这给 Raft 带来了显著的曝光。这项工作得到了 Gigascale Systems Research Center 和 Multiscale Systems Center 的支持，这是六个研究中心中的两个，它们由 Focus Center Research Program 资助，这是一个由 Semiconductor Research Corporation 资助的项目，得到了 STARnet 的支持，这是一个由 MARCO 和 DARPA 资助的 Semiconductor Research Corporation 计划，得到了美国国家科学基金会资助号 0963859 的支持，并得到了 Facebook、Google、Mellanox、NEC、NetApp、SAP 和 Samsung 的赠款支持。Diego Ongaro 得到 The Junglee Corporation Stanford Graduate Fellowship 的支持。

参考文献
[1] BOLOSKY, W. J., BRADSHAW, D., HAAGENS, R. B., KUSTERS, N. P., AND LI, P. Paxos replicated state machines as the basis of a high-performance data store. In Proc. NSDI’11, USENIX Conference on Networked Systems Design and Implementation (2011), USENIX, pp. 141–154.
[2] BURROWS, M. The Chubby lock service for loosely-coupled distributed systems. In Proc. OSDI’06, Symposium on Operating Systems Design and Implementation (2006), USENIX, pp. 335–350.
[3] CAMARGOS, L. J., SCHMIDT, R. M., AND PEDONE, F. Multicoordinated Paxos. In Proc. PODC’07, ACM Symposium on Principles of Distributed Computing (2007), ACM, pp. 316–317.
[4] CHANDRA, T. D., GRIESEMER, R., AND REDSTONE, J. Paxos made live: an engineering perspective. In Proc. PODC’07, ACM Symposium on Principles of Distributed Computing (2007), ACM, pp. 398–407.
[5] CHANG, F., DEAN, J., GHEMAWAT, S., HSIEH, W. C., WALLACH, D. A., BURROWS, M., CHANDRA, T., FIKES, A., AND GRUBER, R. E. Bigtable: a distributed storage system for structured data. In Proc. OSDI’06, USENIX Symposium on Operating Systems Design and Implementation (2006), USENIX, pp. 205–218.
[6] CORBETT, J. C., DEAN, J., EPSTEIN, M., FIKES, A., FROST, C., FURMAN, J. J., GHEMAWAT, S., GUBAREV, A., HEISER, C., HOCHSCHILD, P., HSIEH, W., KANTHAK, S., KOGAN, E., LI, H., LLOYD, A., MELNIK, S., MWAURA, D., NAGLE, D., QUINLAN, S., RAO, R., ROLIG, L., SAITO, Y., SZYMANIAK, M., TAYLOR, C., WANG, R., AND WOODFORD, D. Spanner: Google’s globally-distributed database. In Proc. OSDI’12, USENIX Conference on Operating Systems Design and Implementation (2012), USENIX, pp. 251–264.
[7] COUSINEAU, D., DOLIGEZ, D., LAMPORT, L., MERZ, S., RICKETTS, D., AND VANZETTO, H. TLA+ proofs. In Proc. FM’12, Symposium on Formal Methods (2012), D. Giannakopoulou and D. M´ery, Eds., vol. 7436 of Lecture Notes in Computer Science, Springer, pp. 147–154.
[8] GHEMAWAT, S., GOBIOFF, H., AND LEUNG, S.-T. The Google file system. In Proc. SOSP’03, ACM Symposium on Operating Systems Principles (2003), ACM, pp. 29–43.
[9] GRAY, C., AND CHERITON, D. Leases: An efficient fault-tolerant mechanism for distributed file cache consistency. In Proceedings of the 12th ACM Symposium on Operating Systems Principles (1989), pp. 202–210.
[10] HERLIHY, M. P., AND WING, J. M. Linearizability: a correctness condition for concurrent objects. ACM Transactions on Programming Languages and Systems 12 (July 1990), 463–492.
[11] HUNT, P., KONAR, M., JUNQUEIRA, F. P., AND REED, B. ZooKeeper: wait-free coordination for internet-scale systems. In Proc ATC’10, USENIX Annual Technical Conference (2010), USENIX, pp. 145–158.
[12] JUNQUEIRA, F. P., REED, B. C., AND SERAFINI, M. Zab: High-performance broadcast for primary-backup systems. In Proc. DSN’11, IEEE/IFIP Int’l Conf. on Dependable Systems & Networks (2011), IEEE Computer Society, pp. 245–256.
[13] KIRSCH, J., AND AMIR, Y. Paxos for system builders. Tech. Rep. CNDS-2008-2, Johns Hopkins University, 2008.
[14] LAMPORT, L. Time, clocks, and the ordering of events in a distributed system. Commununications of the ACM 21, 7 (July 1978), 558–565.
[15] LAMPORT, L. The part-time parliament. ACM Transactions on Computer Systems 16, 2 (May 1998), 133–169.
[16] LAMPORT, L. Paxos made simple. ACM SIGACT News 32, 4 (Dec. 2001), 18–25.
[17] LAMPORT, L. Specifying Systems, The TLA+ Language and Tools for Hardware and Software Engineers. Addison-Wesley, 2002.
[18] LAMPORT, L. Generalized consensus and Paxos. Tech. Rep. MSR-TR-2005-33, Microsoft Research, 2005.
[19] LAMPORT, L. Fast paxos. Distributed Computing 19, 2 (2006), 79–103.
[20] LAMPSON, B. W. How to build a highly available system using consensus. In Distributed Algorithms, O. Baboaglu and K. Marzullo, Eds. Springer-Verlag, 1996, pp. 1–17.
[21] LAMPSON, B. W. The ABCD’s of Paxos. In Proc. PODC’01, ACM Symposium on Principles of Distributed Computing (2001), ACM, pp. 13–13.
[22] LISKOV, B., AND COWLING, J. Viewstamped replication revisited. Tech. Rep. MIT-CSAIL-TR-2012-021, MIT, July 2012.
[23] LogCabin source code. http://github.com/logcabin/logcabin.
[24] LORCH, J. R., ADYA, A., BOLOSKY, W. J., CHAIKEN, R., DOUCEUR, J. R., AND HOWELL, J. The SMART way to migrate replicated stateful services. In Proc. EuroSys’06, ACM SIGOPS/EuroSys European Conference on Computer Systems (2006), ACM, pp. 103–115.
[25] MAO, Y., JUNQUEIRA, F. P., AND MARZULLO, K. Mencius: building efficient replicated state machines for WANs. In Proc. OSDI’08, USENIX Conference on Operating Systems Design and Implementation (2008), USENIX, pp. 369–384.
[26] MAZI `ERES, D. Paxos made practical. http://www.scs.stanford.edu/~dm/home/papers/paxos.pdf, Jan. 2007.
[27] MORARU, I., ANDERSEN, D. G., AND KAMINSKY, M. There is more consensus in egalitarian parliaments. In Proc. SOSP’13, ACM Symposium on Operating System Principles (2013), ACM.
[28] Raft user study. http://ramcloud.stanford.edu/~ongaro/userstudy/.
[29] OKI, B. M., AND LISKOV, B. H. Viewstamped replication: A new primary copy method to support highly-available distributed systems. In Proc. PODC’88, ACM Symposium on Principles of Distributed Computing (1988), ACM, pp. 8–17.
[30] O’NEIL, P., CHENG, E., GAWLICK, D., AND ONEIL, E. The log-structured merge-tree (LSM-tree). Acta Informatica 33, 4 (1996), 351–385.
[31] ONGARO, D. Consensus: Bridging Theory and Practice. PhD thesis, Stanford University, 2014 (work in progress). http://ramcloud.stanford.edu/~ongaro/thesis.pdf.
[32] ONGARO, D., AND OUSTERHOUT, J. In search of an understandable consensus algorithm. In Proc ATC’14, USENIX Annual Technical Conference (2014), USENIX.
[33] OUSTERHOUT, J., AGRAWAL, P., ERICKSON, D., KOZYRAKIS, C., LEVERICH, J., MAZI `ERES, D., MITRA, S., NARAYANAN, A., ONGARO, D., PARULKAR, G., ROSENBLUM, M., RUMBLE, S. M., STRATMANN, E., AND STUTSMAN, R. The case for RAMCloud. Communications of the ACM 54 (July 2011), 121–130.
[34] Raft consensus algorithm website. http://raftconsensus.github.io.
[35] REED, B. Personal communications, May 17, 2013.
[36] ROSENBLUM, M., AND OUSTERHOUT, J. The design and implementation of a log-structured file system. ACM Trans. Comput. Syst. 10 (February 1992), 26–52.
[37] SCHNEIDER, F. B. Implementing fault-tolerant services using the state machine approach: a tutorial. ACM Computing Surveys 22, 4 (Dec. 1990), 299–319.
[38] SHVACHKO, K., KUANG, H., RADIA, R., AND CHANSLER, R. The Hadoop distributed file system. In Proc. MSST’10, Symposium on Mass Storage Systems and Technologies (2010), IEEE Computer Society, pp. 1–10.
[39] VAN RENESSE, R. Paxos made moderately complex. Tech. rep., Cornell University, 2012.